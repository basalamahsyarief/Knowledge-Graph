{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Relational graph convolutional network\n",
    "================================================\n",
    "\n",
    "**Author:** Lingfan Yu, Mufei Li, Zheng Zhang\n",
    "\n",
    "In this tutorial, you learn how to implement a relational graph convolutional\n",
    "network (R-GCN). This type of network is one effort to generalize GCN \n",
    "to handle different relationships between entities in a knowledge base. To \n",
    "learn more about the research behind R-GCN, see `Modeling Relational Data with Graph Convolutional\n",
    "Networks <https://arxiv.org/pdf/1703.06103.pdf>`_ \n",
    "\n",
    "The straightforward graph convolutional network (GCN) and \n",
    "`DGL tutorial <http://doc.dgl.ai/tutorials/index.html>`_) exploits\n",
    "structural information of a dataset (that is, the graph connectivity) in order to\n",
    "improve the extraction of node representations. Graph edges are left as\n",
    "untyped.\n",
    "\n",
    "A knowledge graph is made up of a collection of triples in the form\n",
    "subject, relation, object. Edges thus encode important information and\n",
    "have their own embeddings to be learned. Furthermore, there may exist\n",
    "multiple edges among any given pair.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief introduction to R-GCN\n",
    "---------------------------\n",
    "In *statistical relational learning* (SRL), there are two fundamental\n",
    "tasks:\n",
    "\n",
    "- **Entity classification** - Where you assign types and categorical\n",
    "  properties to entities.\n",
    "- **Link prediction** - Where you recover missing triples.\n",
    "\n",
    "In both cases, missing information is expected to be recovered from the \n",
    "neighborhood structure of the graph. For example, the R-GCN\n",
    "paper cited earlier provides the following example. Knowing that Mikhail Baryshnikov was educated at the Vaganova Academy\n",
    "implies both that Mikhail Baryshnikov should have the label person, and\n",
    "that the triple (Mikhail Baryshnikov, lived in, Russia) must belong to the\n",
    "knowledge graph.\n",
    "\n",
    "R-GCN solves these two problems using a common graph convolutional network. It's \n",
    "extended with multi-edge encoding to compute embedding of the entities, but\n",
    "with different downstream processing.\n",
    "\n",
    "- Entity classification is done by attaching a softmax classifier at the\n",
    "  final embedding of an entity (node). Training is through loss of standard\n",
    "  cross-entropy.\n",
    "- Link prediction is done by reconstructing an edge with an autoencoder\n",
    "  architecture, using a parameterized score function. Training uses negative\n",
    "  sampling.\n",
    "\n",
    "This tutorial focuses on the first task, entity classification, to show how to generate entity\n",
    "representation. `Complete\n",
    "code <https://github.com/dmlc/dgl/tree/rgcn/examples/pytorch/rgcn>`_\n",
    "for both tasks is found in the DGL Github repository.\n",
    "\n",
    "Key ideas of R-GCN\n",
    "-------------------\n",
    "Recall that in GCN, the hidden representation for each node $i$ at\n",
    "$(l+1)^{th}$ layer is computed by:\n",
    "\n",
    "\\begin{align}h_i^{l+1} = \\sigma\\left(\\sum_{j\\in N_i}\\frac{1}{c_i} W^{(l)} h_j^{(l)}\\right)~~~~~~~~~~(1)\\\\\\end{align}\n",
    "\n",
    "where $c_i$ is a normalization constant.\n",
    "\n",
    "The key difference between R-GCN and GCN is that in R-GCN, edges can\n",
    "represent different relations. In GCN, weight $W^{(l)}$ in equation\n",
    "$(1)$ is shared by all edges in layer $l$. In contrast, in\n",
    "R-GCN, different edge types use different weights and only edges of the\n",
    "same relation type $r$ are associated with the same projection weight\n",
    "$W_r^{(l)}$.\n",
    "\n",
    "So the hidden representation of entities in $(l+1)^{th}$ layer in\n",
    "R-GCN can be formulated as the following equation:\n",
    "\n",
    "\\begin{align}h_i^{l+1} = \\sigma\\left(W_0^{(l)}h_i^{(l)}+\\sum_{r\\in R}\\sum_{j\\in N_i^r}\\frac{1}{c_{i,r}}W_r^{(l)}h_j^{(l)}\\right)~~~~~~~~~~(2)\\\\\\end{align}\n",
    "\n",
    "where $N_i^r$ denotes the set of neighbor indices of node $i$\n",
    "under relation $r\\in R$ and $c_{i,r}$ is a normalization\n",
    "constant. In entity classification, the R-GCN paper uses\n",
    "$c_{i,r}=|N_i^r|$.\n",
    "\n",
    "The problem of applying the above equation directly is the rapid growth of\n",
    "the number of parameters, especially with highly multi-relational data. In\n",
    "order to reduce model parameter size and prevent overfitting, the original\n",
    "paper proposes to use basis decomposition.\n",
    "\n",
    "\\begin{align}W_r^{(l)}=\\sum\\limits_{b=1}^B a_{rb}^{(l)}V_b^{(l)}~~~~~~~~~~(3)\\\\\\end{align}\n",
    "\n",
    "Therefore, the weight $W_r^{(l)}$ is a linear combination of basis\n",
    "transformation $V_b^{(l)}$ with coefficients $a_{rb}^{(l)}$.\n",
    "The number of bases $B$ is much smaller than the number of relations\n",
    "in the knowledge base.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Another weight regularization, block-decomposition, is implemented in\n",
    "   the `link prediction <link-prediction_>`_.</p></div>\n",
    "\n",
    "Implement R-GCN in DGL\n",
    "----------------------\n",
    "\n",
    "An R-GCN model is composed of several R-GCN layers. The first R-GCN layer\n",
    "also serves as input layer and takes in features (for example, description texts)\n",
    "that are associated with node entity and project to hidden space. In this tutorial,\n",
    "we only use the entity ID as an entity feature.\n",
    "\n",
    "R-GCN layers\n",
    "~~~~~~~~~~~~\n",
    "\n",
    "For each node, an R-GCN layer performs the following steps:\n",
    "\n",
    "- Compute outgoing message using node representation and weight matrix\n",
    "  associated with the edge type (message function)\n",
    "- Aggregate incoming messages and generate new node representations (reduce\n",
    "  and apply function)\n",
    "\n",
    "The following code is the definition of an R-GCN hidden layer.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Each relation type is associated with a different weight. Therefore,\n",
    "   the full weight matrix has three dimensions: relation, input_feature,\n",
    "   output_feature.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "from functools import partial\n",
    "import dgl\n",
    "\n",
    "class RGCNLayer(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, num_rels, num_bases=-1, bias=None,\n",
    "                 activation=None, is_input_layer=False):\n",
    "        super(RGCNLayer, self).__init__()\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.is_input_layer = is_input_layer\n",
    "\n",
    "        # sanity check\n",
    "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
    "            self.num_bases = self.num_rels\n",
    "\n",
    "        # weight bases in equation (3)\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.num_bases, self.in_feat,\n",
    "                                                self.out_feat))\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # linear combination coefficients in equation (3)\n",
    "            self.w_comp = nn.Parameter(torch.Tensor(self.num_rels, self.num_bases))\n",
    "\n",
    "        # add bias\n",
    "        if self.bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_feat))\n",
    "\n",
    "        # init trainable parameters\n",
    "        nn.init.xavier_uniform_(self.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        if self.num_bases < self.num_rels:\n",
    "            nn.init.xavier_uniform_(self.w_comp,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "        if self.bias:\n",
    "            nn.init.xavier_uniform_(self.bias,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # generate all weights from bases (equation (3))\n",
    "            weight = self.weight.view(self.in_feat, self.num_bases, self.out_feat)\n",
    "            weight = torch.matmul(self.w_comp, weight).view(self.num_rels,\n",
    "                                                        self.in_feat, self.out_feat)\n",
    "        else:\n",
    "            weight = self.weight\n",
    "\n",
    "        if self.is_input_layer:\n",
    "            def message_func(edges):\n",
    "                # for input layer, matrix multiply can be converted to be\n",
    "                # an embedding lookup using source node id\n",
    "                embed = weight.view(-1, self.out_feat)\n",
    "                index = edges.data['rel_type'] * self.in_feat + edges.src['id']\n",
    "                return {'msg': embed[index] * edges.data['norm']}\n",
    "        else:\n",
    "            def message_func(edges):\n",
    "                w = weight[edges.data['rel_type'].long()]\n",
    "                msg = torch.bmm(edges.src['h'].unsqueeze(1), w).squeeze()\n",
    "                msg = msg * edges.data['norm']\n",
    "                return {'msg': msg}\n",
    "\n",
    "        def apply_func(nodes):\n",
    "            h = nodes.data['h']\n",
    "            if self.bias:\n",
    "                h = h + self.bias\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            return {'h': h}\n",
    "\n",
    "        g.update_all(message_func, fn.sum(msg='msg', out='h'), apply_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full R-GCN model defined\n",
    "~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels,\n",
    "                 num_bases=-1, num_hidden_layers=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "        # create initial features\n",
    "        self.features = self.create_features()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input to hidden\n",
    "        i2h = self.build_input_layer()\n",
    "        self.layers.append(i2h)\n",
    "        # hidden to hidden\n",
    "        for _ in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer()\n",
    "            self.layers.append(h2h)\n",
    "        # hidden to output\n",
    "        h2o = self.build_output_layer()\n",
    "        self.layers.append(h2o)\n",
    "\n",
    "    # initialize feature for each node\n",
    "    def create_features(self):\n",
    "        features = torch.arange(self.num_nodes)\n",
    "        return features\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return RGCNLayer(self.num_nodes, self.h_dim, self.num_rels, self.num_bases,\n",
    "                         activation=F.relu, is_input_layer=True)\n",
    "\n",
    "    def build_hidden_layer(self):\n",
    "        return RGCNLayer(self.h_dim, self.h_dim, self.num_rels, self.num_bases,\n",
    "                         activation=F.relu)\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return RGCNLayer(self.h_dim, self.out_dim, self.num_rels, self.num_bases,\n",
    "                         activation=partial(F.softmax, dim=1))\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.features is not None:\n",
    "            g.ndata['id'] = self.features\n",
    "        for layer in self.layers:\n",
    "            layer(g)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle dataset\n",
    "~~~~~~~~~~~~~~~~\n",
    "This tutorial uses Institute for Applied Informatics and Formal Description Methods (AIFB) dataset from R-GCN paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset aifb\n",
      "Number of nodes:  8285\n",
      "Number of edges:  66371\n",
      "Number of relations:  91\n",
      "Number of classes:  4\n",
      "removing nodes that are more than 3 hops away\n"
     ]
    }
   ],
   "source": [
    "# load graph data\n",
    "from dgl.contrib.data import load_data\n",
    "import numpy as np\n",
    "data = load_data(dataset='aifb')\n",
    "num_nodes = data.num_nodes\n",
    "num_rels = data.num_rels\n",
    "num_classes = data.num_classes\n",
    "labels = data.labels\n",
    "train_idx = data.train_idx\n",
    "# split training and validation set\n",
    "val_idx = train_idx[:len(train_idx) // 5]\n",
    "train_idx = train_idx[len(train_idx) // 5:]\n",
    "\n",
    "# edge type and normalization factor\n",
    "edge_type = torch.from_numpy(data.edge_type)\n",
    "edge_norm = torch.from_numpy(data.edge_norm).unsqueeze(1)\n",
    "\n",
    "labels = torch.from_numpy(labels).view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create graph and model\n",
    "~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8285"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "919 0\n",
      "919 0\n",
      "1127 0\n",
      "1127 0\n",
      "1934 0\n",
      "2062 0\n",
      "5286 0\n",
      "5286 0\n",
      "7786 0\n",
      "7786 0\n",
      "1 1\n",
      "1534 1\n",
      "2 2\n",
      "3577 2\n",
      "3 3\n",
      "7656 3\n",
      "4 4\n",
      "377 4\n",
      "1028 4\n",
      "1148 4\n",
      "1148 4\n",
      "1174 4\n",
      "1384 4\n",
      "1384 4\n",
      "3298 4\n",
      "3722 4\n",
      "3822 4\n",
      "3822 4\n",
      "5080 4\n",
      "5080 4\n",
      "5176 4\n",
      "5176 4\n",
      "5354 4\n",
      "5667 4\n",
      "5961 4\n",
      "7056 4\n",
      "5 5\n",
      "49 5\n",
      "6 6\n",
      "7519 6\n",
      "7 7\n",
      "7708 7\n",
      "8 8\n",
      "377 8\n",
      "592 8\n",
      "1668 8\n",
      "2364 8\n",
      "3705 8\n",
      "4362 8\n",
      "4583 8\n",
      "5761 8\n",
      "5761 8\n",
      "6110 8\n",
      "7056 8\n",
      "9 9\n",
      "2724 9\n",
      "10 10\n",
      "6516 10\n",
      "11 11\n",
      "3049 11\n",
      "12 12\n",
      "1934 12\n",
      "3060 12\n",
      "3060 12\n",
      "5474 12\n",
      "5474 12\n",
      "7229 12\n",
      "7229 12\n",
      "7836 12\n",
      "8274 12\n",
      "8274 12\n",
      "13 13\n",
      "4757 13\n",
      "14 14\n",
      "5979 14\n",
      "15 15\n",
      "7005 15\n",
      "16 16\n",
      "1357 16\n",
      "17 17\n",
      "1490 17\n",
      "1934 17\n",
      "2682 17\n",
      "3765 17\n",
      "3765 17\n",
      "18 18\n",
      "63 18\n",
      "63 18\n",
      "1934 18\n",
      "7693 18\n",
      "19 19\n",
      "7242 19\n",
      "20 20\n",
      "2966 20\n",
      "21 21\n",
      "7884 21\n",
      "22 22\n",
      "5355 22\n",
      "23 23\n",
      "2374 23\n",
      "24 24\n",
      "2369 24\n",
      "25 25\n",
      "7255 25\n",
      "26 26\n",
      "2453 26\n",
      "27 27\n",
      "60 27\n",
      "28 28\n",
      "1606 28\n",
      "2104 28\n",
      "29 29\n",
      "6855 29\n",
      "30 30\n",
      "3193 30\n",
      "31 31\n",
      "2661 31\n",
      "32 32\n",
      "4790 32\n",
      "33 33\n",
      "277 33\n",
      "944 33\n",
      "962 33\n",
      "1174 33\n",
      "2028 33\n",
      "2028 33\n",
      "2672 33\n",
      "3823 33\n",
      "4123 33\n",
      "6024 33\n",
      "6024 33\n",
      "6260 33\n",
      "6260 33\n",
      "6631 33\n",
      "6631 33\n",
      "7008 33\n",
      "7056 33\n",
      "7264 33\n",
      "34 34\n",
      "7980 34\n",
      "35 35\n",
      "8192 35\n",
      "36 36\n",
      "2359 36\n",
      "37 37\n",
      "377 37\n",
      "541 37\n",
      "2491 37\n",
      "2810 37\n",
      "2810 37\n",
      "3820 37\n",
      "4385 37\n",
      "5016 37\n",
      "5080 37\n",
      "5080 37\n",
      "5347 37\n",
      "7056 37\n",
      "38 38\n",
      "1759 38\n",
      "39 39\n",
      "2128 39\n",
      "41 41\n",
      "7307 41\n",
      "42 42\n",
      "3737 42\n",
      "43 43\n",
      "3150 43\n",
      "44 44\n",
      "377 44\n",
      "1446 44\n",
      "1446 44\n",
      "4250 44\n",
      "4250 44\n",
      "4274 44\n",
      "4317 44\n",
      "4362 44\n",
      "4365 44\n",
      "4365 44\n",
      "5669 44\n",
      "5966 44\n",
      "6440 44\n",
      "6633 44\n",
      "7056 44\n",
      "7072 44\n",
      "45 45\n",
      "2421 45\n",
      "46 46\n",
      "49 46\n",
      "49 46\n",
      "218 46\n",
      "218 46\n",
      "1197 46\n",
      "1197 46\n",
      "1491 46\n",
      "1911 46\n",
      "1911 46\n",
      "2449 46\n",
      "2449 46\n",
      "2597 46\n",
      "2597 46\n",
      "2672 46\n",
      "2672 46\n",
      "3281 46\n",
      "3524 46\n",
      "3524 46\n",
      "3922 46\n",
      "4642 46\n",
      "4642 46\n",
      "4933 46\n",
      "5140 46\n",
      "5140 46\n",
      "5675 46\n",
      "5870 46\n",
      "5870 46\n",
      "6024 46\n",
      "6207 46\n",
      "6845 46\n",
      "7117 46\n",
      "7126 46\n",
      "7222 46\n",
      "7222 46\n",
      "8234 46\n",
      "47 47\n",
      "2013 47\n",
      "48 48\n",
      "1191 48\n",
      "2436 48\n",
      "2664 48\n",
      "3393 48\n",
      "3487 48\n",
      "3487 48\n",
      "3496 48\n",
      "3496 48\n",
      "4463 48\n",
      "5137 48\n",
      "5137 48\n",
      "5612 48\n",
      "6712 48\n",
      "5 49\n",
      "46 49\n",
      "46 49\n",
      "49 49\n",
      "218 49\n",
      "384 49\n",
      "962 49\n",
      "1911 49\n",
      "2308 49\n",
      "3259 49\n",
      "3313 49\n",
      "3313 49\n",
      "4362 49\n",
      "4652 49\n",
      "4933 49\n",
      "4933 49\n",
      "5675 49\n",
      "5675 49\n",
      "6326 49\n",
      "7056 49\n",
      "8234 49\n",
      "8234 49\n",
      "50 50\n",
      "2963 50\n",
      "51 51\n",
      "5297 51\n",
      "52 52\n",
      "838 52\n",
      "1339 52\n",
      "1934 52\n",
      "3578 52\n",
      "54 54\n",
      "4879 54\n",
      "55 55\n",
      "5017 55\n",
      "56 56\n",
      "3585 56\n",
      "57 57\n",
      "6097 57\n",
      "58 58\n",
      "828 58\n",
      "59 59\n",
      "319 59\n",
      "1668 59\n",
      "1690 59\n",
      "1690 59\n",
      "1911 59\n",
      "1995 59\n",
      "1995 59\n",
      "3561 59\n",
      "4274 59\n",
      "4933 59\n",
      "4933 59\n",
      "6677 59\n",
      "7056 59\n",
      "7808 59\n",
      "27 60\n",
      "60 60\n",
      "1934 60\n",
      "5989 60\n",
      "6147 60\n",
      "7463 60\n",
      "61 61\n",
      "377 61\n",
      "541 61\n",
      "554 61\n",
      "712 61\n",
      "915 61\n",
      "1196 61\n",
      "1206 61\n",
      "1399 61\n",
      "2695 61\n",
      "2695 61\n",
      "2897 61\n",
      "2897 61\n",
      "3324 61\n",
      "3677 61\n",
      "3677 61\n",
      "3747 61\n",
      "3820 61\n",
      "4274 61\n",
      "4320 61\n",
      "4320 61\n",
      "5584 61\n",
      "5799 61\n",
      "6645 61\n",
      "6645 61\n",
      "7056 61\n",
      "7243 61\n",
      "7243 61\n",
      "7739 61\n",
      "7739 61\n",
      "62 62\n",
      "377 62\n",
      "781 62\n",
      "781 62\n",
      "1395 62\n",
      "1683 62\n",
      "1683 62\n",
      "2031 62\n",
      "2031 62\n",
      "2313 62\n",
      "2313 62\n",
      "2660 62\n",
      "2660 62\n",
      "3019 62\n",
      "3019 62\n",
      "3197 62\n",
      "3657 62\n",
      "3657 62\n",
      "4362 62\n",
      "4526 62\n",
      "5695 62\n",
      "6142 62\n",
      "6265 62\n",
      "7056 62\n",
      "7926 62\n",
      "8027 62\n",
      "8027 62\n",
      "18 63\n",
      "18 63\n",
      "63 63\n",
      "377 63\n",
      "396 63\n",
      "396 63\n",
      "956 63\n",
      "1174 63\n",
      "1371 63\n",
      "1371 63\n",
      "1446 63\n",
      "1446 63\n",
      "1668 63\n",
      "2123 63\n",
      "2123 63\n",
      "2618 63\n",
      "2709 63\n",
      "2709 63\n",
      "4355 63\n",
      "4448 63\n",
      "5069 63\n",
      "5069 63\n",
      "5080 63\n",
      "5080 63\n",
      "6142 63\n",
      "6505 63\n",
      "6692 63\n",
      "7056 63\n",
      "64 64\n",
      "4513 64\n",
      "6825 64\n",
      "65 65\n",
      "413 65\n",
      "413 65\n",
      "5016 65\n",
      "7056 65\n",
      "7468 65\n",
      "66 66\n",
      "8203 66\n",
      "67 67\n",
      "1923 67\n",
      "68 68\n",
      "3054 68\n",
      "70 70\n",
      "1017 70\n",
      "71 71\n",
      "6810 71\n",
      "72 72\n",
      "735 72\n",
      "777 72\n",
      "1044 72\n",
      "1044 72\n",
      "1713 72\n",
      "1713 72\n",
      "2394 72\n",
      "2394 72\n",
      "2672 72\n",
      "3820 72\n",
      "4274 72\n",
      "4362 72\n",
      "4884 72\n",
      "6059 72\n",
      "6423 72\n",
      "7056 72\n",
      "73 73\n",
      "230 73\n",
      "377 73\n",
      "453 73\n",
      "541 73\n",
      "1131 73\n",
      "1131 73\n",
      "1557 73\n",
      "1557 73\n",
      "2690 73\n",
      "2690 73\n",
      "3324 73\n",
      "3677 73\n",
      "3677 73\n",
      "3776 73\n",
      "3776 73\n",
      "4011 73\n",
      "4268 73\n",
      "4268 73\n",
      "4320 73\n",
      "4320 73\n",
      "4525 73\n",
      "4525 73\n",
      "4617 73\n",
      "5667 73\n",
      "6312 73\n",
      "6312 73\n",
      "7056 73\n",
      "8073 73\n",
      "74 74\n",
      "541 74\n",
      "1911 74\n",
      "3298 74\n",
      "3530 74\n",
      "3728 74\n",
      "3728 74\n",
      "4923 74\n",
      "5775 74\n",
      "5775 74\n",
      "6015 74\n",
      "7056 74\n",
      "75 75\n",
      "838 75\n",
      "1934 75\n",
      "2805 75\n",
      "4748 75\n",
      "76 76\n",
      "4176 76\n",
      "77 77\n",
      "5712 77\n",
      "78 78\n",
      "6647 78\n",
      "79 79\n",
      "2406 79\n",
      "80 80\n",
      "3505 80\n",
      "81 81\n",
      "377 81\n",
      "741 81\n",
      "741 81\n",
      "1174 81\n",
      "1559 81\n",
      "2129 81\n",
      "2706 81\n",
      "2706 81\n",
      "3251 81\n",
      "4274 81\n",
      "5882 81\n",
      "7056 81\n",
      "7839 81\n",
      "82 82\n",
      "6438 82\n",
      "83 83\n",
      "2110 83\n",
      "84 84\n",
      "5879 84\n",
      "85 85\n",
      "787 85\n",
      "1934 85\n",
      "86 86\n",
      "6117 86\n",
      "87 87\n",
      "3355 87\n",
      "88 88\n",
      "4562 88\n",
      "89 89\n",
      "1181 89\n",
      "90 90\n",
      "113 90\n",
      "91 91\n",
      "1000 91\n",
      "92 92\n",
      "230 92\n",
      "377 92\n",
      "1395 92\n",
      "1455 92\n",
      "1455 92\n",
      "2445 92\n",
      "2445 92\n",
      "2618 92\n",
      "3786 92\n",
      "4037 92\n",
      "4250 92\n",
      "4250 92\n",
      "4362 92\n",
      "5014 92\n",
      "5584 92\n",
      "5962 92\n",
      "6467 92\n",
      "6798 92\n",
      "7056 92\n",
      "7391 92\n",
      "8079 92\n",
      "93 93\n",
      "6318 93\n",
      "94 94\n",
      "1346 94\n",
      "95 95\n",
      "5348 95\n",
      "96 96\n",
      "3829 96\n",
      "97 97\n",
      "6207 97\n",
      "7203 97\n",
      "98 98\n",
      "6977 98\n",
      "99 99\n",
      "5140 99\n",
      "100 100\n",
      "1934 100\n",
      "4034 100\n",
      "6536 100\n",
      "6536 100\n",
      "101 101\n",
      "2580 101\n",
      "102 102\n",
      "167 102\n",
      "103 103\n",
      "7508 103\n",
      "104 104\n",
      "6904 104\n",
      "105 105\n",
      "164 105\n",
      "1911 105\n",
      "1934 105\n",
      "2087 105\n",
      "2087 105\n",
      "2232 105\n",
      "2232 105\n",
      "2828 105\n",
      "3246 105\n",
      "3606 105\n",
      "4499 105\n",
      "5042 105\n",
      "5113 105\n",
      "5701 105\n",
      "5989 105\n",
      "6354 105\n",
      "7334 105\n",
      "7334 105\n",
      "7446 105\n",
      "7487 105\n",
      "7487 105\n",
      "106 106\n",
      "1384 106\n",
      "107 107\n",
      "4433 107\n",
      "108 108\n",
      "1385 108\n",
      "109 109\n",
      "4156 109\n",
      "110 110\n",
      "1329 110\n",
      "111 111\n",
      "291 111\n",
      "112 112\n",
      "230 112\n",
      "1421 112\n",
      "1476 112\n",
      "2229 112\n",
      "3393 112\n",
      "3393 112\n",
      "3554 112\n",
      "3606 112\n",
      "3685 112\n",
      "3685 112\n",
      "3820 112\n",
      "3903 112\n",
      "4044 112\n",
      "4362 112\n",
      "5140 112\n",
      "5525 112\n",
      "5525 112\n",
      "6205 112\n",
      "7056 112\n",
      "7117 112\n",
      "7117 112\n",
      "7264 112\n",
      "8213 112\n",
      "90 113\n",
      "113 113\n",
      "162 113\n",
      "162 113\n",
      "1750 113\n",
      "1934 113\n",
      "2554 113\n",
      "2554 113\n",
      "3406 113\n",
      "3406 113\n",
      "3408 113\n",
      "3496 113\n",
      "3496 113\n",
      "4412 113\n",
      "4412 113\n",
      "4608 113\n",
      "4608 113\n",
      "4610 113\n",
      "4610 113\n",
      "5140 113\n",
      "5701 113\n",
      "5989 113\n",
      "5989 113\n",
      "6205 113\n",
      "7085 113\n",
      "7085 113\n",
      "114 114\n",
      "7151 114\n",
      "115 115\n",
      "897 115\n",
      "116 116\n",
      "541 116\n",
      "3685 116\n",
      "3685 116\n",
      "5140 116\n",
      "6107 116\n",
      "6319 116\n",
      "6539 116\n",
      "7056 116\n",
      "117 117\n",
      "486 117\n",
      "118 118\n",
      "817 118\n",
      "119 119\n",
      "3181 119\n",
      "120 120\n",
      "1660 120\n",
      "3151 120\n",
      "121 121\n",
      "218 121\n",
      "770 121\n",
      "770 121\n",
      "962 121\n",
      "1911 121\n",
      "3313 121\n",
      "3313 121\n",
      "3329 121\n",
      "3365 121\n",
      "4362 121\n",
      "4933 121\n",
      "4933 121\n",
      "6721 121\n",
      "7056 121\n",
      "7059 121\n",
      "122 122\n",
      "461 122\n",
      "123 123\n",
      "377 123\n",
      "962 123\n",
      "1384 123\n",
      "1384 123\n",
      "1399 123\n",
      "3117 123\n",
      "3822 123\n",
      "3822 123\n",
      "4042 123\n",
      "4042 123\n",
      "4362 123\n",
      "4526 123\n",
      "5436 123\n",
      "5663 123\n",
      "5667 123\n",
      "6164 123\n",
      "6164 123\n",
      "6707 123\n",
      "6808 123\n",
      "7056 123\n",
      "124 124\n",
      "4788 124\n",
      "125 125\n",
      "1445 125\n",
      "126 126\n",
      "1703 126\n",
      "127 127\n",
      "3887 127\n",
      "128 128\n",
      "5378 128\n",
      "129 129\n",
      "1934 129\n",
      "4052 129\n",
      "5623 129\n",
      "5623 129\n",
      "130 130\n",
      "7556 130\n",
      "131 131\n",
      "3085 131\n",
      "132 132\n",
      "377 132\n",
      "396 132\n",
      "396 132\n",
      "541 132\n",
      "2729 132\n",
      "2746 132\n",
      "3996 132\n",
      "6107 132\n",
      "7056 132\n",
      "7196 132\n",
      "7196 132\n",
      "133 133\n",
      "3634 133\n",
      "134 134\n",
      "1269 134\n",
      "135 135\n",
      "2865 135\n",
      "5880 135\n",
      "136 136\n",
      "377 136\n",
      "1174 136\n",
      "1395 136\n",
      "1799 136\n",
      "1833 136\n",
      "2165 136\n",
      "2165 136\n",
      "2448 136\n",
      "2810 136\n",
      "2810 136\n",
      "5080 136\n",
      "5080 136\n",
      "6049 136\n",
      "6282 136\n",
      "6537 136\n",
      "7056 136\n",
      "7720 136\n",
      "8075 136\n",
      "8075 136\n",
      "137 137\n",
      "222 137\n",
      "441 137\n",
      "1010 137\n",
      "1138 137\n",
      "1778 137\n",
      "2128 137\n",
      "2249 137\n",
      "2352 137\n",
      "2603 137\n",
      "2995 137\n",
      "3133 137\n",
      "3147 137\n",
      "3147 137\n",
      "3496 137\n",
      "3496 137\n",
      "4070 137\n",
      "4403 137\n",
      "4463 137\n",
      "4793 137\n",
      "4861 137\n",
      "5186 137\n",
      "5511 137\n",
      "5612 137\n",
      "5649 137\n",
      "6695 137\n",
      "6822 137\n",
      "6907 137\n",
      "7269 137\n",
      "7578 137\n",
      "7729 137\n",
      "7843 137\n",
      "7933 137\n",
      "138 138\n",
      "1781 138\n",
      "139 139\n",
      "2165 139\n",
      "140 140\n",
      "4881 140\n",
      "141 141\n",
      "377 141\n",
      "2027 141\n",
      "2165 141\n",
      "2165 141\n",
      "4006 141\n",
      "4232 141\n",
      "4362 141\n",
      "5080 141\n",
      "5080 141\n",
      "7056 141\n",
      "142 142\n",
      "966 142\n",
      "1191 142\n",
      "2436 142\n",
      "4463 142\n",
      "5612 142\n",
      "143 143\n",
      "1054 143\n",
      "1934 143\n",
      "4581 143\n",
      "4581 143\n",
      "144 144\n",
      "6139 144\n",
      "145 145\n",
      "3533 145\n",
      "146 146\n",
      "2336 146\n",
      "147 147\n",
      "7461 147\n",
      "148 148\n",
      "3579 148\n",
      "149 149\n",
      "4017 149\n",
      "150 150\n",
      "2950 150\n",
      "3804 150\n",
      "6548 150\n",
      "151 151\n",
      "377 151\n",
      "962 151\n",
      "1148 151\n",
      "1148 151\n",
      "1741 151\n",
      "3751 151\n",
      "3998 151\n",
      "3998 151\n",
      "4192 151\n",
      "4192 151\n",
      "4362 151\n",
      "5667 151\n",
      "5895 151\n",
      "6683 151\n",
      "7056 151\n",
      "152 152\n",
      "7321 152\n",
      "153 153\n",
      "1174 153\n",
      "1655 153\n",
      "1901 153\n",
      "2672 153\n",
      "4274 153\n",
      "6163 153\n",
      "6260 153\n",
      "6260 153\n",
      "6433 153\n",
      "7056 153\n",
      "8115 153\n",
      "154 154\n",
      "2901 154\n",
      "3822 154\n",
      "4463 154\n",
      "5612 154\n",
      "5915 154\n",
      "7284 154\n",
      "155 155\n",
      "1324 155\n",
      "156 156\n",
      "7975 156\n",
      "157 157\n",
      "561 157\n",
      "158 158\n",
      "5852 158\n",
      "159 159\n",
      "2737 159\n",
      "160 160\n",
      "552 160\n",
      "161 161\n",
      "652 161\n",
      "652 161\n",
      "962 161\n",
      "997 161\n",
      "2564 161\n",
      "3046 161\n",
      "3069 161\n",
      "3613 161\n",
      "4362 161\n",
      "5496 161\n",
      "6467 161\n",
      "7056 161\n",
      "113 162\n",
      "113 162\n",
      "162 162\n",
      "2012 162\n",
      "2048 162\n",
      "3230 162\n",
      "3425 162\n",
      "3439 162\n",
      "3496 162\n",
      "3496 162\n",
      "4274 162\n",
      "4362 162\n",
      "5140 162\n",
      "6199 162\n",
      "7056 162\n",
      "7067 162\n",
      "7067 162\n",
      "7117 162\n",
      "7117 162\n",
      "163 163\n",
      "7975 163\n",
      "105 164\n",
      "164 164\n",
      "662 164\n",
      "1165 164\n",
      "1165 164\n",
      "1231 164\n",
      "1474 164\n",
      "1519 164\n",
      "1539 164\n",
      "1652 164\n",
      "1737 164\n",
      "1909 164\n",
      "1909 164\n",
      "2901 164\n",
      "2979 164\n",
      "3015 164\n",
      "3080 164\n",
      "3080 164\n",
      "3137 164\n",
      "3526 164\n",
      "3737 164\n",
      "4268 164\n",
      "4268 164\n",
      "4463 164\n",
      "4493 164\n",
      "4690 164\n",
      "4690 164\n",
      "4861 164\n",
      "5369 164\n",
      "5612 164\n",
      "5849 164\n",
      "6647 164\n",
      "6738 164\n",
      "6819 164\n",
      "6919 164\n",
      "6953 164\n",
      "7123 164\n",
      "7123 164\n",
      "7162 164\n",
      "7610 164\n",
      "7729 164\n",
      "8166 164\n",
      "165 165\n",
      "4166 165\n",
      "166 166\n",
      "911 166\n",
      "5999 166\n",
      "102 167\n",
      "167 167\n",
      "377 167\n",
      "377 167\n",
      "523 167\n",
      "523 167\n",
      "741 167\n",
      "741 167\n",
      "1274 167\n",
      "1274 167\n",
      "1507 167\n",
      "1507 167\n",
      "2233 167\n",
      "2233 167\n",
      "2618 167\n",
      "2618 167\n",
      "3577 167\n",
      "3577 167\n",
      "3786 167\n",
      "3786 167\n",
      "3942 167\n",
      "3942 167\n",
      "4032 167\n",
      "4032 167\n",
      "4365 167\n",
      "4365 167\n",
      "4474 167\n",
      "4474 167\n",
      "4818 167\n",
      "4818 167\n",
      "5667 167\n",
      "5667 167\n",
      "5941 167\n",
      "6207 167\n",
      "7148 167\n",
      "7148 167\n",
      "168 168\n",
      "8284 168\n",
      "169 169\n",
      "523 169\n",
      "523 169\n",
      "1152 169\n",
      "1152 169\n",
      "1158 169\n",
      "1158 169\n",
      "2447 169\n",
      "2447 169\n",
      "2629 169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2629 169\n",
      "2672 169\n",
      "2672 169\n",
      "3009 169\n",
      "3009 169\n",
      "3150 169\n",
      "3150 169\n",
      "3181 169\n",
      "3181 169\n",
      "3531 169\n",
      "3531 169\n",
      "3554 169\n",
      "3554 169\n",
      "4462 169\n",
      "4462 169\n",
      "6024 169\n",
      "6024 169\n",
      "6207 169\n",
      "6260 169\n",
      "6260 169\n",
      "7264 169\n",
      "7264 169\n",
      "7315 169\n",
      "7315 169\n",
      "7373 169\n",
      "7373 169\n",
      "7536 169\n",
      "7815 169\n",
      "7815 169\n",
      "170 170\n",
      "6478 170\n",
      "171 171\n",
      "5302 171\n",
      "172 172\n",
      "2090 172\n",
      "173 173\n",
      "5644 173\n",
      "174 174\n",
      "218 174\n",
      "1911 174\n",
      "1945 174\n",
      "2689 174\n",
      "2689 174\n",
      "3298 174\n",
      "4344 174\n",
      "4933 174\n",
      "4933 174\n",
      "6142 174\n",
      "6677 174\n",
      "7056 174\n",
      "7703 174\n",
      "7703 174\n",
      "8080 174\n",
      "175 175\n",
      "3765 175\n",
      "176 176\n",
      "441 176\n",
      "1010 176\n",
      "2130 176\n",
      "4463 176\n",
      "5612 176\n",
      "5649 176\n",
      "5796 176\n",
      "6781 176\n",
      "7578 176\n",
      "7699 176\n",
      "177 177\n",
      "1934 177\n",
      "4826 177\n",
      "4826 177\n",
      "4920 177\n",
      "4920 177\n",
      "5558 177\n",
      "178 178\n",
      "7643 178\n",
      "179 179\n",
      "377 179\n",
      "1174 179\n",
      "1655 179\n",
      "2074 179\n",
      "3197 179\n",
      "3512 179\n",
      "3998 179\n",
      "3998 179\n",
      "4250 179\n",
      "4250 179\n",
      "4274 179\n",
      "5080 179\n",
      "5080 179\n",
      "6049 179\n",
      "6633 179\n",
      "7056 179\n",
      "180 180\n",
      "1934 180\n",
      "6239 180\n",
      "6239 180\n",
      "6418 180\n",
      "182 182\n",
      "377 182\n",
      "529 182\n",
      "1395 182\n",
      "1399 182\n",
      "3298 182\n",
      "3998 182\n",
      "3998 182\n",
      "4362 182\n",
      "5569 182\n",
      "5667 182\n",
      "5726 182\n",
      "6164 182\n",
      "6164 182\n",
      "7056 182\n",
      "183 183\n",
      "1315 183\n",
      "184 184\n",
      "742 184\n",
      "185 185\n",
      "2265 185\n",
      "186 186\n",
      "452 186\n",
      "835 186\n",
      "875 186\n",
      "1251 186\n",
      "1251 186\n",
      "1503 186\n",
      "1540 186\n",
      "1616 186\n",
      "2689 186\n",
      "2752 186\n",
      "3526 186\n",
      "4282 186\n",
      "4282 186\n",
      "4463 186\n",
      "5137 186\n",
      "5137 186\n",
      "5169 186\n",
      "5612 186\n",
      "6155 186\n",
      "6730 186\n",
      "7254 186\n",
      "7510 186\n",
      "7655 186\n",
      "7805 186\n",
      "187 187\n",
      "377 187\n",
      "396 187\n",
      "396 187\n",
      "1456 187\n",
      "1668 187\n",
      "3033 187\n",
      "3033 187\n",
      "4362 187\n",
      "5887 187\n",
      "5936 187\n",
      "5936 187\n",
      "6575 187\n",
      "7056 187\n",
      "7196 187\n",
      "7196 187\n",
      "8282 187\n",
      "188 188\n",
      "1934 188\n",
      "3097 188\n",
      "5836 188\n",
      "5836 188\n",
      "189 189\n",
      "230 189\n",
      "270 189\n",
      "377 189\n",
      "1682 189\n",
      "2321 189\n",
      "2321 189\n",
      "2618 189\n",
      "2641 189\n",
      "2948 189\n",
      "3820 189\n",
      "4250 189\n",
      "4250 189\n",
      "4362 189\n",
      "5065 189\n",
      "5077 189\n",
      "5077 189\n",
      "5584 189\n",
      "6877 189\n",
      "7056 189\n",
      "7196 189\n",
      "7196 189\n",
      "7437 189\n",
      "7510 189\n",
      "7510 189\n",
      "8079 189\n",
      "190 190\n",
      "7141 190\n",
      "191 191\n",
      "377 191\n",
      "541 191\n",
      "2165 191\n",
      "2165 191\n",
      "6612 191\n",
      "6633 191\n",
      "7056 191\n",
      "7573 191\n",
      "7873 191\n",
      "192 192\n",
      "230 192\n",
      "1174 192\n",
      "2057 192\n",
      "2672 192\n",
      "2967 192\n",
      "2967 192\n",
      "6260 192\n",
      "6260 192\n",
      "6943 192\n",
      "6943 192\n",
      "7056 192\n",
      "7717 192\n",
      "193 193\n",
      "3434 193\n",
      "194 194\n",
      "2122 194\n",
      "4611 194\n",
      "195 195\n",
      "404 195\n",
      "1081 195\n",
      "196 196\n",
      "299 196\n",
      "299 196\n",
      "1860 196\n",
      "1934 196\n",
      "1990 196\n",
      "2369 196\n",
      "2369 196\n",
      "3246 196\n",
      "3486 196\n",
      "3905 196\n",
      "4517 196\n",
      "4581 196\n",
      "4581 196\n",
      "4698 196\n",
      "6166 196\n",
      "6239 196\n",
      "6239 196\n",
      "6556 196\n",
      "6556 196\n",
      "6822 196\n",
      "6822 196\n",
      "8057 196\n",
      "8232 196\n",
      "197 197\n",
      "1934 197\n",
      "6117 197\n",
      "6117 197\n",
      "6868 197\n",
      "198 198\n",
      "2813 198\n",
      "199 199\n",
      "2924 199\n",
      "200 200\n",
      "3845 200\n",
      "201 201\n",
      "752 201\n",
      "1944 201\n",
      "2672 201\n",
      "3126 201\n",
      "3298 201\n",
      "3820 201\n",
      "4201 201\n",
      "4334 201\n",
      "4334 201\n",
      "4362 201\n",
      "5320 201\n",
      "6024 201\n",
      "6024 201\n",
      "7056 201\n",
      "202 202\n",
      "6919 202\n",
      "203 203\n",
      "1606 203\n",
      "1606 203\n",
      "1934 203\n",
      "2104 203\n",
      "2104 203\n",
      "4477 203\n",
      "204 204\n",
      "3824 204\n",
      "4029 204\n",
      "205 205\n",
      "7256 205\n",
      "206 206\n",
      "1934 206\n",
      "3144 206\n",
      "7936 206\n",
      "7936 206\n",
      "207 207\n",
      "838 207\n",
      "4362 207\n",
      "4748 207\n",
      "208 208\n",
      "441 208\n",
      "209 209\n",
      "1024 209\n",
      "210 210\n",
      "6788 210\n",
      "211 211\n",
      "353 211\n",
      "353 211\n",
      "1934 211\n",
      "2272 211\n",
      "4868 211\n",
      "4868 211\n",
      "6067 211\n",
      "6067 211\n",
      "6617 211\n",
      "6617 211\n",
      "7354 211\n",
      "212 212\n",
      "377 212\n",
      "396 212\n",
      "396 212\n",
      "1942 212\n",
      "1942 212\n",
      "2095 212\n",
      "3159 212\n",
      "3822 212\n",
      "3822 212\n",
      "4320 212\n",
      "4320 212\n",
      "4362 212\n",
      "5080 212\n",
      "5080 212\n",
      "5936 212\n",
      "5936 212\n",
      "6575 212\n",
      "7056 212\n",
      "7196 212\n",
      "7196 212\n",
      "7288 212\n",
      "8075 212\n",
      "8075 212\n",
      "213 213\n",
      "5051 213\n",
      "214 214\n",
      "\n",
      "1768 218\n",
      "1841 218\n",
      "1841 218\n",
      "2022 218\n",
      "2090 218\n",
      "2090 218\n",
      "2342 218\n",
      "2520 218\n",
      "2689 218\n",
      "3003 218\n",
      "3295 218\n",
      "3313 218\n",
      "3313 218\n",
      "3454 218\n",
      "3470 218\n",
      "3585 218\n",
      "3728 218\n",
      "3931 218\n",
      "4334 218\n",
      "4463 218\n",
      "4642 218\n",
      "4801 218\n",
      "4933 218\n",
      "4967 218\n",
      "4999 218\n",
      "5071 218\n",
      "5071 218\n",
      "5080 218\n",
      "5089 218\n",
      "5163 218\n",
      "5236 218\n",
      "5286 218\n",
      "5337 218\n",
      "5425 218\n",
      "5612 218\n",
      "5870 218\n",
      "6024 218\n",
      "6094 218\n",
      "6112 218\n",
      "6283 218\n",
      "6295 218\n",
      "6340 218\n",
      "6527 218\n",
      "6527 218\n",
      "6805 218\n",
      "6826 218\n",
      "7055 218\n",
      "7100 218\n",
      "7126 218\n",
      "7196 218\n",
      "7222 218\n",
      "7421 218\n",
      "7511 218\n",
      "7535 218\n",
      "7592 218\n",
      "7634 218\n",
      "7703 218\n",
      "7703 218\n",
      "7786 218\n",
      "7948 218\n",
      "7975 218\n",
      "7975 218\n",
      "7988 218\n",
      "7988 218\n",
      "8095 218\n",
      "8195 218\n",
      "8231 218\n",
      "8234 218\n",
      "219 219\n",
      "377 219\n",
      "621 219\n",
      "621 219\n",
      "712 219\n",
      "799 219\n",
      "2568 219\n",
      "3128 219\n",
      "3128 219\n",
      "3822 219\n",
      "3822 219\n",
      "4362 219\n",
      "4365 219\n",
      "4365 219\n",
      "5077 219\n",
      "5077 219\n",
      "5978 219\n",
      "6142 219\n",
      "6319 219\n",
      "7056 219\n",
      "8037 219\n",
      "220 220\n",
      "5995 220\n",
      "221 221\n",
      "6735 221\n",
      "137 222\n",
      "222 222\n",
      "377 222\n",
      "441 222\n",
      "441 222\n",
      "1831 222\n",
      "2462 222\n",
      "3170 222\n",
      "3574 222\n",
      "3574 222\n",
      "3822 222\n",
      "3822 222\n",
      "4183 222\n",
      "4362 222\n",
      "4921 222\n",
      "6142 222\n",
      "7056 222\n",
      "223 223\n",
      "3175 223\n",
      "224 224\n",
      "7618 224\n",
      "225 225\n",
      "5645 225\n",
      "226 226\n",
      "838 226\n",
      "1051 226\n",
      "2716 226\n",
      "227 227\n",
      "2661 227\n",
      "228 228\n",
      "5897 228\n",
      "229 229\n",
      "3295 229\n",
      "73 230\n",
      "92 230\n",
      "112 230\n",
      "189 230\n",
      "192 230\n",
      "230 230\n",
      "260 230\n",
      "268 230\n",
      "397 230\n",
      "404 230\n",
      "414 230\n",
      "432 230\n",
      "440 230\n",
      "460 230\n",
      "473 230\n",
      "527 230\n",
      "552 230\n",
      "561 230\n",
      "573 230\n",
      "742 230\n",
      "833 230\n",
      "896 230\n",
      "919 230\n",
      "1025 230\n",
      "1081 230\n",
      "1084 230\n",
      "1111 230\n",
      "1119 230\n",
      "1151 230\n",
      "1158 230\n",
      "1257 230\n",
      "1411 230\n",
      "1505 230\n",
      "1526 230\n",
      "1534 230\n",
      "1539 230\n",
      "1547 230\n",
      "1598 230\n",
      "1652 230\n",
      "1747 230\n",
      "1778 230\n",
      "1820 230\n",
      "1843 230\n",
      "1867 230\n",
      "1880 230\n",
      "1919 230\n",
      "1923 230\n",
      "1981 230\n",
      "2032 230\n",
      "2036 230\n",
      "2061 230\n",
      "2087 230\n",
      "2121 230\n",
      "2130 230\n",
      "2159 230\n",
      "2173 230\n",
      "2245 230\n",
      "2278 230\n",
      "2286 230\n",
      "2318 230\n",
      "2359 230\n",
      "2515 230\n",
      "2554 230\n",
      "2589 230\n",
      "2749 230\n",
      "2771 230\n",
      "2845 230\n",
      "2901 230\n",
      "2905 230\n",
      "2914 230\n",
      "2950 230\n",
      "3009 230\n",
      "3015 230\n",
      "3060 230\n",
      "3133 230\n",
      "3184 230\n",
      "3190 230\n",
      "3286 230\n",
      "3306 230\n",
      "3323 230\n",
      "3380 230\n",
      "3453 230\n",
      "3507 230\n",
      "3527 230\n",
      "3640 230\n",
      "3732 230\n",
      "3733 230\n",
      "3752 230\n",
      "3804 230\n",
      "3836 230\n",
      "3837 230\n",
      "4070 230\n",
      "4087 230\n",
      "4144 230\n",
      "4162 230\n",
      "4176 230\n",
      "4220 230\n",
      "4256 230\n",
      "4399 230\n",
      "4403 230\n",
      "4412 230\n",
      "4497 230\n",
      "4504 230\n",
      "4518 230\n",
      "4542 230\n",
      "4702 230\n",
      "4738 230\n",
      "4759 230\n",
      "4786 230\n",
      "4788 230\n",
      "4790 230\n",
      "4826 230\n",
      "4861 230\n",
      "4868 230\n",
      "4937 230\n",
      "5028 230\n",
      "5051 230\n",
      "5086 230\n",
      "5104 230\n",
      "5110 230\n",
      "5126 230\n",
      "5127 230\n",
      "5186 230\n",
      "5203 230\n",
      "5277 230\n",
      "5297 230\n",
      "5369 230\n",
      "5378 230\n",
      "5449 230\n",
      "5451 230\n",
      "5527 230\n",
      "5566 230\n",
      "5641 230\n",
      "5649 230\n",
      "5668 230\n",
      "5676 230\n",
      "5687 230\n",
      "5713 230\n",
      "5849 230\n",
      "5888 230\n",
      "5939 230\n",
      "5943 230\n",
      "5980 230\n",
      "5999 230\n",
      "6004 230\n",
      "6094 230\n",
      "6128 230\n",
      "6191 230\n",
      "6239 230\n",
      "6349 230\n",
      "6355 230\n",
      "6364 230\n",
      "6431 230\n",
      "6432 230\n",
      "6485 230\n",
      "6491 230\n",
      "6493 230\n",
      "6508 230\n",
      "6548 230\n",
      "6562 230\n",
      "6712 230\n",
      "6738 230\n",
      "6773 230\n",
      "6819 230\n",
      "6825 230\n",
      "6897 230\n",
      "6919 230\n",
      "6936 230\n",
      "7034 230\n",
      "7043 230\n",
      "7045 230\n",
      "7161 230\n",
      "7186 230\n",
      "7334 230\n",
      "7432 230\n",
      "7487 230\n",
      "7491 230\n",
      "7521 230\n",
      "7556 230\n",
      "7779 230\n",
      "7801 230\n",
      "7856 230\n",
      "7918 230\n",
      "7921 230\n",
      "7942 230\n",
      "8000 230\n",
      "8166 230\n",
      "8259 230\n",
      "8271 230\n",
      "8274 230\n",
      "231 231\n",
      "4134 231\n",
      "232 232\n",
      "2374 232\n",
      "233 233\n",
      "2619 233\n",
      "234 234\n",
      "1877 234\n",
      "235 235\n",
      "5943 235\n",
      "236 236\n",
      "473 236\n",
      "6493 236\n",
      "237 237\n",
      "2098 237\n",
      "238 238\n",
      "1934 238\n",
      "2882 238\n",
      "5310 238\n",
      "7521 238\n",
      "7521 238\n",
      "239 239\n",
      "1934 239\n",
      "2783 239\n",
      "2783 239\n",
      "6927 239\n",
      "240 240\n",
      "4920 240\n",
      "241 241\n",
      "7495 241\n",
      "242 242\n",
      "7113 242\n",
      "243 243\n",
      "6562 243\n",
      "244 244\n",
      "6037 244\n",
      "245 245\n",
      "2924 245\n",
      "246 246\n",
      "2818 246\n",
      "247 247\n",
      "2026 247\n",
      "248 248\n",
      "2331 248\n",
      "249 249\n",
      "3416 249\n",
      "218 250\n",
      "218 250\n",
      "250 250\n",
      "252 250\n",
      "919 250\n",
      "919 250\n",
      "1911 250\n",
      "1911 250\n",
      "2689 250\n",
      "3141 250\n",
      "3344 250\n",
      "5286 250\n",
      "5286 2506142 266\n",
      "6887 266\n",
      "6887 266\n",
      "7056 266\n",
      "8014 266\n",
      "267 267\n",
      "414 267\n",
      "414 267\n",
      "1934 267\n",
      "7525 267\n",
      "230 268\n",
      "268 268\n",
      "377 268\n",
      "741 268\n",
      "741 268\n",
      "2118 268\n",
      "2314 268\n",
      "2584 268\n",
      "2706 268\n",
      "2706 268\n",
      "4362 268\n",
      "4923 268\n",
      "6126 268\n",
      "6126 268\n",
      "7056 268\n",
      "269 269\n",
      "386 269\n",
      "189 270\n",
      "270 270\n",
      "271 271\n",
      "6735 271\n",
      "272 272\n",
      "319 272\n",
      "702 272\n",
      "702 272\n",
      "1668 272\n",
      "1860 272\n",
      "1911 272\n",
      "3750 272\n",
      "3750 272\n",
      "4199 272\n",
      "4274 272\n",
      "4362 272\n",
      "4819 272\n",
      "7056 272\n",
      "7336 272\n",
      "273 273\n",
      "4352 273\n",
      "274 274\n",
      "948 274\n",
      "948 274\n",
      "1934 274\n",
      "3221 274\n",
      "275 275\n",
      "377 275\n",
      "1252 275\n",
      "1384 275\n",
      "1384 275\n",
      "2392 275\n",
      "6132 275\n",
      "6319 275\n",
      "6575 275\n",
      "7056 275\n",
      "276 276\n",
      "4920 276\n",
      "33 277\n",
      "277 277\n",
      "312 277\n",
      "864 277\n",
      "1616 277\n",
      "1623 277\n",
      "4025 277\n",
      "4373 277\n",
      "7582 277\n",
      "278 278\n",
      "6850 278\n",
      "279 279\n",
      "1495 279\n",
      "280 280\n",
      "396 280\n",
      "281 281\n",
      "1981 281\n",
      "282 282\n",
      "6738 282\n",
      "283 283\n",
      "4821 283\n",
      "284 284\n",
      "6139 284\n",
      "285 285\n",
      "958 285\n",
      "286 286\n",
      "869 286\n",
      "3100 286\n",
      "7835 286\n",
      "287 287\n",
      "1048 287\n",
      "288 288\n",
      "1118 288\n",
      "1118 288\n",
      "1825 288\n",
      "1934 288\n",
      "3935 288\n",
      "3935 288\n",
      "6019 288\n",
      "6900 288\n",
      "6900 288\n",
      "289 289\n",
      "603 289\n",
      "290 290\n",
      "2309 290\n",
      "111 291\n",
      "291 291\n",
      "377 291\n",
      "1399 291\n",
      "1628 291\n",
      "2618 291\n",
      "3197 291\n",
      "3298 291\n",
      "4123 291\n",
      "4320 291\n",
      "4320 291\n",
      "4811 291\n",
      "5037 291\n",
      "5037 291\n",
      "5080 291\n",
      "5080 291\n",
      "5247 291\n",
      "5584 291\n",
      "5677 291\n",
      "6467 291\n",
      "7056 291\n",
      "7196 291\n",
      "7196 291\n",
      "8197 291\n",
      "292 292\n",
      "1934 292\n",
      "5989 292\n",
      "5989 292\n",
      "8226 292\n",
      "293 293\n",
      "6119 293\n",
      "294 294\n",
      "1934 294\n",
      "3060 294\n",
      "3060 294\n",
      "3130 294\n",
      "4738 294\n",
      "4738 294\n",
      "5474 294\n",
      "5474 294\n",
      "8274 294\n",
      "8274 294\n",
      "295 295\n",
      "4362 295\n",
      "4764 295\n",
      "6957 295\n",
      "7873 295\n",
      "296 296\n",
      "7399 296\n",
      "8027 296\n",
      "297 297\n",
      "1151 297\n",
      "1491 297\n",
      "2520 297\n",
      "3313 297\n",
      "3313 297\n",
      "3931 297\n",
      "4463 297\n",
      "5293 297\n",
      "5612 297\n",
      "6527 297\n",
      "6527 297\n",
      "7511 297\n",
      "7535 297\n",
      "7988 297\n",
      "7988 297\n",
      "8234 297\n",
      "298 298\n",
      "5257 298\n",
      "196 299\n",
      "196 299\n",
      "299 299\n",
      "377 299\n",
      "2641 299\n",
      "3351 299\n",
      "3822 299\n",
      "3822 299\n",
      "4250 299\n",
      "4250 299\n",
      "4268 299\n",
      "4268 299\n",
      "4274 299\n",
      "4320 299\n",
      "4320 299\n",
      "4362 299\n",
      "4365 299\n",
      "4365 299\n",
      "5065 299\n",
      "5667 299\n",
      "5826 299\n",
      "5966 299\n",
      "6633 299\n",
      "7056 299\n",
      "7072 299\n",
      "7691 299\n",
      "300 300\n",
      "1462 300\n",
      "1934 300\n",
      "4581 300\n",
      "4581 300\n",
      "5013 300\n",
      "5013 300\n",
      "7955 300\n",
      "7955 300\n",
      "302 302\n",
      "377 302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541 302\n",
      "1131 302\n",
      "1131 302\n",
      "1380 302\n",
      "2206 302\n",
      "2712 302\n",
      "2712 302\n",
      "3324 302\n",
      "3624 302\n",
      "3624 302\n",
      "3626 302\n",
      "3677 302\n",
      "3677 302\n",
      "3916 302\n",
      "3916 302\n",
      "3942 302\n",
      "4032 302\n",
      "4032 302\n",
      "4881 302\n",
      "4881 302\n",
      "5667 302\n",
      "6142 302\n",
      "6346 302\n",
      "6346 302\n",
      "6909 302\n",
      "6909 302\n",
      "7056 302\n",
      "7934 302\n",
      "8194 302\n",
      "303 303\n",
      "377 303\n",
      "1174 303\n",
      "1344 303\n",
      "2165 303\n",
      "2165 303\n",
      "2235 303\n",
      "3688 303\n",
      "3722 303\n",
      "7056 303\n",
      "304 304\n",
      "1686 304\n",
      "1686 304\n",
      "1934 304\n",
      "3660 304\n",
      "3660 304\n",
      "7106 304\n",
      "305 305\n",
      "3845 305\n",
      "4463 305\n",
      "4959 305\n",
      "5612 305\n",
      "306 306\n",
      "838 306\n",
      "4748 306\n",
      "5622 306\n",
      "6207 306\n",
      "307 307\n",
      "4296 307\n",
      "308 308\n",
      "8234 308\n",
      "309 309\n",
      "6496 309\n",
      "310 310\n",
      "6518 310\n",
      "311 311\n",
      "4099 311\n",
      "277 312\n",
      "312 312\n",
      "377 312\n",
      "396 312\n",
      "396 312\n",
      "428 312\n",
      "428 312\n",
      "1174 312\n",
      "1622 312\n",
      "2165 312\n",
      "2165 312\n",
      "3071 312\n",
      "3298 312\n",
      "4044 312\n",
      "5009 312\n",
      "5009 312\n",
      "5047 312\n",
      "5443 312\n",
      "5443 312\n",
      "6889 312\n",
      "7056 312\n",
      "7196 312\n",
      "7196 312\n",
      "7519 312\n",
      "7519 312\n",
      "8250 312\n",
      "313 313\n",
      "1770 313\n",
      "1770 313\n",
      "1934 313\n",
      "6584 313\n",
      "315 315\n",
      "6069 315\n",
      "316 316\n",
      "377 316\n",
      "1395 316\n",
      "1446 316\n",
      "1446 316\n",
      "2656 316\n",
      "3298 316\n",
      "3998 316\n",
      "3998 316\n",
      "4362 316\n",
      "5080 316\n",
      "5080 316\n",
      "6164 316\n",
      "6164 316\n",
      "6799 316\n",
      "7032 316\n",
      "7056 316\n",
      "8081 316\n",
      "317 317\n",
      "1010 317\n",
      "318 318\n",
      "6024 318\n",
      "59 319\n",
      "272 319\n",
      "319 319\n",
      "479 319\n",
      "1111 319\n",
      "1690 319\n",
      "1690 319\n",
      "1770 319\n",
      "1961 319\n",
      "1995 319\n",
      "2159 319\n",
      "3083 319\n",
      "3555 319\n",
      "3750 319\n",
      "3975 319\n",
      "4220 319\n",
      "4463 319\n",
      "4678 319\n",
      "5051 319\n",
      "5136 319\n",
      "5358 319\n",
      "5571 319\n",
      "5612 319\n",
      "5999 319\n",
      "6904 319\n",
      "8039 319\n",
      "320 320\n",
      "670 320\n",
      "948 320\n",
      "1160 320\n",
      "1249 320\n",
      "2425 320\n",
      "4245 320\n",
      "4266 320\n",
      "4426 320\n",
      "4463 320\n",
      "4675 320\n",
      "4984 320\n",
      "4362 344\n",
      "4365 344\n",
      "4365 344\n",
      "4454 344\n",
      "4454 344\n",
      "5065 344\n",
      "5584 344\n",
      "5667 344\n",
      "5799 344\n",
      "7056 344\n",
      "7123 344\n",
      "7123 344\n",
      "7148 344\n",
      "7838 344\n",
      "7934 344\n",
      "7979 344\n",
      "8079 344\n",
      "8210 344\n",
      "345 345\n",
      "611 345\n",
      "346 346\n",
      "377 346\n",
      "441 346\n",
      "441 346\n",
      "1387 346\n",
      "1387 346\n",
      "1404 346\n",
      "1404 346\n",
      "1681 346\n",
      "1681 346\n",
      "2462 346\n",
      "2618 346\n",
      "3192 346\n",
      "3192 346\n",
      "3366 346\n",
      "3366 346\n",
      "3574 346\n",
      "3574 346\n",
      "3786 346\n",
      "3807 346\n",
      "3807 346\n",
      "4362 346\n",
      "5459 346\n",
      "5459 346\n",
      "5584 346\n",
      "6142 346\n",
      "6895 346\n",
      "6912 346\n",
      "7056 346\n",
      "7148 346\n",
      "7196 346\n",
      "7196 346\n",
      "7531 346\n",
      "347 347\n",
      "2356 347\n",
      "348 348\n",
      "377 348\n",
      "1221 348\n",
      "1446 348\n",
      "1446 348\n",
      "3687 348\n",
      "4139 348\n",
      "4362 348\n",
      "5080 348\n",
      "5080 348\n",
      "6107 348\n",
      "7056 348\n",
      "7196 348\n",
      "7196 348\n",
      "349 349\n",
      "2801 349\n",
      "350 350\n",
      "6174 350\n",
      "351 351\n",
      "6191 351\n",
      "352 352\n",
      "6590 352\n",
      "211 353\n",
      "211 353\n",
      "353 353\n",
      "377 353\n",
      "523 353\n",
      "1668 353\n",
      "3156 353\n",
      "3677 353\n",
      "3677 353\n",
      "4032 353\n",
      "4032 353\n",
      "4129 353\n",
      "4250 353\n",
      "4250 353\n",
      "4274 353\n",
      "4633 353\n",
      "4633 353\n",
      "5460 353\n",
      "5460 353\n",
      "6677 353\n",
      "7056 353\n",
      "354 354\n",
      "1934 354\n",
      "2036 354\n",
      "2036 354\n",
      "2486 354\n",
      "2486 354\n",
      "3525 354\n",
      "3850 354\n",
      "3850 354\n",
      "355 355\n",
      "2912 355\n",
      "356 356\n",
      "4717 356\n",
      "357 357\n",
      "734 357\n",
      "358 358\n",
      "8126 358\n",
      "359 359\n",
      "7857 359\n",
      "360 360\n",
      "6009 360\n",
      "361 361\n",
      "2924 361\n",
      "7815 361\n",
      "362 362\n",
      "2609 362\n",
      "363 363\n",
      "5641 363\n",
      "364 364\n",
      "2436 364\n",
      "4296 364\n",
      "365 365\n",
      "5210 365\n",
      "366 366\n",
      "6343 366\n",
      "367 367\n",
      "2717 367\n",
      "368 368\n",
      "1072 368\n",
      "1072 368\n",
      "1934 368\n",
      "4581 368\n",
      "4581 368\n",
      "6323 368\n",
      "369 369\n",
      "1934 369\n",
      "2669 369\n",
      "370 370\n",
      "639 370\n",
      "1934 370\n",
      "6000 370\n",
      "6000 370\n",
      "6536 370\n",
      "6536 370\n",
      "371 371\n",
      "4223 371\n",
      "7056 371\n",
      "372 372\n",
      "1091 372\n",
      "1091 372\n",
      "1594 372\n",
      "1934 372\n",
      "4418 372\n",
      "4418 372\n",
      "7251 372\n",
      "7251 372\n",
      "373 373\n",
      "3002 373\n",
      "3101 373\n",
      "5932 373\n",
      "375 375\n",
      "1915 375\n",
      "1934 375\n",
      "4702 375\n",
      "4702 375\n",
      "376 376\n",
      "377 376\n",
      "1411 376\n",
      "1411 376\n",
      "1504 376\n",
      "1505 376\n",
      "1505 376\n",
      "1843 376\n",
      "1843 376\n",
      "1934 376\n",
      "2264 376\n",
      "2264 376\n",
      "2515 376\n",
      "2515 376\n",
      "2618 376\n",
      "2915 376\n",
      "3678 376\n",
      "3894 376\n",
      "3894 376\n",
      "5127 376\n",
      "5127 376\n",
      "5165 376\n",
      "5297 376\n",
      "5297 376\n",
      "5701 376\n",
      "5713 376\n",
      "5713 376\n",
      "6285 376\n",
      "6285 376\n",
      "6364 376\n",
      "6364 376\n",
      "6829 376\n",
      "6829 376\n",
      "7148 376\n",
      "7752 376\n",
      "4 377\n",
      "8 377\n",
      "37 377\n",
      "44 377\n",
      "61 377\n",
      "62 377\n",
      "63 377\n",
      "73 377\n",
      "81 377\n",
      "92 377\n",
      "123 377\n",
      "132 377\n",
      "136 377\n",
      "141 377\n",
      "151 377\n",
      "167 377\n",
      "167 377\n",
      "179 377\n",
      "182 377\n",
      "187 377\n",
      "189 377\n",
      "191 377\n",
      "212 377\n",
      "219 377\n",
      "222 377\n",
      "260 377\n",
      "266 377\n",
      "268 377\n",
      "275 377\n",
      "291 377\n",
      "299 377\n",
      "302 377\n",
      "303 377\n",
      "312 377\n",
      "316 377\n",
      "324 377\n",
      "335 377\n",
      "344 377\n",
      "346 377\n",
      "348 377\n",
      "353 377\n",
      "376 377\n",
      "377 377\n",
      "386 377\n",
      "388 377\n",
      "394 377\n",
      "401 377\n",
      "404 377\n",
      "411 377\n",
      "415 377\n",
      "423 377\n",
      "431 377\n",
      "432 377\n",
      "440 377\n",
      "441 377\n",
      "445 377\n",
      "452 377\n",
      "461 377\n",
      "472 377\n",
      "473 377\n",
      "493 377\n",
      "495 377\n",
      "498 377\n",
      "505 377\n",
      "514 377\n",
      "517 377\n",
      "517 377\n",
      "527 377\n",
      "530 377\n",
      "540 377\n",
      "547 377\n",
      "550 377\n",
      "552 377\n",
      "561 377\n",
      "562 377\n",
      "602 377\n",
      "603 377\n",
      "605 377\n",
      "617 377\n",
      "622 377\n",
      "645 377\n",
      "661 377\n",
      "662 377\n",
      "663 377\n",
      "675 377\n",
      "681 377\n",
      "688 377\n",
      "690 377\n",
      "701 377\n",
      "701 377\n",
      "720 377\n",
      "733 377\n",
      "733 377\n",
      "741 377\n",
      "742 377\n",
      "749 377\n",
      "756 377\n",
      "759 377\n",
      "773 377\n",
      "781 377\n",
      "797 377\n",
      "798 377\n",
      "805 377\n",
      "807 377\n",
      "809 377\n",
      "815 377\n",
      "817 377\n",
      "828 377\n",
      "844 377\n",
      "848 377\n",
      "861 377\n",
      "864 377\n",
      "869 377\n",
      "886 377\n",
      "894 377\n",
      "896 377\n",
      "914 377\n",
      "916 377\n",
      "933 377\n",
      "936 377\n",
      "961 377\n",
      "964 377\n",
      "974 377\n",
      "1000 377\n",
      "1008 377\n",
      "1010 377\n",
      "1015 377\n",
      "1017 377\n",
      "1020 377\n",
      "1029 377\n",
      "1036 377\n",
      "1048 377\n",
      "1057 377\n",
      "1064 377\n",
      "1072 377\n",
      "1076 377\n",
      "1078 377\n",
      "1081 377\n",
      "1084 377\n",
      "1090 377\n",
      "1091 377\n",
      "1118 377\n",
      "1119 377\n",
      "1133 377\n",
      "1139 377\n",
      "1141 377\n",
      "1165 377\n",
      "1165 377\n",
      "1170 377\n",
      "1198 377\n",
      "1205 377\n",
      "1205 377\n",
      "1222 377\n",
      "1231 377\n",
      "1233 377\n",
      "1239 377\n",
      "1243 377\n",
      "1244 377\n",
      "1257 377\n",
      "1274 377\n",
      "1280 377\n",
      "1284 377\n",
      "1319 377\n",
      "1325 377\n",
      "1345 377\n",
      "1346 377\n",
      "1353 377\n",
      "1385 377\n",
      "1396 377\n",
      "1398 377\n",
      "1400 377\n",
      "1411 377\n",
      "1433 377\n",
      "1435 377\n",
      "1440 377\n",
      "1441 377\n",
      "1457 377\n",
      "1469 377\n",
      "1474 377\n",
      "1484 377\n",
      "1494 377\n",
      "1495 377\n",
      "1498 377\n",
      "1505 377\n",
      "1512 377\n",
      "1517 377\n",
      "1523 377\n",
      "1526 377\n",
      "1534 377\n",
      "1538 377\n",
      "1539 377\n",
      "1543 377\n",
      "1547 377\n",
      "1549 377\n",
      "1564 377\n",
      "1569 377\n",
      "1591 377\n",
      "1591 377\n",
      "1598 377\n",
      "1621 377\n",
      "1623 377\n",
      "1646 377\n",
      "1652 377\n",
      "1657 377\n",
      "1660 377\n",
      "1663 377\n",
      "1673 377\n",
      "1686 377\n",
      "1688 377\n",
      "1703 377\n",
      "1707 377\n",
      "1711 377\n",
      "1724 377\n",
      "1727 377\n",
      "1729 377\n",
      "1737 377\n",
      "1747 377\n",
      "1762 377\n",
      "1763 377\n",
      "1771 377\n",
      "1773 377\n",
      "1778 377\n",
      "1781 377\n",
      "1787 377\n",
      "1789 377\n",
      "1805 377\n",
      "1819 377\n",
      "1829 377\n",
      "1843 377\n",
      "1853 377\n",
      "1868 377\n",
      "1875 377\n",
      "1877 377\n",
      "1878 377\n",
      "1909 377\n",
      "1909 377\n",
      "1923 377\n",
      "1926 377\n",
      "1937 377\n",
      "1946 377\n",
      "1956 377\n",
      "1963 377\n",
      "1963 377\n",
      "1967 377\n",
      "1979 377\n",
      "1981 377\n",
      "1993 377\n",
      "1996 377\n",
      "2013 377\n",
      "2015 377\n",
      "2021 377\n",
      "2023 377\n",
      "2026 377\n",
      "2032 377\n",
      "2036 377\n",
      "2044 377\n",
      "2054 377\n",
      "2055 377\n",
      "2061 377\n",
      "2073 377\n",
      "2088 377\n",
      "2090 377\n",
      "2090 377\n",
      "2096 377\n",
      "2098 377\n",
      "2120 377\n",
      "2127 377\n",
      "2128 377\n",
      "2130 377\n",
      "2148 377\n",
      "2163 377\n",
      "2173 377\n",
      "2182 377\n",
      "2189 377\n",
      "2199 377\n",
      "2220 377\n",
      "2223 377\n",
      "2227 377\n",
      "2236 377\n",
      "2237 377\n",
      "2245 377\n",
      "2249 377\n",
      "2264 377\n",
      "2264 377\n",
      "2268 377\n",
      "2271 377\n",
      "2279 377\n",
      "2307 377\n",
      "2309 377\n",
      "2318 377\n",
      "2328 377\n",
      "2328 377\n",
      "2338 377\n",
      "2338 377\n",
      "2342 377\n",
      "2352 377\n",
      "2361 377\n",
      "2369 377\n",
      "2372 377\n",
      "2374 377\n",
      "2376 377\n",
      "2393 377\n",
      "2393 377\n",
      "2403 377\n",
      "2421 377\n",
      "2423 377\n",
      "2434 377\n",
      "2439 377\n",
      "2445 377\n",
      "2455 377\n",
      "2456 377\n",
      "2467 377\n",
      "2470 377\n",
      "2475 377\n",
      "2476 377\n",
      "2486 377\n",
      "2503 377\n",
      "2503 377\n",
      "2505 377\n",
      "2507 377\n",
      "2515 377\n",
      "2525 377\n",
      "2529 377\n",
      "2558 377\n",
      "2559 377\n",
      "2589 377\n",
      "2601 377\n",
      "2602 377\n",
      "2603 377\n",
      "2609 377\n",
      "2616 377\n",
      "2639 377\n",
      "2648 377\n",
      "2658 377\n",
      "2661 377\n",
      "2685 377\n",
      "2692 377\n",
      "2694 377\n",
      "2700 377\n",
      "2727 377\n",
      "2732 377\n",
      "2737 377\n",
      "2752 377\n",
      "2766 377\n",
      "2771 377\n",
      "2784 377\n",
      "2790 377\n",
      "2801 377\n",
      "2803 377\n",
      "2808 377\n",
      "2818 377\n",
      "2845 377\n",
      "2848 377\n",
      "2860 377\n",
      "2866 377\n",
      "2867 377\n",
      "2867 377\n",
      "2884 377\n",
      "2886 377\n",
      "2889 377\n",
      "2891 377\n",
      "2901 377\n",
      "2912 377\n",
      "2914 377\n",
      "2919 377\n",
      "2920 377\n",
      "2929 377\n",
      "2937 377\n",
      "2946 377\n",
      "2950 377\n",
      "2963 377\n",
      "2966 377\n",
      "2971 377\n",
      "2979 377\n",
      "2982 377\n",
      "2995 377\n",
      "3005 377\n",
      "3006 377\n",
      "3007 377\n",
      "3015 377\n",
      "3019 377\n",
      "3019 377\n",
      "3026 377\n",
      "3034 377\n",
      "3035 377\n",
      "3058 377\n",
      "3074 377\n",
      "3080 377\n",
      "3080 377\n",
      "3085 377\n",
      "3100 377\n",
      "3108 377\n",
      "3109 377\n",
      "3128 377\n",
      "3129 377\n",
      "3133 377\n",
      "3137 377\n",
      "3145 377\n",
      "3147 377\n",
      "3147 377\n",
      "3148 377\n",
      "3149 377\n",
      "3151 377\n",
      "3164 377\n",
      "3175 377\n",
      "3184 377\n",
      "3190 377\n",
      "3207 377\n",
      "3214 377\n",
      "3224 377\n",
      "3254 377\n",
      "3276 377\n",
      "3303 377\n",
      "3306 377\n",
      "3311 377\n",
      "3315 377\n",
      "3316 377\n",
      "3316 377\n",
      "3355 377\n",
      "3371 377\n",
      "3395 377\n",
      "3398 377\n",
      "3401 377\n",
      "3404 377\n",
      "3413 377\n",
      "3416 377\n",
      "3418 377\n",
      "3433 377\n",
      "3449 377\n",
      "3463 377\n",
      "3471 377\n",
      "3482 377\n",
      "3502 377\n",
      "3505 377\n",
      "3511 377\n",
      "3521 377\n",
      "3526 377\n",
      "3527 377\n",
      "3533 377\n",
      "3541 377\n",
      "3567 377\n",
      "3574 377\n",
      "3574 377\n",
      "3579 377\n",
      "3584 377\n",
      "3591 377\n",
      "3609 377\n",
      "3620 377\n",
      "3625 377\n",
      "3640 377\n",
      "3642 377\n",
      "3660 377\n",
      "3676 377\n",
      "3677 377\n",
      "3677 377\n",
      "3699 377\n",
      "3703 377\n",
      "3704 377\n",
      "3713 377\n",
      "3717 377\n",
      "3732 377\n",
      "3737 377\n",
      "3739 377\n",
      "3755 377\n",
      "3765 377\n",
      "3769 377\n",
      "3771 377\n",
      "3777 377\n",
      "3779 377\n",
      "3798 377\n",
      "3799 377\n",
      "3804 377\n",
      "3824 377\n",
      "3825 377\n",
      "3826 377\n",
      "3840 377\n",
      "3841 377\n",
      "3850 377\n",
      "3860 377\n",
      "3870 377\n",
      "3874 377\n",
      "3877 377\n",
      "3879 377\n",
      "3881 377\n",
      "3885 377\n",
      "3887 377\n",
      "3894 377\n",
      "3894 377\n",
      "3899 377\n",
      "3929 377\n",
      "3935 377\n",
      "3949 377\n",
      "3954 377\n",
      "3955 377\n",
      "3959 "
     ]
    }
   ],
   "source": [
    "for i, j in zip(data.edge_src, data.edge_dst):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.edge_dst),len(data.edge_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "n_hidden = 16 # number of hidden units\n",
    "n_bases = -1 # use number of relations as number of bases\n",
    "n_hidden_layers = 0 # use 1 input layer, 1 output layer, no hidden layer\n",
    "n_epochs = 25 # epochs to train\n",
    "lr = 0.01 # learning rate\n",
    "l2norm = 0 # L2 norm coefficient\n",
    "\n",
    "# create graph\n",
    "g = DGLGraph((data.edge_src, data.edge_dst))\n",
    "g.edata.update({'rel_type': edge_type, 'norm': edge_norm})\n",
    "\n",
    "# create model\n",
    "model = Model(len(g),\n",
    "              n_hidden,\n",
    "              num_classes,\n",
    "              num_rels,\n",
    "              num_bases=n_bases,\n",
    "              num_hidden_layers=n_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([   0,  919,  919,  ..., 5939, 5939, 8284]),\n",
       "  tensor([   0,    0,    0,  ..., 8284, 8284, 8284])),\n",
       " tensor([   0,    1,    2,  ..., 8282, 8283, 8284]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.edges(),g.nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch 00000 | Train Accuracy: 0.1429 | Train Loss: 1.3868 | Validation Accuracy: 0.1786 | Validation loss: 1.3866\n",
      "Epoch 00001 | Train Accuracy: 0.9554 | Train Loss: 1.3479 | Validation Accuracy: 1.0000 | Validation loss: 1.3601\n",
      "Epoch 00002 | Train Accuracy: 0.9554 | Train Loss: 1.2883 | Validation Accuracy: 1.0000 | Validation loss: 1.3200\n",
      "Epoch 00003 | Train Accuracy: 0.9554 | Train Loss: 1.2108 | Validation Accuracy: 1.0000 | Validation loss: 1.2657\n",
      "Epoch 00004 | Train Accuracy: 0.9464 | Train Loss: 1.1272 | Validation Accuracy: 1.0000 | Validation loss: 1.2005\n",
      "Epoch 00005 | Train Accuracy: 0.9554 | Train Loss: 1.0516 | Validation Accuracy: 1.0000 | Validation loss: 1.1306\n",
      "Epoch 00006 | Train Accuracy: 0.9643 | Train Loss: 0.9899 | Validation Accuracy: 1.0000 | Validation loss: 1.0632\n",
      "Epoch 00007 | Train Accuracy: 0.9821 | Train Loss: 0.9412 | Validation Accuracy: 1.0000 | Validation loss: 1.0039\n",
      "Epoch 00008 | Train Accuracy: 0.9821 | Train Loss: 0.9022 | Validation Accuracy: 1.0000 | Validation loss: 0.9554\n",
      "Epoch 00009 | Train Accuracy: 0.9821 | Train Loss: 0.8701 | Validation Accuracy: 1.0000 | Validation loss: 0.9171\n",
      "Epoch 00010 | Train Accuracy: 0.9821 | Train Loss: 0.8435 | Validation Accuracy: 1.0000 | Validation loss: 0.8870\n",
      "Epoch 00011 | Train Accuracy: 0.9821 | Train Loss: 0.8218 | Validation Accuracy: 1.0000 | Validation loss: 0.8633\n",
      "Epoch 00012 | Train Accuracy: 0.9821 | Train Loss: 0.8049 | Validation Accuracy: 0.9643 | Validation loss: 0.8444\n",
      "Epoch 00013 | Train Accuracy: 0.9821 | Train Loss: 0.7922 | Validation Accuracy: 0.9643 | Validation loss: 0.8294\n",
      "Epoch 00014 | Train Accuracy: 0.9821 | Train Loss: 0.7829 | Validation Accuracy: 0.9643 | Validation loss: 0.8179\n",
      "Epoch 00015 | Train Accuracy: 0.9821 | Train Loss: 0.7762 | Validation Accuracy: 0.9643 | Validation loss: 0.8092\n",
      "Epoch 00016 | Train Accuracy: 0.9821 | Train Loss: 0.7711 | Validation Accuracy: 0.9643 | Validation loss: 0.8030\n",
      "Epoch 00017 | Train Accuracy: 0.9821 | Train Loss: 0.7672 | Validation Accuracy: 0.9643 | Validation loss: 0.7986\n",
      "Epoch 00018 | Train Accuracy: 0.9821 | Train Loss: 0.7641 | Validation Accuracy: 0.9643 | Validation loss: 0.7957\n",
      "Epoch 00019 | Train Accuracy: 0.9821 | Train Loss: 0.7615 | Validation Accuracy: 0.9643 | Validation loss: 0.7940\n",
      "Epoch 00020 | Train Accuracy: 0.9821 | Train Loss: 0.7592 | Validation Accuracy: 0.9643 | Validation loss: 0.7932\n",
      "Epoch 00021 | Train Accuracy: 0.9821 | Train Loss: 0.7570 | Validation Accuracy: 0.9643 | Validation loss: 0.7932\n",
      "Epoch 00022 | Train Accuracy: 0.9911 | Train Loss: 0.7549 | Validation Accuracy: 0.9643 | Validation loss: 0.7938\n",
      "Epoch 00023 | Train Accuracy: 0.9911 | Train Loss: 0.7529 | Validation Accuracy: 0.9643 | Validation loss: 0.7951\n",
      "Epoch 00024 | Train Accuracy: 1.0000 | Train Loss: 0.7511 | Validation Accuracy: 0.9643 | Validation loss: 0.7970\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "\n",
    "print(\"start training...\")\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model.forward(g)\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx].long())\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx])\n",
    "    train_acc = train_acc.item() / len(train_idx)\n",
    "    val_loss = F.cross_entropy(logits[val_idx], labels[val_idx].long())\n",
    "    val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx])\n",
    "    val_acc = val_acc.item() / len(val_idx)\n",
    "    print(\"Epoch {:05d} | \".format(epoch) +\n",
    "          \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "              train_acc, loss.item()) +\n",
    "          \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "              val_acc, val_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The second task, link prediction\n",
    "--------------------------------\n",
    "So far, you have seen how to use DGL to implement entity classification with an \n",
    "R-GCN model. In the knowledge base setting, representation generated by\n",
    "R-GCN can be used to uncover potential relationships between nodes. In the \n",
    "R-GCN paper, the authors feed the entity representations generated by R-GCN\n",
    "into the `DistMult <https://arxiv.org/pdf/1412.6575.pdf>`_ prediction model\n",
    "to predict possible relationships.\n",
    "\n",
    "The implementation is similar to that presented here, but with an extra DistMult layer\n",
    "stacked on top of the R-GCN layers. You can find the complete\n",
    "implementation of link prediction with R-GCN in our `Github Python code example\n",
    " <https://github.com/dmlc/dgl/blob/master/examples/pytorch/rgcn/link_predict.py>`_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch import RelGraphConv\n",
    "class BaseRGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels, num_bases,\n",
    "                 num_hidden_layers=1, dropout=0,\n",
    "                 use_self_loop=False, use_cuda=False):\n",
    "        super(BaseRGCN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = None if num_bases < 0 else num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_self_loop = use_self_loop\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # i2h\n",
    "        i2h = self.build_input_layer()\n",
    "        if i2h is not None:\n",
    "            self.layers.append(i2h)\n",
    "        # h2h\n",
    "        for idx in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer(idx)\n",
    "            self.layers.append(h2h)\n",
    "        # h2o\n",
    "        h2o = self.build_output_layer()\n",
    "        if h2o is not None:\n",
    "            self.layers.append(h2o)\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return None\n",
    "\n",
    "    def build_hidden_layer(self, idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return None\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h, r, norm)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_nodes, h_dim)\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        return self.embedding(h.squeeze())\n",
    "\n",
    "class RGCN(BaseRGCN):\n",
    "    def build_input_layer(self):\n",
    "        return EmbeddingLayer(self.num_nodes, self.h_dim)\n",
    "\n",
    "    def build_hidden_layer(self, idx):\n",
    "        act = F.relu if idx < self.num_hidden_layers - 1 else None\n",
    "        return RelGraphConv(self.h_dim, self.h_dim, self.num_rels, \"bdd\",\n",
    "                self.num_bases, activation=act, self_loop=True,\n",
    "                dropout=self.dropout)\n",
    "\n",
    "class LinkPredict(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, num_rels, num_bases=-1,\n",
    "                 num_hidden_layers=1, dropout=0, use_cuda=False, reg_param=0):\n",
    "        super(LinkPredict, self).__init__()\n",
    "        self.rgcn = RGCN(in_dim, h_dim, h_dim, num_rels * 2, num_bases,\n",
    "                         num_hidden_layers, dropout, use_cuda)\n",
    "        self.reg_param = reg_param\n",
    "        self.w_relation = nn.Parameter(torch.Tensor(num_rels, h_dim))\n",
    "        nn.init.xavier_uniform_(self.w_relation,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def calc_score(self, embedding, triplets):\n",
    "        # DistMult\n",
    "        s = embedding[triplets[:,0]]\n",
    "        r = self.w_relation[triplets[:,1]]\n",
    "        o = embedding[triplets[:,2]]\n",
    "        score = torch.sum(s * r * o, dim=1)\n",
    "        return score\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        return self.rgcn.forward(g, h, r, norm)\n",
    "\n",
    "    def regularization_loss(self, embedding):\n",
    "        return torch.mean(embedding.pow(2)) + torch.mean(self.w_relation.pow(2))\n",
    "\n",
    "    def get_loss(self, g, embed, triplets, labels):\n",
    "        # triplets is a list of data samples (positive and negative)\n",
    "        # each row in the triplets is a 3-tuple of (source, relation, destination)\n",
    "        score = self.calc_score(embed, triplets)\n",
    "        predict_loss = F.binary_cross_entropy_with_logits(score, labels)\n",
    "        reg_loss = self.regularization_loss(embed)\n",
    "        return predict_loss + self.reg_param * reg_loss\n",
    "\n",
    "def node_norm_to_edge_norm(g, node_norm):\n",
    "    g = g.local_var()\n",
    "    # convert to edge norm\n",
    "    g.ndata['norm'] = node_norm\n",
    "    g.apply_edges(lambda edges : {'norm' : edges.dst['norm']})\n",
    "    return g.edata['norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinkPredict(num_nodes,\n",
    "                        500,\n",
    "                        num_rels,\n",
    "                        num_bases=100,\n",
    "                        num_hidden_layers=2,\n",
    "                        dropout=0.2,\n",
    "                        use_cuda=-1,\n",
    "                        reg_param=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# entities: 14541\n",
      "# relations: 237\n",
      "# edges: 272115\n"
     ]
    }
   ],
   "source": [
    "data = load_data('FB15k-237')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-eff6dd6d3d91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnum_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvalid_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_rels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_rels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "num_nodes = data.num_nodes\n",
    "train_data = data.train\n",
    "valid_data = data.valid\n",
    "test_data = data.test\n",
    "num_rels = data.num_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = torch.LongTensor(valid_data)\n",
    "test_data = torch.LongTensor(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_and_degrees(num_nodes, triplets):\n",
    "    \"\"\" Get adjacency list and degrees of the graph\n",
    "    \"\"\"\n",
    "    adj_list = [[] for _ in range(num_nodes)]\n",
    "    for i,triplet in enumerate(triplets):\n",
    "        adj_list[triplet[0]].append([i, triplet[2]])\n",
    "        adj_list[triplet[2]].append([i, triplet[0]])\n",
    "\n",
    "    degrees = np.array([len(a) for a in adj_list])\n",
    "    adj_list = [np.array(a) for a in adj_list]\n",
    "    return adj_list, degrees\n",
    "\n",
    "def sample_edge_neighborhood(adj_list, degrees, n_triplets, sample_size):\n",
    "    \"\"\"Sample edges by neighborhool expansion.\n",
    "    This guarantees that the sampled edges form a connected graph, which\n",
    "    may help deeper GNNs that require information from more than one hop.\n",
    "    \"\"\"\n",
    "    edges = np.zeros((sample_size), dtype=np.int32)\n",
    "\n",
    "    #initialize\n",
    "    sample_counts = np.array([d for d in degrees])\n",
    "    picked = np.array([False for _ in range(n_triplets)])\n",
    "    seen = np.array([False for _ in degrees])\n",
    "\n",
    "    for i in range(0, sample_size):\n",
    "        weights = sample_counts * seen\n",
    "\n",
    "        if np.sum(weights) == 0:\n",
    "            weights = np.ones_like(weights)\n",
    "            weights[np.where(sample_counts == 0)] = 0\n",
    "\n",
    "        probabilities = (weights) / np.sum(weights)\n",
    "        chosen_vertex = np.random.choice(np.arange(degrees.shape[0]),\n",
    "                                         p=probabilities)\n",
    "        chosen_adj_list = adj_list[chosen_vertex]\n",
    "        seen[chosen_vertex] = True\n",
    "\n",
    "        chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
    "        chosen_edge = chosen_adj_list[chosen_edge]\n",
    "        edge_number = chosen_edge[0]\n",
    "\n",
    "        while picked[edge_number]:\n",
    "            chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
    "            chosen_edge = chosen_adj_list[chosen_edge]\n",
    "            edge_number = chosen_edge[0]\n",
    "\n",
    "        edges[i] = edge_number\n",
    "        other_vertex = chosen_edge[1]\n",
    "        picked[edge_number] = True\n",
    "        sample_counts[chosen_vertex] -= 1\n",
    "        sample_counts[other_vertex] -= 1\n",
    "        seen[other_vertex] = True\n",
    "\n",
    "    return edges\n",
    "\n",
    "def sample_edge_uniform(adj_list, degrees, n_triplets, sample_size):\n",
    "    \"\"\"Sample edges uniformly from all the edges.\"\"\"\n",
    "    all_edges = np.arange(n_triplets)\n",
    "    return np.random.choice(all_edges, sample_size, replace=False)\n",
    "\n",
    "def generate_sampled_graph_and_labels(triplets, sample_size, split_size,\n",
    "                                      num_rels, adj_list, degrees,\n",
    "                                      negative_rate, sampler=\"uniform\"):\n",
    "    \"\"\"Get training graph and signals\n",
    "    First perform edge neighborhood sampling on graph, then perform negative\n",
    "    sampling to generate negative samples\n",
    "    \"\"\"\n",
    "    # perform edge neighbor sampling\n",
    "    if sampler == \"uniform\":\n",
    "        edges = sample_edge_uniform(adj_list, degrees, len(triplets), sample_size)\n",
    "    elif sampler == \"neighbor\":\n",
    "        edges = sample_edge_neighborhood(adj_list, degrees, len(triplets), sample_size)\n",
    "    else:\n",
    "        raise ValueError(\"Sampler type must be either 'uniform' or 'neighbor'.\")\n",
    "\n",
    "    # relabel nodes to have consecutive node ids\n",
    "    edges = triplets[edges]\n",
    "    src, rel, dst = edges.transpose()\n",
    "    uniq_v, edges = np.unique((src, dst), return_inverse=True)\n",
    "    src, dst = np.reshape(edges, (2, -1))\n",
    "    relabeled_edges = np.stack((src, rel, dst)).transpose()\n",
    "\n",
    "    # negative sampling\n",
    "    samples, labels = negative_sampling(relabeled_edges, len(uniq_v),\n",
    "                                        negative_rate)\n",
    "\n",
    "    # further split graph, only half of the edges will be used as graph\n",
    "    # structure, while the rest half is used as unseen positive samples\n",
    "    split_size = int(sample_size * split_size)\n",
    "    graph_split_ids = np.random.choice(np.arange(sample_size),\n",
    "                                       size=split_size, replace=False)\n",
    "    src = src[graph_split_ids]\n",
    "    dst = dst[graph_split_ids]\n",
    "    rel = rel[graph_split_ids]\n",
    "\n",
    "    # build DGL graph\n",
    "    print(\"# sampled nodes: {}\".format(len(uniq_v)))\n",
    "    print(\"# sampled edges: {}\".format(len(src) * 2))\n",
    "    g, rel, norm = build_graph_from_triplets(len(uniq_v), num_rels,\n",
    "                                             (src, rel, dst))\n",
    "    return g, uniq_v, rel, norm, samples, labels\n",
    "\n",
    "def comp_deg_norm(g):\n",
    "    g = g.local_var()\n",
    "    in_deg = g.in_degrees(range(g.number_of_nodes())).float().numpy()\n",
    "    norm = 1.0 / in_deg\n",
    "    norm[np.isinf(norm)] = 0\n",
    "    return norm\n",
    "\n",
    "def build_graph_from_triplets(num_nodes, num_rels, triplets):\n",
    "    \"\"\" Create a DGL graph. The graph is bidirectional because RGCN authors\n",
    "        use reversed relations.\n",
    "        This function also generates edge type and normalization factor\n",
    "        (reciprocal of node incoming degree)\n",
    "    \"\"\"\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(num_nodes)\n",
    "    src, rel, dst = triplets\n",
    "    src, dst = np.concatenate((src, dst)), np.concatenate((dst, src))\n",
    "    rel = np.concatenate((rel, rel + num_rels))\n",
    "    edges = sorted(zip(dst, src, rel))\n",
    "    dst, src, rel = np.array(edges).transpose()\n",
    "    g.add_edges(src, dst)\n",
    "    norm = comp_deg_norm(g)\n",
    "    print(\"# nodes: {}, # edges: {}\".format(num_nodes, len(src)))\n",
    "    return g, rel.astype('int64'), norm.astype('int64')\n",
    "\n",
    "def build_test_graph(num_nodes, num_rels, edges):\n",
    "    src, rel, dst = edges.transpose()\n",
    "    print(\"Test graph:\")\n",
    "    return build_graph_from_triplets(num_nodes, num_rels, (src, rel, dst))\n",
    "\n",
    "def negative_sampling(pos_samples, num_entity, negative_rate):\n",
    "    size_of_batch = len(pos_samples)\n",
    "    num_to_generate = size_of_batch * negative_rate\n",
    "    neg_samples = np.tile(pos_samples, (negative_rate, 1))\n",
    "    labels = np.zeros(size_of_batch * (negative_rate + 1), dtype=np.float32)\n",
    "    labels[: size_of_batch] = 1\n",
    "    values = np.random.randint(num_entity, size=num_to_generate)\n",
    "    choices = np.random.uniform(size=num_to_generate)\n",
    "    subj = choices > 0.5\n",
    "    obj = choices <= 0.5\n",
    "    neg_samples[subj, 0] = values[subj]\n",
    "    neg_samples[obj, 2] = values[obj]\n",
    "\n",
    "    return np.concatenate((pos_samples, neg_samples)), labels\n",
    "\n",
    "#######################################################################\n",
    "#\n",
    "# Utility functions for evaluations (raw)\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "def sort_and_rank(score, target):\n",
    "    _, indices = torch.sort(score, dim=1, descending=True)\n",
    "    indices = torch.nonzero(indices == target.view(-1, 1))\n",
    "    indices = indices[:, 1].view(-1)\n",
    "    return indices\n",
    "\n",
    "def perturb_and_get_raw_rank(embedding, w, a, r, b, test_size, batch_size=100):\n",
    "    \"\"\" Perturb one element in the triplets\n",
    "    \"\"\"\n",
    "    n_batch = (test_size + batch_size - 1) // batch_size\n",
    "    ranks = []\n",
    "    for idx in range(n_batch):\n",
    "        print(\"batch {} / {}\".format(idx, n_batch))\n",
    "        batch_start = idx * batch_size\n",
    "        batch_end = min(test_size, (idx + 1) * batch_size)\n",
    "        batch_a = a[batch_start: batch_end]\n",
    "        batch_r = r[batch_start: batch_end]\n",
    "        emb_ar = embedding[batch_a] * w[batch_r]\n",
    "        emb_ar = emb_ar.transpose(0, 1).unsqueeze(2) # size: D x E x 1\n",
    "        emb_c = embedding.transpose(0, 1).unsqueeze(1) # size: D x 1 x V\n",
    "        # out-prod and reduce sum\n",
    "        out_prod = torch.bmm(emb_ar, emb_c) # size D x E x V\n",
    "        score = torch.sum(out_prod, dim=0) # size E x V\n",
    "        score = torch.sigmoid(score)\n",
    "        target = b[batch_start: batch_end]\n",
    "        ranks.append(sort_and_rank(score, target))\n",
    "    return torch.cat(ranks)\n",
    "\n",
    "# return MRR (raw), and Hits @ (1, 3, 10)\n",
    "def calc_raw_mrr(embedding, w, test_triplets, hits=[], eval_bz=100):\n",
    "    with torch.no_grad():\n",
    "        s = test_triplets[:, 0]\n",
    "        r = test_triplets[:, 1]\n",
    "        o = test_triplets[:, 2]\n",
    "        test_size = test_triplets.shape[0]\n",
    "\n",
    "        # perturb subject\n",
    "        ranks_s = perturb_and_get_raw_rank(embedding, w, o, r, s, test_size, eval_bz)\n",
    "        # perturb object\n",
    "        ranks_o = perturb_and_get_raw_rank(embedding, w, s, r, o, test_size, eval_bz)\n",
    "\n",
    "        ranks = torch.cat([ranks_s, ranks_o])\n",
    "        ranks += 1 # change to 1-indexed\n",
    "\n",
    "        mrr = torch.mean(1.0 / ranks.float())\n",
    "        print(\"MRR (raw): {:.6f}\".format(mrr.item()))\n",
    "\n",
    "        for hit in hits:\n",
    "            avg_count = torch.mean((ranks <= hit).float())\n",
    "            print(\"Hits (raw) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
    "    return mrr.item()\n",
    "\n",
    "#######################################################################\n",
    "#\n",
    "# Utility functions for evaluations (filtered)\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "def filter_o(triplets_to_filter, target_s, target_r, target_o, num_entities):\n",
    "    target_s, target_r, target_o = int(target_s), int(target_r), int(target_o)\n",
    "    filtered_o = []\n",
    "    # Do not filter out the test triplet, since we want to predict on it\n",
    "    if (target_s, target_r, target_o) in triplets_to_filter:\n",
    "        triplets_to_filter.remove((target_s, target_r, target_o))\n",
    "    # Do not consider an object if it is part of a triplet to filter\n",
    "    for o in range(num_entities):\n",
    "        if (target_s, target_r, o) not in triplets_to_filter:\n",
    "            filtered_o.append(o)\n",
    "    return torch.LongTensor(filtered_o)\n",
    "\n",
    "def filter_s(triplets_to_filter, target_s, target_r, target_o, num_entities):\n",
    "    target_s, target_r, target_o = int(target_s), int(target_r), int(target_o)\n",
    "    filtered_s = []\n",
    "    # Do not filter out the test triplet, since we want to predict on it\n",
    "    if (target_s, target_r, target_o) in triplets_to_filter:\n",
    "        triplets_to_filter.remove((target_s, target_r, target_o))\n",
    "    # Do not consider a subject if it is part of a triplet to filter\n",
    "    for s in range(num_entities):\n",
    "        if (s, target_r, target_o) not in triplets_to_filter:\n",
    "            filtered_s.append(s)\n",
    "    return torch.LongTensor(filtered_s)\n",
    "\n",
    "def perturb_o_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter):\n",
    "    \"\"\" Perturb object in the triplets\n",
    "    \"\"\"\n",
    "    num_entities = embedding.shape[0]\n",
    "    ranks = []\n",
    "    for idx in range(test_size):\n",
    "        if idx % 100 == 0:\n",
    "            print(\"test triplet {} / {}\".format(idx, test_size))\n",
    "        target_s = s[idx]\n",
    "        target_r = r[idx]\n",
    "        target_o = o[idx]\n",
    "        filtered_o = filter_o(triplets_to_filter, target_s, target_r, target_o, num_entities)\n",
    "        target_o_idx = int((filtered_o == target_o).nonzero())\n",
    "        emb_s = embedding[target_s]\n",
    "        emb_r = w[target_r]\n",
    "        emb_o = embedding[filtered_o]\n",
    "        emb_triplet = emb_s * emb_r * emb_o\n",
    "        scores = torch.sigmoid(torch.sum(emb_triplet, dim=1))\n",
    "        _, indices = torch.sort(scores, descending=True)\n",
    "        rank = int((indices == target_o_idx).nonzero())\n",
    "        ranks.append(rank)\n",
    "    return torch.LongTensor(ranks)\n",
    "\n",
    "def perturb_s_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter):\n",
    "    \"\"\" Perturb subject in the triplets\n",
    "    \"\"\"\n",
    "    num_entities = embedding.shape[0]\n",
    "    ranks = []\n",
    "    for idx in range(test_size):\n",
    "        if idx % 100 == 0:\n",
    "            print(\"test triplet {} / {}\".format(idx, test_size))\n",
    "        target_s = s[idx]\n",
    "        target_r = r[idx]\n",
    "        target_o = o[idx]\n",
    "        filtered_s = filter_s(triplets_to_filter, target_s, target_r, target_o, num_entities)\n",
    "        target_s_idx = int((filtered_s == target_s).nonzero())\n",
    "        emb_s = embedding[filtered_s]\n",
    "        emb_r = w[target_r]\n",
    "        emb_o = embedding[target_o]\n",
    "        emb_triplet = emb_s * emb_r * emb_o\n",
    "        scores = torch.sigmoid(torch.sum(emb_triplet, dim=1))\n",
    "        _, indices = torch.sort(scores, descending=True)\n",
    "        rank = int((indices == target_s_idx).nonzero())\n",
    "        ranks.append(rank)\n",
    "    return torch.LongTensor(ranks)\n",
    "\n",
    "def calc_filtered_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits=[]):\n",
    "    with torch.no_grad():\n",
    "        s = test_triplets[:, 0]\n",
    "        r = test_triplets[:, 1]\n",
    "        o = test_triplets[:, 2]\n",
    "        test_size = test_triplets.shape[0]\n",
    "\n",
    "        triplets_to_filter = torch.cat([train_triplets, valid_triplets, test_triplets]).tolist()\n",
    "        triplets_to_filter = {tuple(triplet) for triplet in triplets_to_filter}\n",
    "        print('Perturbing subject...')\n",
    "        ranks_s = perturb_s_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter)\n",
    "        print('Perturbing object...')\n",
    "        ranks_o = perturb_o_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter)\n",
    "\n",
    "        ranks = torch.cat([ranks_s, ranks_o])\n",
    "        ranks += 1 # change to 1-indexed\n",
    "\n",
    "        mrr = torch.mean(1.0 / ranks.float())\n",
    "        print(\"MRR (filtered): {:.6f}\".format(mrr.item()))\n",
    "\n",
    "        for hit in hits:\n",
    "            avg_count = torch.mean((ranks <= hit).float())\n",
    "            print(\"Hits (filtered) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
    "    return mrr.item()\n",
    "\n",
    "#######################################################################\n",
    "#\n",
    "# Main evaluation function\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "def calc_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits=[], eval_bz=100, eval_p=\"filtered\"):\n",
    "    if eval_p == \"filtered\":\n",
    "        mrr = calc_filtered_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits)\n",
    "    else:\n",
    "        mrr = calc_raw_mrr(embedding, w, test_triplets, hits, eval_bz)\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test graph:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Aplikasi\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:106: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nodes: 14541, # edges: 544230\n"
     ]
    }
   ],
   "source": [
    "# build test graph\n",
    "test_graph, test_rel, test_norm = build_test_graph(\n",
    "    num_nodes, num_rels, train_data)\n",
    "test_deg = test_graph.in_degrees(range(test_graph.number_of_nodes())).float().view(-1,1)\n",
    "test_node_id = torch.arange(0, num_nodes, dtype=torch.long).view(-1, 1)\n",
    "test_rel = torch.from_numpy(test_rel)\n",
    "test_norm = node_norm_to_edge_norm(test_graph, torch.from_numpy(test_norm).view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    " # build adj list and calculate degrees for sampling\n",
    "adj_list, degrees = get_adj_and_degrees(num_nodes, train_data)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-11284d3435c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mbest_mrr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "model_state_file = 'model_state.pth'\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "use_cuda=False\n",
    "# training loop\n",
    "print(\"start training...\")\n",
    "\n",
    "epoch = 0\n",
    "best_mrr = 0\n",
    "while True:\n",
    "    model.train()\n",
    "    epoch += 1\n",
    "\n",
    "    # perform edge neighborhood sampling to generate training graph and data\n",
    "    g, node_id, edge_type, node_norm, data, labels = \\\n",
    "        generate_sampled_graph_and_labels(\n",
    "            train_data, 30000, 0.5,\n",
    "            num_rels, adj_list, degrees, 10,\n",
    "            'uniform')\n",
    "    print(\"Done edge sampling\")\n",
    "\n",
    "        # set node/edge feature\n",
    "    node_id = torch.from_numpy(node_id).view(-1, 1).long()\n",
    "    edge_type = torch.from_numpy(edge_type)\n",
    "    edge_norm = node_norm_to_edge_norm(g, torch.from_numpy(node_norm).view(-1, 1))\n",
    "    data, labels = torch.from_numpy(data), torch.from_numpy(labels)\n",
    "    deg = g.in_degrees(range(g.number_of_nodes())).float().view(-1, 1)\n",
    "    if use_cuda:\n",
    "        node_id, deg = node_id.cuda(), deg.cuda()\n",
    "        edge_type, edge_norm = edge_type.cuda(), edge_norm.cuda()\n",
    "        data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "    t0 = time.time()\n",
    "    embed = model(g, node_id, edge_type, edge_norm)\n",
    "    loss = model.get_loss(g, embed, data, labels)\n",
    "    t1 = time.time()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clip gradients\n",
    "    optimizer.step()\n",
    "    t2 = time.time()\n",
    "\n",
    "    forward_time.append(t1 - t0)\n",
    "    backward_time.append(t2 - t1)\n",
    "    print(\"Epoch {:04d} | Loss {:.4f} | Best MRR {:.4f} | Forward {:.4f}s | Backward {:.4f}s\".\n",
    "            format(epoch, loss.item(), best_mrr, forward_time[-1], backward_time[-1]))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "        # validation\n",
    "    if epoch % 2 == 0:\n",
    "        # perform validation on CPU because full graph is too large\n",
    "        if use_cuda:\n",
    "            model.cpu()\n",
    "        model.eval()\n",
    "        print(\"start eval\")\n",
    "        embed = model(test_graph, test_node_id, test_rel, test_norm)\n",
    "        mrr = utils.calc_mrr(embed, model.w_relation, torch.LongTensor(train_data),\n",
    "                                valid_data, test_data, hits=[1, 3, 10], eval_bz=500,\n",
    "                                eval_p='filtered')\n",
    "        # save best model\n",
    "        if mrr < best_mrr:\n",
    "            if epoch >= 10:\n",
    "                break\n",
    "        else:\n",
    "            best_mrr = mrr\n",
    "            torch.save({'state_dict': model.state_dict(), 'epoch': epoch},\n",
    "                        model_state_file)\n",
    "        if use_cuda:\n",
    "            model.cuda()\n",
    "\n",
    "print(\"training done\")\n",
    "print(\"Mean forward time: {:4f}s\".format(np.mean(forward_time)))\n",
    "print(\"Mean Backward time: {:4f}s\".format(np.mean(backward_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DGLGraph(num_nodes=11774, num_edges=30000,\n",
       "         ndata_schemes={}\n",
       "         edata_schemes={})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14541"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DGLGraph(num_nodes=11752, num_edges=30000,\n",
       "          ndata_schemes={}\n",
       "          edata_schemes={}), 11752, 30000, 30000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g, len(node_id), len(edge_type), len(edge_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
