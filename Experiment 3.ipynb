{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "from functools import partial\n",
    "import dgl\n",
    "from dgl.contrib.data import load_data\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch import RelGraphConv\n",
    "class BaseRGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels, num_bases,\n",
    "                 num_hidden_layers=1, dropout=0,\n",
    "                 use_self_loop=False, use_cuda=False):\n",
    "        super(BaseRGCN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = None if num_bases < 0 else num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_self_loop = use_self_loop\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # i2h\n",
    "        i2h = self.build_input_layer()\n",
    "        if i2h is not None:\n",
    "            self.layers.append(i2h)\n",
    "        # h2h\n",
    "        for idx in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer(idx)\n",
    "            self.layers.append(h2h)\n",
    "        # h2o\n",
    "        h2o = self.build_output_layer()\n",
    "        if h2o is not None:\n",
    "            self.layers.append(h2o)\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return None\n",
    "\n",
    "    def build_hidden_layer(self, idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return None\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h, r, norm)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_nodes, h_dim)\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        return self.embedding(h.squeeze())\n",
    "\n",
    "class RGCN(BaseRGCN):\n",
    "    def build_input_layer(self):\n",
    "        return EmbeddingLayer(self.num_nodes, self.h_dim)\n",
    "\n",
    "    def build_hidden_layer(self, idx):\n",
    "        act = F.relu if idx < self.num_hidden_layers - 1 else None\n",
    "        return RelGraphConv(self.h_dim, self.h_dim, self.num_rels, \"bdd\",\n",
    "                self.num_bases, activation=act, self_loop=True,\n",
    "                dropout=self.dropout)\n",
    "\n",
    "class LinkPredict(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, num_rels, num_bases=-1,\n",
    "                 num_hidden_layers=1, dropout=0, use_cuda=False, reg_param=0):\n",
    "        super(LinkPredict, self).__init__()\n",
    "        self.rgcn = RGCN(in_dim, h_dim, h_dim, num_rels * 2, num_bases,\n",
    "                         num_hidden_layers, dropout, use_cuda)\n",
    "        self.reg_param = reg_param\n",
    "        self.w_relation = nn.Parameter(torch.Tensor(num_rels, h_dim))\n",
    "        nn.init.xavier_uniform_(self.w_relation,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def calc_score(self, embedding, triplets):\n",
    "        # DistMult\n",
    "        s = embedding[triplets[:,0]]\n",
    "        r = self.w_relation[triplets[:,1]]\n",
    "        o = embedding[triplets[:,2]]\n",
    "        score = torch.sum(s * r * o, dim=1)\n",
    "        return score\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        return self.rgcn.forward(g, h, r, norm)\n",
    "\n",
    "    def regularization_loss(self, embedding):\n",
    "        return torch.mean(embedding.pow(2)) + torch.mean(self.w_relation.pow(2))\n",
    "\n",
    "    def get_loss(self, g, embed, triplets, labels):\n",
    "        # triplets is a list of data samples (positive and negative)\n",
    "        # each row in the triplets is a 3-tuple of (source, relation, destination)\n",
    "        score = self.calc_score(embed, triplets)\n",
    "        predict_loss = F.binary_cross_entropy_with_logits(score, labels)\n",
    "        reg_loss = self.regularization_loss(embed)\n",
    "        return predict_loss + self.reg_param * reg_loss\n",
    "    def evaluate(self, g):\n",
    "        # get embedding and relation weight without grad\n",
    "        embedding = self.forward(g)\n",
    "        return embedding, self.w_relation\n",
    "\n",
    "def node_norm_to_edge_norm(g, node_norm):\n",
    "    g = g.local_var()\n",
    "    # convert to edge norm\n",
    "    g.ndata['norm'] = node_norm\n",
    "    g.apply_edges(lambda edges : {'norm' : edges.dst['norm']})\n",
    "    return g.edata['norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sample_dataset_learnavi.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(data.source_name.append(data.target_name).unique())\n",
    "num_rels = len(data.edge.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4623)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rels, num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinkPredict(num_nodes,\n",
    "                    140,\n",
    "                    num_rels,\n",
    "                    num_bases=140,\n",
    "                    num_hidden_layers=2,\n",
    "                    dropout=0.2,\n",
    "                    use_cuda=-1,\n",
    "                    reg_param=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, valid_data = train_test_split(data[['source','edge','target']].values, test_size=0.33, random_state=42)\n",
    "valid_data, test_data = train_test_split(valid_data, test_size=0.5, random_state=42)\n",
    "# data = shuffle(data).reset_index(drop=True)\n",
    "# train_data = data[:6055]\n",
    "# valid_data = data[6055:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[['source','edge','target']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7569, 3), (1249, 3), (1249, 3))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, valid_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = torch.LongTensor(valid_data)\n",
    "test_data = torch.LongTensor(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_and_degrees(num_nodes, triplets):\n",
    "    \"\"\" Get adjacency list and degrees of the graph\n",
    "    \"\"\"\n",
    "    adj_list = [[] for _ in range(num_nodes)]\n",
    "    for i,triplet in enumerate(triplets):\n",
    "        adj_list[triplet[0]].append([i, triplet[2]])\n",
    "        adj_list[triplet[2]].append([i, triplet[0]])\n",
    "\n",
    "    degrees = np.array([len(a) for a in adj_list])\n",
    "    adj_list = [np.array(a) for a in adj_list]\n",
    "    return adj_list, degrees\n",
    "\n",
    "def sample_edge_neighborhood(adj_list, degrees, n_triplets, sample_size):\n",
    "    \"\"\"Sample edges by neighborhool expansion.\n",
    "    This guarantees that the sampled edges form a connected graph, which\n",
    "    may help deeper GNNs that require information from more than one hop.\n",
    "    \"\"\"\n",
    "    edges = np.zeros((sample_size), dtype=np.int32)\n",
    "\n",
    "    #initialize\n",
    "    sample_counts = np.array([d for d in degrees])\n",
    "    picked = np.array([False for _ in range(n_triplets)])\n",
    "    seen = np.array([False for _ in degrees])\n",
    "\n",
    "    for i in range(0, sample_size):\n",
    "        weights = sample_counts * seen\n",
    "\n",
    "        if np.sum(weights) == 0:\n",
    "            weights = np.ones_like(weights)\n",
    "            weights[np.where(sample_counts == 0)] = 0\n",
    "\n",
    "        probabilities = (weights) / np.sum(weights)\n",
    "        chosen_vertex = np.random.choice(np.arange(degrees.shape[0]),\n",
    "                                         p=probabilities)\n",
    "        chosen_adj_list = adj_list[chosen_vertex]\n",
    "        seen[chosen_vertex] = True\n",
    "\n",
    "        chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
    "        chosen_edge = chosen_adj_list[chosen_edge]\n",
    "        edge_number = chosen_edge[0]\n",
    "\n",
    "        while picked[edge_number]:\n",
    "            chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
    "            chosen_edge = chosen_adj_list[chosen_edge]\n",
    "            edge_number = chosen_edge[0]\n",
    "\n",
    "        edges[i] = edge_number\n",
    "        other_vertex = chosen_edge[1]\n",
    "        picked[edge_number] = True\n",
    "        sample_counts[chosen_vertex] -= 1\n",
    "        sample_counts[other_vertex] -= 1\n",
    "        seen[other_vertex] = True\n",
    "\n",
    "    return edges\n",
    "\n",
    "def sample_edge_uniform(adj_list, degrees, n_triplets, sample_size):\n",
    "    \"\"\"Sample edges uniformly from all the edges.\"\"\"\n",
    "    all_edges = np.arange(n_triplets)\n",
    "    return np.random.choice(all_edges, sample_size, replace=False)\n",
    "\n",
    "def generate_sampled_graph_and_labels(triplets, sample_size, split_size,\n",
    "                                      num_rels, adj_list, degrees,\n",
    "                                      negative_rate, sampler=\"uniform\"):\n",
    "    \"\"\"Get training graph and signals\n",
    "    First perform edge neighborhood sampling on graph, then perform negative\n",
    "    sampling to generate negative samples\n",
    "    \"\"\"\n",
    "    # perform edge neighbor sampling\n",
    "    if sampler == \"uniform\":\n",
    "        edges = sample_edge_uniform(adj_list, degrees, len(triplets), sample_size)\n",
    "    elif sampler == \"neighbor\":\n",
    "        edges = sample_edge_neighborhood(adj_list, degrees, len(triplets), sample_size)\n",
    "    else:\n",
    "        raise ValueError(\"Sampler type must be either 'uniform' or 'neighbor'.\")\n",
    "\n",
    "    # relabel nodes to have consecutive node ids\n",
    "    edges = triplets[edges]\n",
    "    src, rel, dst = edges.transpose()\n",
    "    uniq_v, edges = np.unique((src, dst), return_inverse=True)\n",
    "    src, dst = np.reshape(edges, (2, -1))\n",
    "    relabeled_edges = np.stack((src, rel, dst)).transpose()\n",
    "\n",
    "    # negative sampling\n",
    "    samples, labels = negative_sampling(relabeled_edges, len(uniq_v),\n",
    "                                        negative_rate)\n",
    "\n",
    "    # further split graph, only half of the edges will be used as graph\n",
    "    # structure, while the rest half is used as unseen positive samples\n",
    "    split_size = int(sample_size * split_size)\n",
    "    graph_split_ids = np.random.choice(np.arange(sample_size),\n",
    "                                       size=split_size, replace=False)\n",
    "    src = src[graph_split_ids]\n",
    "    dst = dst[graph_split_ids]\n",
    "    rel = rel[graph_split_ids]\n",
    "\n",
    "    # build DGL graph\n",
    "    print(\"# sampled nodes: {}\".format(len(uniq_v)))\n",
    "    print(\"# sampled edges: {}\".format(len(src) * 2))\n",
    "    g, rel, norm = build_graph_from_triplets(len(uniq_v), num_rels,\n",
    "                                             (src, rel, dst))\n",
    "    return g, uniq_v, rel, norm, samples, labels\n",
    "\n",
    "def comp_deg_norm(g):\n",
    "    g = g.local_var()\n",
    "    in_deg = g.in_degrees(range(g.number_of_nodes())).float().numpy()\n",
    "    norm = 1.0 / in_deg\n",
    "    norm[np.isinf(norm)] = 0\n",
    "    return norm\n",
    "\n",
    "def build_graph_from_triplets(num_nodes, num_rels, triplets):\n",
    "    \"\"\" Create a DGL graph. The graph is bidirectional because RGCN authors\n",
    "        use reversed relations.\n",
    "        This function also generates edge type and normalization factor\n",
    "        (reciprocal of node incoming degree)\n",
    "    \"\"\"\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(num_nodes)\n",
    "    src, rel, dst = triplets\n",
    "    src, dst = np.concatenate((src, dst)), np.concatenate((dst, src))\n",
    "    rel = np.concatenate((rel, rel + num_rels))\n",
    "    edges = sorted(zip(dst, src, rel))\n",
    "    dst, src, rel = np.array(edges).transpose()\n",
    "    g.add_edges(src, dst)\n",
    "    norm = comp_deg_norm(g)\n",
    "    print(\"# nodes: {}, # edges: {}\".format(num_nodes, len(src)))\n",
    "    return g, rel.astype('int64'), norm.astype('int64')\n",
    "\n",
    "def build_test_graph(num_nodes, num_rels, edges):\n",
    "    src, rel, dst = edges.transpose()\n",
    "    print(\"Test graph:\")\n",
    "    return build_graph_from_triplets(num_nodes, num_rels, (src, rel, dst))\n",
    "\n",
    "def negative_sampling(pos_samples, num_entity, negative_rate):\n",
    "    size_of_batch = len(pos_samples)\n",
    "    num_to_generate = size_of_batch * negative_rate\n",
    "    neg_samples = np.tile(pos_samples, (negative_rate, 1))\n",
    "    labels = np.zeros(size_of_batch * (negative_rate + 1), dtype=np.float32)\n",
    "    labels[: size_of_batch] = 1\n",
    "    values = np.random.randint(num_entity, size=num_to_generate)\n",
    "    choices = np.random.uniform(size=num_to_generate)\n",
    "    subj = choices > 0.5\n",
    "    obj = choices <= 0.5\n",
    "    neg_samples[subj, 0] = values[subj]\n",
    "    neg_samples[obj, 2] = values[obj]\n",
    "\n",
    "    return np.concatenate((pos_samples, neg_samples)), labels\n",
    "\n",
    "#######################################################################\n",
    "#\n",
    "# Utility functions for evaluations (raw)\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "def sort_and_rank(score, target):\n",
    "    _, indices = torch.sort(score, dim=1, descending=True)\n",
    "    indices = torch.nonzero(indices == target.view(-1, 1))\n",
    "    indices = indices[:, 1].view(-1)\n",
    "    return indices\n",
    "\n",
    "def perturb_and_get_raw_rank(embedding, w, a, r, b, test_size, batch_size=100):\n",
    "    \"\"\" Perturb one element in the triplets\n",
    "    \"\"\"\n",
    "    n_batch = (test_size + batch_size - 1) // batch_size\n",
    "    ranks = []\n",
    "    for idx in range(n_batch):\n",
    "        print(\"batch {} / {}\".format(idx, n_batch))\n",
    "        batch_start = idx * batch_size\n",
    "        batch_end = min(test_size, (idx + 1) * batch_size)\n",
    "        batch_a = a[batch_start: batch_end]\n",
    "        batch_r = r[batch_start: batch_end]\n",
    "        emb_ar = embedding[batch_a] * w[batch_r]\n",
    "        emb_ar = emb_ar.transpose(0, 1).unsqueeze(2) # size: D x E x 1\n",
    "        emb_c = embedding.transpose(0, 1).unsqueeze(1) # size: D x 1 x V\n",
    "        # out-prod and reduce sum\n",
    "        out_prod = torch.bmm(emb_ar, emb_c) # size D x E x V\n",
    "        score = torch.sum(out_prod, dim=0) # size E x V\n",
    "        score = torch.sigmoid(score)\n",
    "        target = b[batch_start: batch_end]\n",
    "        print(score)\n",
    "        print(target)\n",
    "        ranks.append(sort_and_rank(score, target))\n",
    "        print(ranks)\n",
    "    return torch.cat(ranks)\n",
    "\n",
    "# return MRR (raw), and Hits @ (1, 3, 10)\n",
    "def calc_raw_mrr(embedding, w, test_triplets, hits=[], eval_bz=100):\n",
    "    with torch.no_grad():\n",
    "        s = test_triplets[:, 0]\n",
    "        r = test_triplets[:, 1]\n",
    "        o = test_triplets[:, 2]\n",
    "        test_size = test_triplets.shape[0]\n",
    "        print(s,r,o)\n",
    "        # perturb subject\n",
    "        ranks_s = perturb_and_get_raw_rank(embedding, w, o, r, s, test_size, eval_bz)\n",
    "        # perturb object\n",
    "        ranks_o = perturb_and_get_raw_rank(embedding, w, s, r, o, test_size, eval_bz)\n",
    "\n",
    "        ranks = torch.cat([ranks_s, ranks_o])\n",
    "        ranks += 1 # change to 1-indexed\n",
    "        print(ranks)\n",
    "        mrr = torch.mean(1.0 / ranks.float())\n",
    "        print(\"MRR (raw): {:.6f}\".format(mrr.item()))\n",
    "\n",
    "        for hit in hits:\n",
    "            avg_count = torch.mean((ranks <= hit).float())\n",
    "            print(\"Hits (raw) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
    "    return mrr.item()\n",
    "\n",
    "def filter_o(triplets_to_filter, target_s, target_r, target_o, num_entities):\n",
    "    target_s, target_r, target_o = int(target_s), int(target_r), int(target_o)\n",
    "    filtered_o = []\n",
    "    # Do not filter out the test triplet, since we want to predict on it\n",
    "    if (target_s, target_r, target_o) in triplets_to_filter:\n",
    "        triplets_to_filter.remove((target_s, target_r, target_o))\n",
    "    # Do not consider an object if it is part of a triplet to filter\n",
    "    for o in range(num_entities):\n",
    "        if (target_s, target_r, o) not in triplets_to_filter:\n",
    "            filtered_o.append(o)\n",
    "    return torch.LongTensor(filtered_o)\n",
    "\n",
    "def filter_s(triplets_to_filter, target_s, target_r, target_o, num_entities):\n",
    "    target_s, target_r, target_o = int(target_s), int(target_r), int(target_o)\n",
    "    filtered_s = []\n",
    "    # Do not filter out the test triplet, since we want to predict on it\n",
    "    if (target_s, target_r, target_o) in triplets_to_filter:\n",
    "        triplets_to_filter.remove((target_s, target_r, target_o))\n",
    "    # Do not consider a subject if it is part of a triplet to filter\n",
    "    for s in range(num_entities):\n",
    "        if (s, target_r, target_o) not in triplets_to_filter:\n",
    "            filtered_s.append(s)\n",
    "    return torch.LongTensor(filtered_s)\n",
    "\n",
    "def perturb_o_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter):\n",
    "    \"\"\" Perturb object in the triplets\n",
    "    \"\"\"\n",
    "    num_entities = embedding.shape[0]\n",
    "    ranks = []\n",
    "    for idx in range(test_size):\n",
    "        if idx % 100 == 0:\n",
    "            print(\"test triplet {} / {}\".format(idx, test_size))\n",
    "        target_s = s[idx]\n",
    "        target_r = r[idx]\n",
    "        target_o = o[idx]\n",
    "        filtered_o = filter_o(triplets_to_filter, target_s, target_r, target_o, num_entities)\n",
    "        target_o_idx = int((filtered_o == target_o).nonzero())\n",
    "        emb_s = embedding[target_s]\n",
    "        emb_r = w[target_r]\n",
    "        emb_o = embedding[filtered_o]\n",
    "        emb_triplet = emb_s * emb_r * emb_o\n",
    "        scores = torch.sigmoid(torch.sum(emb_triplet, dim=1))\n",
    "        _, indices = torch.sort(scores, descending=True)\n",
    "        rank = int((indices == target_o_idx).nonzero())\n",
    "        ranks.append(rank)\n",
    "    return torch.LongTensor(ranks)\n",
    "\n",
    "def perturb_s_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter):\n",
    "    \"\"\" Perturb subject in the triplets\n",
    "    \"\"\"\n",
    "    num_entities = embedding.shape[0]\n",
    "    ranks = []\n",
    "    for idx in range(test_size):\n",
    "        if idx % 100 == 0:\n",
    "            print(\"test triplet {} / {}\".format(idx, test_size))\n",
    "        target_s = s[idx]\n",
    "        target_r = r[idx]\n",
    "        target_o = o[idx]\n",
    "        filtered_s = filter_s(triplets_to_filter, target_s, target_r, target_o, num_entities)\n",
    "        target_s_idx = int((filtered_s == target_s).nonzero())\n",
    "        emb_s = embedding[filtered_s]\n",
    "        emb_r = w[target_r]\n",
    "        emb_o = embedding[target_o]\n",
    "        emb_triplet = emb_s * emb_r * emb_o\n",
    "        scores = torch.sigmoid(torch.sum(emb_triplet, dim=1))\n",
    "        _, indices = torch.sort(scores, descending=True)\n",
    "        rank = int((indices == target_s_idx).nonzero())\n",
    "        ranks.append(rank)\n",
    "    return torch.LongTensor(ranks)\n",
    "\n",
    "def calc_filtered_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits=[]):\n",
    "    with torch.no_grad():\n",
    "        s = test_triplets[:, 0]\n",
    "        r = test_triplets[:, 1]\n",
    "        o = test_triplets[:, 2]\n",
    "        test_size = test_triplets.shape[0]\n",
    "\n",
    "        triplets_to_filter = torch.cat([train_triplets, valid_triplets, test_triplets]).tolist()\n",
    "        triplets_to_filter = {tuple(triplet) for triplet in triplets_to_filter}\n",
    "        print('Perturbing subject...')\n",
    "        ranks_s = perturb_s_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter)\n",
    "        print('Perturbing object...')\n",
    "        ranks_o = perturb_o_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter)\n",
    "\n",
    "        ranks = torch.cat([ranks_s, ranks_o])\n",
    "        ranks += 1 # change to 1-indexed\n",
    "\n",
    "        mrr = torch.mean(1.0 / ranks.float())\n",
    "        print(\"MRR (filtered): {:.6f}\".format(mrr.item()))\n",
    "\n",
    "        for hit in hits:\n",
    "            avg_count = torch.mean((ranks <= hit).float())\n",
    "            print(\"Hits (filtered) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
    "    return mrr.item()\n",
    "\n",
    "def calc_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits=[], eval_bz=100, eval_p=\"filtered\"):\n",
    "    if eval_p == \"filtered\":\n",
    "        mrr = calc_filtered_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits)\n",
    "    else:\n",
    "        mrr = calc_raw_mrr(embedding, w, test_triplets, hits, eval_bz)\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test graph:\n",
      "# nodes: 4623, # edges: 15138\n"
     ]
    }
   ],
   "source": [
    "# build test graph\n",
    "test_graph, test_rel, test_norm = build_test_graph(\n",
    "    num_nodes, num_rels, train_data)\n",
    "test_deg = test_graph.in_degrees(range(test_graph.number_of_nodes())).float().view(-1,1)\n",
    "test_node_id = torch.arange(0, num_nodes, dtype=torch.long).view(-1, 1)\n",
    "test_rel = torch.from_numpy(test_rel)\n",
    "test_norm = node_norm_to_edge_norm(test_graph, torch.from_numpy(test_norm).view(-1, 1))\n",
    "# test_graph.ndata.update({'id': test_node_id, 'norm': test_norm})\n",
    "# test_graph.edata['type'] = test_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    " # build adj list and calculate degrees for sampling\n",
    "adj_list, degrees = get_adj_and_degrees(num_nodes, train_data)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "# sampled nodes: 34\n",
      "# sampled edges: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Aplikasi\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:106: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nodes: 34, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0001 | Loss 3.2399 | Best MRR 0.0000 | Forward 0.1469s | Backward 0.0907s\n",
      "# sampled nodes: 34\n",
      "# sampled edges: 20\n",
      "# nodes: 34, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0002 | Loss 1.4982 | Best MRR 0.0000 | Forward 0.0221s | Backward 0.0536s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.002905\n",
      "Hits (filtered) @ 1: 0.000400\n",
      "Hits (filtered) @ 3: 0.000801\n",
      "Hits (filtered) @ 10: 0.004404\n",
      "# sampled nodes: 31\n",
      "# sampled edges: 20\n",
      "# nodes: 31, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0003 | Loss 1.4046 | Best MRR 0.0029 | Forward 0.0521s | Backward 0.0406s\n",
      "# sampled nodes: 30\n",
      "# sampled edges: 20\n",
      "# nodes: 30, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0004 | Loss 1.2026 | Best MRR 0.0029 | Forward 0.0110s | Backward 0.0501s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.022371\n",
      "Hits (filtered) @ 1: 0.011609\n",
      "Hits (filtered) @ 3: 0.020016\n",
      "Hits (filtered) @ 10: 0.041633\n",
      "# sampled nodes: 30\n",
      "# sampled edges: 20\n",
      "# nodes: 30, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0005 | Loss 1.6107 | Best MRR 0.0224 | Forward 0.0572s | Backward 0.0486s\n",
      "# sampled nodes: 31\n",
      "# sampled edges: 20\n",
      "# nodes: 31, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0006 | Loss 1.6174 | Best MRR 0.0224 | Forward 0.0175s | Backward 0.0587s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.103734\n",
      "Hits (filtered) @ 1: 0.082866\n",
      "Hits (filtered) @ 3: 0.106485\n",
      "Hits (filtered) @ 10: 0.142914\n",
      "# sampled nodes: 29\n",
      "# sampled edges: 20\n",
      "# nodes: 29, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0007 | Loss 1.6991 | Best MRR 0.1037 | Forward 0.0662s | Backward 0.0461s\n",
      "# sampled nodes: 32\n",
      "# sampled edges: 20\n",
      "# nodes: 32, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0008 | Loss 1.1873 | Best MRR 0.1037 | Forward 0.0170s | Backward 0.0511s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.041720\n",
      "Hits (filtered) @ 1: 0.015612\n",
      "Hits (filtered) @ 3: 0.053643\n",
      "Hits (filtered) @ 10: 0.076861\n",
      "# sampled nodes: 27\n",
      "# sampled edges: 20\n",
      "# nodes: 27, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0009 | Loss 0.9751 | Best MRR 0.1037 | Forward 0.0679s | Backward 0.0456s\n",
      "# sampled nodes: 30\n",
      "# sampled edges: 20\n",
      "# nodes: 30, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0010 | Loss 1.2217 | Best MRR 0.1037 | Forward 0.0201s | Backward 0.0727s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.038528\n",
      "Hits (filtered) @ 1: 0.008807\n",
      "Hits (filtered) @ 3: 0.051241\n",
      "Hits (filtered) @ 10: 0.093675\n",
      "# sampled nodes: 28\n",
      "# sampled edges: 20\n",
      "# nodes: 28, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0011 | Loss 1.0776 | Best MRR 0.1037 | Forward 0.0717s | Backward 0.0556s\n",
      "# sampled nodes: 29\n",
      "# sampled edges: 20\n",
      "# nodes: 29, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0012 | Loss 1.0474 | Best MRR 0.1037 | Forward 0.0140s | Backward 0.0536s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.172692\n",
      "Hits (filtered) @ 1: 0.138911\n",
      "Hits (filtered) @ 3: 0.191353\n",
      "Hits (filtered) @ 10: 0.230584\n",
      "# sampled nodes: 34\n",
      "# sampled edges: 20\n",
      "# nodes: 34, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0013 | Loss 1.2329 | Best MRR 0.1727 | Forward 0.1715s | Backward 0.1063s\n",
      "# sampled nodes: 34\n",
      "# sampled edges: 20\n",
      "# nodes: 34, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0014 | Loss 0.9955 | Best MRR 0.1727 | Forward 0.0396s | Backward 0.0772s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.217062\n",
      "Hits (filtered) @ 1: 0.199760\n",
      "Hits (filtered) @ 3: 0.229784\n",
      "Hits (filtered) @ 10: 0.246197\n",
      "# sampled nodes: 29\n",
      "# sampled edges: 20\n",
      "# nodes: 29, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0015 | Loss 0.8168 | Best MRR 0.2171 | Forward 0.0617s | Backward 0.0551s\n",
      "# sampled nodes: 34\n",
      "# sampled edges: 20\n",
      "# nodes: 34, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0016 | Loss 1.0575 | Best MRR 0.2171 | Forward 0.0165s | Backward 0.0561s\n",
      "start eval\n",
      "Perturbing subject...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.209623\n",
      "Hits (filtered) @ 1: 0.185348\n",
      "Hits (filtered) @ 3: 0.224980\n",
      "Hits (filtered) @ 10: 0.247798\n",
      "# sampled nodes: 27\n",
      "# sampled edges: 20\n",
      "# nodes: 27, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0017 | Loss 0.6592 | Best MRR 0.2171 | Forward 0.0518s | Backward 0.0426s\n",
      "# sampled nodes: 34\n",
      "# sampled edges: 20\n",
      "# nodes: 34, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0018 | Loss 0.9198 | Best MRR 0.2171 | Forward 0.0140s | Backward 0.0436s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.203528\n",
      "Hits (filtered) @ 1: 0.176541\n",
      "Hits (filtered) @ 3: 0.222578\n",
      "Hits (filtered) @ 10: 0.248999\n",
      "# sampled nodes: 31\n",
      "# sampled edges: 20\n",
      "# nodes: 31, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0019 | Loss 1.1995 | Best MRR 0.2171 | Forward 0.1083s | Backward 0.0436s\n",
      "# sampled nodes: 34\n",
      "# sampled edges: 20\n",
      "# nodes: 34, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0020 | Loss 0.7844 | Best MRR 0.2171 | Forward 0.0185s | Backward 0.0973s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.193677\n",
      "Hits (filtered) @ 1: 0.165733\n",
      "Hits (filtered) @ 3: 0.204564\n",
      "Hits (filtered) @ 10: 0.244996\n",
      "# sampled nodes: 30\n",
      "# sampled edges: 20\n",
      "# nodes: 30, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0021 | Loss 1.2519 | Best MRR 0.2171 | Forward 0.0486s | Backward 0.0416s\n",
      "# sampled nodes: 29\n",
      "# sampled edges: 20\n",
      "# nodes: 29, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0022 | Loss 0.6625 | Best MRR 0.2171 | Forward 0.0115s | Backward 0.0421s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.180322\n",
      "Hits (filtered) @ 1: 0.151321\n",
      "Hits (filtered) @ 3: 0.190552\n",
      "Hits (filtered) @ 10: 0.234588\n",
      "# sampled nodes: 30\n",
      "# sampled edges: 20\n",
      "# nodes: 30, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0023 | Loss 1.0056 | Best MRR 0.2171 | Forward 0.0556s | Backward 0.0496s\n",
      "# sampled nodes: 32\n",
      "# sampled edges: 20\n",
      "# nodes: 32, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0024 | Loss 0.7454 | Best MRR 0.2171 | Forward 0.0090s | Backward 0.0546s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.185293\n",
      "Hits (filtered) @ 1: 0.161329\n",
      "Hits (filtered) @ 3: 0.194155\n",
      "Hits (filtered) @ 10: 0.225380\n",
      "# sampled nodes: 28\n",
      "# sampled edges: 20\n",
      "# nodes: 28, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0025 | Loss 0.5633 | Best MRR 0.2171 | Forward 0.0506s | Backward 0.0572s\n",
      "# sampled nodes: 31\n",
      "# sampled edges: 20\n",
      "# nodes: 31, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0026 | Loss 0.8248 | Best MRR 0.2171 | Forward 0.0125s | Backward 0.0707s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.188287\n",
      "Hits (filtered) @ 1: 0.162930\n",
      "Hits (filtered) @ 3: 0.195757\n",
      "Hits (filtered) @ 10: 0.239792\n",
      "# sampled nodes: 30\n",
      "# sampled edges: 20\n",
      "# nodes: 30, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0027 | Loss 0.7259 | Best MRR 0.2171 | Forward 0.0496s | Backward 0.0416s\n",
      "# sampled nodes: 30\n",
      "# sampled edges: 20\n",
      "# nodes: 30, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0028 | Loss 0.8487 | Best MRR 0.2171 | Forward 0.0095s | Backward 0.0431s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.171896\n",
      "Hits (filtered) @ 1: 0.138911\n",
      "Hits (filtered) @ 3: 0.186950\n",
      "Hits (filtered) @ 10: 0.238191\n",
      "# sampled nodes: 29\n",
      "# sampled edges: 20\n",
      "# nodes: 29, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0029 | Loss 0.5902 | Best MRR 0.2171 | Forward 0.0546s | Backward 0.0416s\n",
      "# sampled nodes: 28\n",
      "# sampled edges: 20\n",
      "# nodes: 28, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0030 | Loss 0.6460 | Best MRR 0.2171 | Forward 0.0150s | Backward 0.0451s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.163005\n",
      "Hits (filtered) @ 1: 0.130104\n",
      "Hits (filtered) @ 3: 0.173339\n",
      "Hits (filtered) @ 10: 0.221777\n",
      "training done\n",
      "Mean forward time: 0.045414s\n",
      "Mean Backward time: 0.055883s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "model_state_file = 'model_state.pth'\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "use_cuda=False\n",
    "# training loop\n",
    "print(\"start training...\")\n",
    "\n",
    "epoch = 0\n",
    "best_mrr = 0\n",
    "while True:\n",
    "    model.train()\n",
    "    epoch += 1\n",
    "\n",
    "    # perform edge neighborhood sampling to generate training graph and data\n",
    "    g, node_id, edge_type, node_norm, data, labels = \\\n",
    "        generate_sampled_graph_and_labels(\n",
    "            train_data, 20, 0.5,\n",
    "            num_rels, adj_list, degrees, 10,\n",
    "            'uniform')\n",
    "    print(\"Done edge sampling\")\n",
    "\n",
    "        # set node/edge feature\n",
    "    node_id = torch.from_numpy(node_id).view(-1, 1).long()\n",
    "    edge_type = torch.from_numpy(edge_type)\n",
    "    edge_norm = node_norm_to_edge_norm(g, torch.from_numpy(node_norm).view(-1, 1))\n",
    "    data, labels = torch.from_numpy(data), torch.from_numpy(labels)\n",
    "    deg = g.in_degrees(range(g.number_of_nodes())).float().view(-1, 1)\n",
    "    if use_cuda:\n",
    "        node_id, deg = node_id.cuda(), deg.cuda()\n",
    "        edge_type, edge_norm = edge_type.cuda(), edge_norm.cuda()\n",
    "        data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "    t0 = time.time()\n",
    "    embed = model(g, node_id, edge_type, edge_norm)\n",
    "    loss = model.get_loss(g, embed, data, labels)\n",
    "    t1 = time.time()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clip gradients\n",
    "    optimizer.step()\n",
    "    t2 = time.time()\n",
    "\n",
    "    forward_time.append(t1 - t0)\n",
    "    backward_time.append(t2 - t1)\n",
    "    print(\"Epoch {:04d} | Loss {:.4f} | Best MRR {:.4f} | Forward {:.4f}s | Backward {:.4f}s\".\n",
    "            format(epoch, loss.item(), best_mrr, forward_time[-1], backward_time[-1]))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "        # validation\n",
    "    if epoch % 2 == 0:\n",
    "        # perform validation on CPU because full graph is too large\n",
    "        if use_cuda:\n",
    "            model.cpu()\n",
    "        model.eval()\n",
    "        print(\"start eval\")\n",
    "        embed = model(test_graph, test_node_id, test_rel, test_norm)\n",
    "        mrr = calc_mrr(embed, model.w_relation, torch.LongTensor(train_data),\n",
    "                                valid_data, test_data, hits=[1, 3, 10], eval_bz=500,\n",
    "                                eval_p='filtered')\n",
    "        # save best model\n",
    "        if mrr < best_mrr:\n",
    "            if epoch >= 30:\n",
    "                break\n",
    "        else:\n",
    "            best_mrr = mrr\n",
    "            torch.save({'state_dict': model.state_dict(), 'epoch': epoch},\n",
    "                        model_state_file)\n",
    "        if use_cuda:\n",
    "            model.cuda()\n",
    "\n",
    "print(\"training done\")\n",
    "print(\"Mean forward time: {:4f}s\".format(np.mean(forward_time)))\n",
    "print(\"Mean Backward time: {:4f}s\".format(np.mean(backward_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4623, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_node_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load('model_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinkPredict(num_nodes,\n",
    "                                 140,\n",
    "                                 num_rels,\n",
    "                                 num_bases=140,\n",
    "                                 num_hidden_layers=2,\n",
    "                                 dropout=0.2,\n",
    "                                 use_cuda=-1,\n",
    "                                 reg_param=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "coba = 2\n",
    "coba = torch.LongTensor([coba]).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model(test_graph, test_node_id, test_rel, test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "coba = torch.LongTensor([1,2,3]).view(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4622,    3, 3653],\n",
       "       [4622,    3, 3765],\n",
       "       [4622,    3, 3844],\n",
       "       ...,\n",
       "       [1432,    4, 2437],\n",
       "       [1432,    4, 1838],\n",
       "       [1513,    4, 2397]], dtype=int64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1]) tensor([2]) tensor([3])\n",
      "batch 0 / 1\n",
      "tensor([[2.1096e-06, 1.2538e-21, 1.2835e-06,  ..., 1.2513e-11, 1.9853e-11,\n",
      "         4.2081e-02]])\n",
      "tensor([1])\n",
      "[tensor([4587])]\n",
      "batch 0 / 1\n",
      "tensor([[5.0884e-08, 7.1135e-25, 4.0413e-06,  ..., 1.8919e-10, 9.3751e-10,\n",
      "         1.1866e-01]])\n",
      "tensor([3])\n",
      "[tensor([4592])]\n",
      "tensor([4588, 4593])\n",
      "MRR (raw): 0.000218\n",
      "Hits (raw) @ 1: 0.000000\n",
      "Hits (raw) @ 3: 0.000000\n",
      "Hits (raw) @ 10: 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0002178412687499076"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_raw_mrr(embed, model.w_relation,coba,[1,3,10],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4622,    3],\n",
       "        [4622,    1],\n",
       "        [1249,    5],\n",
       "        ...,\n",
       "        [1035,    6],\n",
       "        [4622,    1],\n",
       "        [1356,    4]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27,  3, 18],\n",
       "        [27,  3, 18],\n",
       "        [ 3,  0, 15],\n",
       "        [27,  3, 16],\n",
       "        [ 7,  2, 11],\n",
       "        [ 4,  4,  6],\n",
       "        [27,  3, 19],\n",
       "        [27,  1,  2],\n",
       "        [17,  0, 13],\n",
       "        [27,  1,  5],\n",
       "        [27,  3, 22],\n",
       "        [27,  3, 25],\n",
       "        [12,  5,  0],\n",
       "        [27,  1,  2],\n",
       "        [ 1,  6, 27],\n",
       "        [ 8,  2, 10],\n",
       "        [20,  4, 26],\n",
       "        [23,  4, 24],\n",
       "        [21,  0, 14],\n",
       "        [ 9,  2, 11],\n",
       "        [27,  3, 22],\n",
       "        [ 9,  3, 18],\n",
       "        [ 3,  0,  6],\n",
       "        [20,  3, 16],\n",
       "        [ 9,  2, 11],\n",
       "        [21,  4,  6],\n",
       "        [14,  3, 19],\n",
       "        [19,  1,  2],\n",
       "        [17,  0, 11],\n",
       "        [ 3,  1,  5],\n",
       "        [ 4,  3, 22],\n",
       "        [12,  3, 25],\n",
       "        [ 6,  5,  0],\n",
       "        [27,  1, 12],\n",
       "        [22,  6, 27],\n",
       "        [ 8,  2, 24],\n",
       "        [15,  4, 26],\n",
       "        [23,  4, 13],\n",
       "        [ 7,  0, 14],\n",
       "        [16,  2, 11],\n",
       "        [27,  3, 23],\n",
       "        [27,  3, 15],\n",
       "        [ 3,  0, 12],\n",
       "        [27,  3, 11],\n",
       "        [12,  2, 11],\n",
       "        [ 4,  4,  8],\n",
       "        [27,  3, 19],\n",
       "        [10,  1,  2],\n",
       "        [17,  0, 10],\n",
       "        [10,  1,  5],\n",
       "        [27,  3, 19],\n",
       "        [27,  3,  1],\n",
       "        [12,  5,  7],\n",
       "        [24,  1,  2],\n",
       "        [ 2,  6, 27],\n",
       "        [17,  2, 10],\n",
       "        [20,  4, 11],\n",
       "        [18,  4, 24],\n",
       "        [ 0,  0, 14],\n",
       "        [ 1,  2, 11],\n",
       "        [27,  3, 18],\n",
       "        [27,  3, 13],\n",
       "        [ 3,  0, 25],\n",
       "        [ 6,  3, 16],\n",
       "        [ 7,  2, 20],\n",
       "        [ 4,  4, 18],\n",
       "        [23,  3, 19],\n",
       "        [27,  1, 22],\n",
       "        [ 7,  0, 13],\n",
       "        [27,  1, 20],\n",
       "        [27,  3, 26],\n",
       "        [10,  3, 25],\n",
       "        [12,  5,  4],\n",
       "        [27,  1, 10],\n",
       "        [ 6,  6, 27],\n",
       "        [ 9,  2, 10],\n",
       "        [13,  4, 26],\n",
       "        [23,  4, 23],\n",
       "        [21,  0,  0],\n",
       "        [14,  2, 11],\n",
       "        [27,  3, 27],\n",
       "        [ 0,  3, 18],\n",
       "        [16,  0, 15],\n",
       "        [14,  3, 16],\n",
       "        [ 7,  2, 19],\n",
       "        [16,  4,  6],\n",
       "        [27,  3,  3],\n",
       "        [27,  1, 22],\n",
       "        [ 6,  0, 13],\n",
       "        [ 3,  1,  5],\n",
       "        [27,  3,  4],\n",
       "        [15,  3, 25],\n",
       "        [12,  5, 19],\n",
       "        [13,  1,  2],\n",
       "        [ 1,  6, 23],\n",
       "        [16,  2, 10],\n",
       "        [22,  4, 26],\n",
       "        [23,  4, 25],\n",
       "        [21,  0, 20],\n",
       "        [20,  2, 11],\n",
       "        [27,  3, 25],\n",
       "        [15,  3, 18],\n",
       "        [ 3,  0,  7],\n",
       "        [11,  3, 16],\n",
       "        [17,  2, 11],\n",
       "        [ 4,  4, 26],\n",
       "        [27,  3,  4],\n",
       "        [ 6,  1,  2],\n",
       "        [27,  0, 13],\n",
       "        [22,  1,  5],\n",
       "        [ 0,  3, 22],\n",
       "        [27,  3, 16],\n",
       "        [17,  5,  0],\n",
       "        [27,  1,  3],\n",
       "        [10,  6, 27],\n",
       "        [ 8,  2, 22],\n",
       "        [20,  4, 25],\n",
       "        [23,  4, 18],\n",
       "        [21,  0,  1],\n",
       "        [20,  2, 11],\n",
       "        [19,  3, 18],\n",
       "        [27,  3, 10],\n",
       "        [ 5,  0, 15],\n",
       "        [27,  3, 24],\n",
       "        [12,  2, 11],\n",
       "        [ 4,  4, 25],\n",
       "        [ 0,  3, 19],\n",
       "        [27,  1, 20],\n",
       "        [17,  0, 17],\n",
       "        [27,  1,  8],\n",
       "        [ 5,  3, 22],\n",
       "        [27,  3, 10],\n",
       "        [13,  5,  0],\n",
       "        [15,  1,  2],\n",
       "        [15,  6, 27],\n",
       "        [ 2,  2, 10],\n",
       "        [20,  4,  4],\n",
       "        [23,  4,  3],\n",
       "        [ 5,  0, 14],\n",
       "        [ 9,  2,  7],\n",
       "        [27,  3, 16],\n",
       "        [13,  3, 18],\n",
       "        [ 3,  0,  7],\n",
       "        [27,  3, 10],\n",
       "        [19,  2, 11],\n",
       "        [19,  4,  6],\n",
       "        [27,  3,  8],\n",
       "        [13,  1,  2],\n",
       "        [17,  0,  3],\n",
       "        [27,  1, 21],\n",
       "        [27,  3,  8],\n",
       "        [24,  3, 25],\n",
       "        [15,  5,  0],\n",
       "        [26,  1,  2],\n",
       "        [ 1,  6, 25],\n",
       "        [ 4,  2, 10],\n",
       "        [11,  4, 26],\n",
       "        [23,  4,  2],\n",
       "        [ 8,  0, 14],\n",
       "        [ 7,  2, 11],\n",
       "        [27,  3, 10],\n",
       "        [27,  3,  0],\n",
       "        [ 3,  0, 11],\n",
       "        [27,  3, 12],\n",
       "        [11,  2, 11],\n",
       "        [ 4,  4, 11],\n",
       "        [27,  3, 20],\n",
       "        [ 9,  1,  2],\n",
       "        [ 9,  0, 13],\n",
       "        [27,  1,  6],\n",
       "        [27,  3, 21],\n",
       "        [21,  3, 25],\n",
       "        [17,  5,  0],\n",
       "        [27,  1, 15],\n",
       "        [22,  6, 27],\n",
       "        [ 9,  2, 10],\n",
       "        [11,  4, 26],\n",
       "        [23,  4, 16],\n",
       "        [11,  0, 14],\n",
       "        [27,  2, 11],\n",
       "        [ 1,  3, 18],\n",
       "        [17,  3, 18],\n",
       "        [11,  0, 15],\n",
       "        [27,  3, 18],\n",
       "        [ 8,  2, 11],\n",
       "        [19,  4,  6],\n",
       "        [25,  3, 19],\n",
       "        [27,  1, 27],\n",
       "        [27,  0, 13],\n",
       "        [22,  1,  5],\n",
       "        [27,  3,  9],\n",
       "        [27,  3, 15],\n",
       "        [21,  5,  0],\n",
       "        [ 3,  1,  2],\n",
       "        [24,  6, 27],\n",
       "        [ 2,  2, 10],\n",
       "        [ 8,  4, 26],\n",
       "        [ 2,  4, 24],\n",
       "        [21,  0,  0],\n",
       "        [ 6,  2, 11],\n",
       "        [27,  3, 11],\n",
       "        [21,  3, 18],\n",
       "        [ 3,  0, 13],\n",
       "        [27,  3, 23],\n",
       "        [ 7,  2, 15],\n",
       "        [22,  4,  6],\n",
       "        [16,  3, 19],\n",
       "        [27,  1, 13],\n",
       "        [17,  0, 26],\n",
       "        [27,  1, 17],\n",
       "        [27,  3,  6],\n",
       "        [27,  3, 14],\n",
       "        [27,  5,  0],\n",
       "        [27,  1, 10],\n",
       "        [ 6,  6, 27],\n",
       "        [26,  2, 10],\n",
       "        [ 6,  4, 26],\n",
       "        [19,  4, 24],\n",
       "        [21,  0,  3],\n",
       "        [ 0,  2, 11]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = data.source\n",
    "target = data.target\n",
    "num_nodes = len(source.append(target).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>source_name</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4622</td>\n",
       "      <td>user_0</td>\n",
       "      <td>3653</td>\n",
       "      <td>topic_1499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4622</td>\n",
       "      <td>user_0</td>\n",
       "      <td>3765</td>\n",
       "      <td>topic_2414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4622</td>\n",
       "      <td>user_0</td>\n",
       "      <td>3844</td>\n",
       "      <td>topic_3056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4622</td>\n",
       "      <td>user_0</td>\n",
       "      <td>3324</td>\n",
       "      <td>topic_11898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4622</td>\n",
       "      <td>user_0</td>\n",
       "      <td>3844</td>\n",
       "      <td>topic_3056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7564</th>\n",
       "      <td>1382</td>\n",
       "      <td>externalResources_2244</td>\n",
       "      <td>2281</td>\n",
       "      <td>library_5838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7565</th>\n",
       "      <td>1376</td>\n",
       "      <td>externalResources_1921</td>\n",
       "      <td>2414</td>\n",
       "      <td>library_7664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7566</th>\n",
       "      <td>1432</td>\n",
       "      <td>externalResources_5436</td>\n",
       "      <td>2437</td>\n",
       "      <td>library_7977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7567</th>\n",
       "      <td>1432</td>\n",
       "      <td>externalResources_5436</td>\n",
       "      <td>1838</td>\n",
       "      <td>library_13770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7568</th>\n",
       "      <td>1513</td>\n",
       "      <td>externalResources_9293</td>\n",
       "      <td>2397</td>\n",
       "      <td>library_7410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7569 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      source             source_name  target    target_name\n",
       "0       4622                  user_0    3653     topic_1499\n",
       "1       4622                  user_0    3765     topic_2414\n",
       "2       4622                  user_0    3844     topic_3056\n",
       "3       4622                  user_0    3324    topic_11898\n",
       "4       4622                  user_0    3844     topic_3056\n",
       "...      ...                     ...     ...            ...\n",
       "7564    1382  externalResources_2244    2281   library_5838\n",
       "7565    1376  externalResources_1921    2414   library_7664\n",
       "7566    1432  externalResources_5436    2437   library_7977\n",
       "7567    1432  externalResources_5436    1838  library_13770\n",
       "7568    1513  externalResources_9293    2397   library_7410\n",
       "\n",
       "[7569 rows x 4 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['source','source_name','target','target_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicti = source.append(target)\n",
    "dict_name = data.source_name.append(data.target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4623"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(dicti,dict_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'INTERESTED_IN': 3,\n",
       " 'BOOKMARK': 1,\n",
       " 'TALK_OF': 5,\n",
       " 'ASSUME_UNDERSTANDING_OF': 0,\n",
       " 'WRITTEN_BY': 6,\n",
       " 'ENCAPSULATES': 2,\n",
       " 'SYNONYM': 4}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(data.edge_name,data.edge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = data[(data.source==4622)&(data.edge==3)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest = set(search.target)\n",
    "dest_name = set(search.target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for a in dest:\n",
    "    result.append(4622+3+a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(7722, 'topic_6501'),\n",
       " (7725, 'topic_5395'),\n",
       " (7732, 'topic_4031'),\n",
       " (7739, 'topic_2911'),\n",
       " (7748, 'topic_7377'),\n",
       " (7751, 'topic_3366'),\n",
       " (7762, 'topic_160'),\n",
       " (7798, 'topic_10316'),\n",
       " (7805, 'topic_13398'),\n",
       " (7816, 'topic_10580'),\n",
       " (7835, 'topic_5913'),\n",
       " (7843, 'topic_8123'),\n",
       " (7870, 'topic_823'),\n",
       " (7878, 'topic_11173'),\n",
       " (7883, 'topic_3364'),\n",
       " (7949, 'topic_1002'),\n",
       " (7961, 'topic_7704'),\n",
       " (7998, 'topic_11990'),\n",
       " (8001, 'topic_2882'),\n",
       " (8014, 'topic_2747'),\n",
       " (8058, 'topic_4617'),\n",
       " (8071, 'topic_10721'),\n",
       " (8074, 'topic_12872'),\n",
       " (8081, 'topic_606'),\n",
       " (8126, 'topic_2925'),\n",
       " (8134, 'topic_216'),\n",
       " (8150, 'topic_10060'),\n",
       " (8176, 'topic_12955'),\n",
       " (8210, 'topic_14476'),\n",
       " (8238, 'topic_5295'),\n",
       " (8256, 'topic_9065'),\n",
       " (8261, 'topic_1943'),\n",
       " (8267, 'topic_12897'),\n",
       " (8278, 'topic_5904'),\n",
       " (8296, 'topic_13506'),\n",
       " (8327, 'topic_12351'),\n",
       " (8338, 'topic_10198'),\n",
       " (8339, 'topic_6759'),\n",
       " (8359, 'topic_11898'),\n",
       " (8386, 'topic_14428'),\n",
       " (8390, 'topic_6980'),\n",
       " (8392, 'topic_2505'),\n",
       " (8401, 'topic_7124'),\n",
       " (8424, 'topic_10854'),\n",
       " (8429, 'topic_12402'),\n",
       " (8443, 'topic_5691'),\n",
       " (8447, 'topic_3056'),\n",
       " (8450, 'topic_5735'),\n",
       " (8453, 'topic_4695'),\n",
       " (8465, 'topic_9223'),\n",
       " (8469, 'topic_7153'),\n",
       " (8498, 'topic_6981'),\n",
       " (8499, 'topic_10904'),\n",
       " (8508, 'topic_1063'),\n",
       " (8512, 'topic_3460'),\n",
       " (8516, 'topic_1332'),\n",
       " (8582, 'topic_12689'),\n",
       " (8584, 'topic_10222'),\n",
       " (8648, 'topic_2029'),\n",
       " (8658, 'topic_2951'),\n",
       " (8693, 'topic_6218'),\n",
       " (8724, 'topic_9851'),\n",
       " (8739, 'topic_14173'),\n",
       " (8768, 'topic_9617'),\n",
       " (8769, 'topic_2414'),\n",
       " (8772, 'topic_8662'),\n",
       " (8774, 'topic_2381'),\n",
       " (8799, 'topic_10154'),\n",
       " (8802, 'topic_2014'),\n",
       " (8823, 'topic_13752'),\n",
       " (8837, 'topic_6902'),\n",
       " (8840, 'topic_13985'),\n",
       " (8861, 'topic_11218'),\n",
       " (8874, 'topic_6409'),\n",
       " (8903, 'topic_9780'),\n",
       " (8917, 'topic_8429'),\n",
       " (8926, 'topic_7253'),\n",
       " (8927, 'topic_4029'),\n",
       " (8942, 'topic_12330'),\n",
       " (8947, 'topic_6247'),\n",
       " (8957, 'topic_268'),\n",
       " (8966, 'topic_3031'),\n",
       " (9000, 'topic_819'),\n",
       " (9010, 'topic_14526'),\n",
       " (9042, 'topic_9034'),\n",
       " (9047, 'topic_10127'),\n",
       " (9052, 'topic_348'),\n",
       " (9073, 'topic_8834'),\n",
       " (9097, 'topic_5696'),\n",
       " (9116, 'topic_3502'),\n",
       " (9134, 'topic_1499'),\n",
       " (9136, 'topic_9003'),\n",
       " (9144, 'topic_7801'),\n",
       " (9159, 'topic_242'),\n",
       " (9202, 'topic_9818'),\n",
       " (9220, 'topic_5753'),\n",
       " (9227, 'topic_1111'),\n",
       " (9231, 'topic_4994')}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(zip(result,dest_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
