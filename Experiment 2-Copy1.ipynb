{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "from functools import partial\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch import RelGraphConv\n",
    "class BaseRGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels, num_bases,\n",
    "                 num_hidden_layers=1, dropout=0,\n",
    "                 use_self_loop=False, use_cuda=False):\n",
    "        super(BaseRGCN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = None if num_bases < 0 else num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_self_loop = use_self_loop\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # i2h\n",
    "        i2h = self.build_input_layer()\n",
    "        if i2h is not None:\n",
    "            self.layers.append(i2h)\n",
    "        # h2h\n",
    "        for idx in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer(idx)\n",
    "            self.layers.append(h2h)\n",
    "        # h2o\n",
    "        h2o = self.build_output_layer()\n",
    "        if h2o is not None:\n",
    "            self.layers.append(h2o)\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return None\n",
    "\n",
    "    def build_hidden_layer(self, idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return None\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h, r, norm)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_nodes, h_dim)\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        return self.embedding(h.squeeze())\n",
    "\n",
    "class RGCN(BaseRGCN):\n",
    "    def build_input_layer(self):\n",
    "        return EmbeddingLayer(self.num_nodes, self.h_dim)\n",
    "\n",
    "    def build_hidden_layer(self, idx):\n",
    "        act = F.relu if idx < self.num_hidden_layers - 1 else None\n",
    "        return RelGraphConv(self.h_dim, self.h_dim, self.num_rels, \"bdd\",\n",
    "                self.num_bases, activation=act, self_loop=True,\n",
    "                dropout=self.dropout)\n",
    "\n",
    "class LinkPredict(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, num_rels, num_bases=-1,\n",
    "                 num_hidden_layers=1, dropout=0, use_cuda=False, reg_param=0):\n",
    "        super(LinkPredict, self).__init__()\n",
    "        self.rgcn = RGCN(in_dim, h_dim, h_dim, num_rels * 2, num_bases,\n",
    "                         num_hidden_layers, dropout, use_cuda)\n",
    "        self.reg_param = reg_param\n",
    "        self.w_relation = nn.Parameter(torch.Tensor(num_rels, h_dim))\n",
    "        nn.init.xavier_uniform_(self.w_relation,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def calc_score(self, embedding, triplets):\n",
    "        # DistMult\n",
    "        s = embedding[triplets[:,0]]\n",
    "        r = self.w_relation[triplets[:,1]]\n",
    "        o = embedding[triplets[:,2]]\n",
    "        score = torch.sum(s * r * o, dim=1)\n",
    "        return score\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        return self.rgcn.forward(g, h, r, norm)\n",
    "\n",
    "    def regularization_loss(self, embedding):\n",
    "        return torch.mean(embedding.pow(2)) + torch.mean(self.w_relation.pow(2))\n",
    "\n",
    "    def get_loss(self, g, embed, triplets, labels):\n",
    "        # triplets is a list of data samples (positive and negative)\n",
    "        # each row in the triplets is a 3-tuple of (source, relation, destination)\n",
    "        score = self.calc_score(embed, triplets)\n",
    "        predict_loss = F.binary_cross_entropy_with_logits(score, labels)\n",
    "        reg_loss = self.regularization_loss(embed)\n",
    "        return predict_loss + self.reg_param * reg_loss\n",
    "\n",
    "def node_norm_to_edge_norm(g, node_norm):\n",
    "    g = g.local_var()\n",
    "    # convert to edge norm\n",
    "    g.ndata['norm'] = node_norm\n",
    "    g.apply_edges(lambda edges : {'norm' : edges.dst['norm']})\n",
    "    return g.edata['norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# entities: 14541\n",
      "# relations: 237\n",
      "# edges: 272115\n"
     ]
    }
   ],
   "source": [
    "data = load_data('FB15k-237')\n",
    "num_nodes = data.num_nodes\n",
    "train_data = data.train\n",
    "valid_data = data.valid\n",
    "test_data = data.test\n",
    "num_rels = data.num_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = data.num_nodes\n",
    "train_data = data.train\n",
    "valid_data = data.valid\n",
    "test_data = data.test\n",
    "num_rels = data.num_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('sample_dataset_learnavi.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(data.source_name.append(data.target_name).unique())\n",
    "num_rels = len(data.edge.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinkPredict(num_nodes,\n",
    "                    100,\n",
    "                    num_rels,\n",
    "                    num_bases=100,\n",
    "                    num_hidden_layers=2,\n",
    "                    dropout=0.2,\n",
    "                    use_cuda=-1,\n",
    "                    reg_param=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RGCNLinkDataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-e2680b28b19f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'source'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'edge'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.33\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# data = shuffle(data).reset_index(drop=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# train_data = data[:6055]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'RGCNLinkDataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, valid_data = train_test_split(data[['source','edge','target']].values, test_size=0.33, random_state=42)\n",
    "# data = shuffle(data).reset_index(drop=True)\n",
    "# train_data = data[:6055]\n",
    "# valid_data = data[6055:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3364,  183, 8619],\n",
       "        [8077,  124, 6142],\n",
       "        [3776,  166, 7385],\n",
       "        ...,\n",
       "        [9124,  189, 7803],\n",
       "        [1318,  155, 3679],\n",
       "        [7664,    9, 6680]]), array([[11792,     4,   743],\n",
       "        [ 4643,   197,  4684],\n",
       "        [ 2122,   126, 11345],\n",
       "        ...,\n",
       "        [11647,    58, 14037],\n",
       "        [ 9178,    75,  9065],\n",
       "        [ 7609,    82,  1584]]))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = torch.LongTensor(valid_data)\n",
    "test_data = torch.LongTensor(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_and_degrees(num_nodes, triplets):\n",
    "    \"\"\" Get adjacency list and degrees of the graph\n",
    "    \"\"\"\n",
    "    adj_list = [[] for _ in range(num_nodes)]\n",
    "    for i,triplet in enumerate(triplets):\n",
    "        adj_list[triplet[0]].append([i, triplet[2]])\n",
    "        adj_list[triplet[2]].append([i, triplet[0]])\n",
    "\n",
    "    degrees = np.array([len(a) for a in adj_list])\n",
    "    adj_list = [np.array(a) for a in adj_list]\n",
    "    return adj_list, degrees\n",
    "\n",
    "def sample_edge_neighborhood(adj_list, degrees, n_triplets, sample_size):\n",
    "    \"\"\"Sample edges by neighborhool expansion.\n",
    "    This guarantees that the sampled edges form a connected graph, which\n",
    "    may help deeper GNNs that require information from more than one hop.\n",
    "    \"\"\"\n",
    "    edges = np.zeros((sample_size), dtype=np.int32)\n",
    "\n",
    "    #initialize\n",
    "    sample_counts = np.array([d for d in degrees])\n",
    "    picked = np.array([False for _ in range(n_triplets)])\n",
    "    seen = np.array([False for _ in degrees])\n",
    "\n",
    "    for i in range(0, sample_size):\n",
    "        weights = sample_counts * seen\n",
    "\n",
    "        if np.sum(weights) == 0:\n",
    "            weights = np.ones_like(weights)\n",
    "            weights[np.where(sample_counts == 0)] = 0\n",
    "\n",
    "        probabilities = (weights) / np.sum(weights)\n",
    "        chosen_vertex = np.random.choice(np.arange(degrees.shape[0]),\n",
    "                                         p=probabilities)\n",
    "        chosen_adj_list = adj_list[chosen_vertex]\n",
    "        seen[chosen_vertex] = True\n",
    "\n",
    "        chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
    "        chosen_edge = chosen_adj_list[chosen_edge]\n",
    "        edge_number = chosen_edge[0]\n",
    "\n",
    "        while picked[edge_number]:\n",
    "            chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
    "            chosen_edge = chosen_adj_list[chosen_edge]\n",
    "            edge_number = chosen_edge[0]\n",
    "\n",
    "        edges[i] = edge_number\n",
    "        other_vertex = chosen_edge[1]\n",
    "        picked[edge_number] = True\n",
    "        sample_counts[chosen_vertex] -= 1\n",
    "        sample_counts[other_vertex] -= 1\n",
    "        seen[other_vertex] = True\n",
    "\n",
    "    return edges\n",
    "\n",
    "def sample_edge_uniform(adj_list, degrees, n_triplets, sample_size):\n",
    "    \"\"\"Sample edges uniformly from all the edges.\"\"\"\n",
    "    all_edges = np.arange(n_triplets)\n",
    "    return np.random.choice(all_edges, sample_size, replace=False)\n",
    "\n",
    "def generate_sampled_graph_and_labels(triplets, sample_size, split_size,\n",
    "                                      num_rels, adj_list, degrees,\n",
    "                                      negative_rate, sampler=\"uniform\"):\n",
    "    \"\"\"Get training graph and signals\n",
    "    First perform edge neighborhood sampling on graph, then perform negative\n",
    "    sampling to generate negative samples\n",
    "    \"\"\"\n",
    "    # perform edge neighbor sampling\n",
    "    if sampler == \"uniform\":\n",
    "        edges = sample_edge_uniform(adj_list, degrees, len(triplets), sample_size)\n",
    "    elif sampler == \"neighbor\":\n",
    "        edges = sample_edge_neighborhood(adj_list, degrees, len(triplets), sample_size)\n",
    "    else:\n",
    "        raise ValueError(\"Sampler type must be either 'uniform' or 'neighbor'.\")\n",
    "\n",
    "    # relabel nodes to have consecutive node ids\n",
    "    edges = triplets[edges]\n",
    "    src, rel, dst = edges.transpose()\n",
    "    uniq_v, edges = np.unique((src, dst), return_inverse=True)\n",
    "    src, dst = np.reshape(edges, (2, -1))\n",
    "    relabeled_edges = np.stack((src, rel, dst)).transpose()\n",
    "\n",
    "    # negative sampling\n",
    "    samples, labels = negative_sampling(relabeled_edges, len(uniq_v),\n",
    "                                        negative_rate)\n",
    "\n",
    "    # further split graph, only half of the edges will be used as graph\n",
    "    # structure, while the rest half is used as unseen positive samples\n",
    "    split_size = int(sample_size * split_size)\n",
    "    graph_split_ids = np.random.choice(np.arange(sample_size),\n",
    "                                       size=split_size, replace=False)\n",
    "    src = src[graph_split_ids]\n",
    "    dst = dst[graph_split_ids]\n",
    "    rel = rel[graph_split_ids]\n",
    "\n",
    "    # build DGL graph\n",
    "    print(\"# sampled nodes: {}\".format(len(uniq_v)))\n",
    "    print(\"# sampled edges: {}\".format(len(src) * 2))\n",
    "    g, rel, norm = build_graph_from_triplets(len(uniq_v), num_rels,\n",
    "                                             (src, rel, dst))\n",
    "    return g, uniq_v, rel, norm, samples, labels\n",
    "\n",
    "def comp_deg_norm(g):\n",
    "    g = g.local_var()\n",
    "    in_deg = g.in_degrees(range(g.number_of_nodes())).float().numpy()\n",
    "    norm = 1.0 / in_deg\n",
    "    norm[np.isinf(norm)] = 0\n",
    "    return norm\n",
    "\n",
    "def build_graph_from_triplets(num_nodes, num_rels, triplets):\n",
    "    \"\"\" Create a DGL graph. The graph is bidirectional because RGCN authors\n",
    "        use reversed relations.\n",
    "        This function also generates edge type and normalization factor\n",
    "        (reciprocal of node incoming degree)\n",
    "    \"\"\"\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(num_nodes)\n",
    "    src, rel, dst = triplets\n",
    "    src, dst = np.concatenate((src, dst)), np.concatenate((dst, src))\n",
    "    rel = np.concatenate((rel, rel + num_rels))\n",
    "    edges = sorted(zip(dst, src, rel))\n",
    "    dst, src, rel = np.array(edges).transpose()\n",
    "    print(dst,src,rel)\n",
    "    print(edges[-1])\n",
    "    g.add_edges(src, dst)\n",
    "    norm = comp_deg_norm(g)\n",
    "    print(\"# nodes: {}, # edges: {}\".format(num_nodes, len(src)))\n",
    "    return g, rel.astype('int64'), norm.astype('int64')\n",
    "\n",
    "def build_test_graph(num_nodes, num_rels, edges):\n",
    "    src, rel, dst = edges.transpose()\n",
    "    print(\"Test graph:\")\n",
    "    return build_graph_from_triplets(num_nodes, num_rels, (src, rel, dst))\n",
    "\n",
    "def negative_sampling(pos_samples, num_entity, negative_rate):\n",
    "    size_of_batch = len(pos_samples)\n",
    "    num_to_generate = size_of_batch * negative_rate\n",
    "    neg_samples = np.tile(pos_samples, (negative_rate, 1))\n",
    "    labels = np.zeros(size_of_batch * (negative_rate + 1), dtype=np.float32)\n",
    "    labels[: size_of_batch] = 1\n",
    "    values = np.random.randint(num_entity, size=num_to_generate)\n",
    "    choices = np.random.uniform(size=num_to_generate)\n",
    "    subj = choices > 0.5\n",
    "    obj = choices <= 0.5\n",
    "    neg_samples[subj, 0] = values[subj]\n",
    "    neg_samples[obj, 2] = values[obj]\n",
    "\n",
    "    return np.concatenate((pos_samples, neg_samples)), labels\n",
    "\n",
    "#######################################################################\n",
    "#\n",
    "# Utility functions for evaluations (raw)\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "def sort_and_rank(score, target):\n",
    "    _, indices = torch.sort(score, dim=1, descending=True)\n",
    "    indices = torch.nonzero(indices == target.view(-1, 1))\n",
    "    indices = indices[:, 1].view(-1)\n",
    "    return indices\n",
    "\n",
    "def perturb_and_get_raw_rank(embedding, w, a, r, b, test_size, batch_size=100):\n",
    "    \"\"\" Perturb one element in the triplets\n",
    "    \"\"\"\n",
    "    n_batch = (test_size + batch_size - 1) // batch_size\n",
    "    ranks = []\n",
    "    for idx in range(n_batch):\n",
    "        print(\"batch {} / {}\".format(idx, n_batch))\n",
    "        batch_start = idx * batch_size\n",
    "        batch_end = min(test_size, (idx + 1) * batch_size)\n",
    "        batch_a = a[batch_start: batch_end]\n",
    "        batch_r = r[batch_start: batch_end]\n",
    "        emb_ar = embedding[batch_a] * w[batch_r]\n",
    "        emb_ar = emb_ar.transpose(0, 1).unsqueeze(2) # size: D x E x 1\n",
    "        emb_c = embedding.transpose(0, 1).unsqueeze(1) # size: D x 1 x V\n",
    "        # out-prod and reduce sum\n",
    "        out_prod = torch.bmm(emb_ar, emb_c) # size D x E x V\n",
    "        score = torch.sum(out_prod, dim=0) # size E x V\n",
    "        score = torch.sigmoid(score)\n",
    "        target = b[batch_start: batch_end]\n",
    "        ranks.append(sort_and_rank(score, target))\n",
    "    return torch.cat(ranks)\n",
    "\n",
    "# return MRR (raw), and Hits @ (1, 3, 10)\n",
    "def calc_raw_mrr(embedding, w, test_triplets, hits=[], eval_bz=100):\n",
    "    with torch.no_grad():\n",
    "        s = test_triplets[:, 0]\n",
    "        r = test_triplets[:, 1]\n",
    "        o = test_triplets[:, 2]\n",
    "        test_size = test_triplets.shape[0]\n",
    "\n",
    "        # perturb subject\n",
    "        ranks_s = perturb_and_get_raw_rank(embedding, w, o, r, s, test_size, eval_bz)\n",
    "        # perturb object\n",
    "        ranks_o = perturb_and_get_raw_rank(embedding, w, s, r, o, test_size, eval_bz)\n",
    "\n",
    "        ranks = torch.cat([ranks_s, ranks_o])\n",
    "        ranks += 1 # change to 1-indexed\n",
    "\n",
    "        mrr = torch.mean(1.0 / ranks.float())\n",
    "        print(\"MRR (raw): {:.6f}\".format(mrr.item()))\n",
    "\n",
    "        for hit in hits:\n",
    "            avg_count = torch.mean((ranks <= hit).float())\n",
    "            print(\"Hits (raw) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
    "    return mrr.item()\n",
    "\n",
    "#######################################################################\n",
    "#\n",
    "# Utility functions for evaluations (filtered)\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "def filter_o(triplets_to_filter, target_s, target_r, target_o, num_entities):\n",
    "    target_s, target_r, target_o = int(target_s), int(target_r), int(target_o)\n",
    "    filtered_o = []\n",
    "    # Do not filter out the test triplet, since we want to predict on it\n",
    "    if (target_s, target_r, target_o) in triplets_to_filter:\n",
    "        triplets_to_filter.remove((target_s, target_r, target_o))\n",
    "    # Do not consider an object if it is part of a triplet to filter\n",
    "    for o in range(num_entities):\n",
    "        if (target_s, target_r, o) not in triplets_to_filter:\n",
    "            filtered_o.append(o)\n",
    "    return torch.LongTensor(filtered_o)\n",
    "\n",
    "def filter_s(triplets_to_filter, target_s, target_r, target_o, num_entities):\n",
    "    target_s, target_r, target_o = int(target_s), int(target_r), int(target_o)\n",
    "    filtered_s = []\n",
    "    # Do not filter out the test triplet, since we want to predict on it\n",
    "    if (target_s, target_r, target_o) in triplets_to_filter:\n",
    "        triplets_to_filter.remove((target_s, target_r, target_o))\n",
    "    # Do not consider a subject if it is part of a triplet to filter\n",
    "    for s in range(num_entities):\n",
    "        if (s, target_r, target_o) not in triplets_to_filter:\n",
    "            filtered_s.append(s)\n",
    "    return torch.LongTensor(filtered_s)\n",
    "\n",
    "def perturb_o_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter):\n",
    "    \"\"\" Perturb object in the triplets\n",
    "    \"\"\"\n",
    "    num_entities = embedding.shape[0]\n",
    "    ranks = []\n",
    "    for idx in range(test_size):\n",
    "        if idx % 100 == 0:\n",
    "            print(\"test triplet {} / {}\".format(idx, test_size))\n",
    "        target_s = s[idx]\n",
    "        target_r = r[idx]\n",
    "        target_o = o[idx]\n",
    "        filtered_o = filter_o(triplets_to_filter, target_s, target_r, target_o, num_entities)\n",
    "        target_o_idx = int((filtered_o == target_o).nonzero())\n",
    "        emb_s = embedding[target_s]\n",
    "        emb_r = w[target_r]\n",
    "        emb_o = embedding[filtered_o]\n",
    "        emb_triplet = emb_s * emb_r * emb_o\n",
    "        scores = torch.sigmoid(torch.sum(emb_triplet, dim=1))\n",
    "        _, indices = torch.sort(scores, descending=True)\n",
    "        rank = int((indices == target_o_idx).nonzero())\n",
    "        ranks.append(rank)\n",
    "    return torch.LongTensor(ranks)\n",
    "\n",
    "def perturb_s_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter):\n",
    "    \"\"\" Perturb subject in the triplets\n",
    "    \"\"\"\n",
    "    num_entities = embedding.shape[0]\n",
    "    ranks = []\n",
    "    for idx in range(test_size):\n",
    "        if idx % 100 == 0:\n",
    "            print(\"test triplet {} / {}\".format(idx, test_size))\n",
    "        target_s = s[idx]\n",
    "        target_r = r[idx]\n",
    "        target_o = o[idx]\n",
    "        filtered_s = filter_s(triplets_to_filter, target_s, target_r, target_o, num_entities)\n",
    "        target_s_idx = int((filtered_s == target_s).nonzero())\n",
    "        emb_s = embedding[filtered_s]\n",
    "        emb_r = w[target_r]\n",
    "        emb_o = embedding[target_o]\n",
    "        emb_triplet = emb_s * emb_r * emb_o\n",
    "        scores = torch.sigmoid(torch.sum(emb_triplet, dim=1))\n",
    "        _, indices = torch.sort(scores, descending=True)\n",
    "        rank = int((indices == target_s_idx).nonzero())\n",
    "        ranks.append(rank)\n",
    "    return torch.LongTensor(ranks)\n",
    "\n",
    "def calc_filtered_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits=[]):\n",
    "    with torch.no_grad():\n",
    "        s = test_triplets[:, 0]\n",
    "        r = test_triplets[:, 1]\n",
    "        o = test_triplets[:, 2]\n",
    "        test_size = test_triplets.shape[0]\n",
    "\n",
    "        triplets_to_filter = torch.cat([train_triplets, valid_triplets, test_triplets]).tolist()\n",
    "        triplets_to_filter = {tuple(triplet) for triplet in triplets_to_filter}\n",
    "        print('Perturbing subject...')\n",
    "        ranks_s = perturb_s_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter)\n",
    "        print('Perturbing object...')\n",
    "        ranks_o = perturb_o_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter)\n",
    "\n",
    "        ranks = torch.cat([ranks_s, ranks_o])\n",
    "        ranks += 1 # change to 1-indexed\n",
    "\n",
    "        mrr = torch.mean(1.0 / ranks.float())\n",
    "        print(\"MRR (filtered): {:.6f}\".format(mrr.item()))\n",
    "\n",
    "        for hit in hits:\n",
    "            avg_count = torch.mean((ranks <= hit).float())\n",
    "            print(\"Hits (filtered) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
    "    return mrr.item()\n",
    "\n",
    "#######################################################################\n",
    "#\n",
    "# Main evaluation function\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "def calc_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits=[], eval_bz=100, eval_p=\"filtered\"):\n",
    "    if eval_p == \"filtered\":\n",
    "        mrr = calc_filtered_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits)\n",
    "    else:\n",
    "        mrr = calc_raw_mrr(embedding, w, test_triplets, hits, eval_bz)\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test graph:\n",
      "[    0     0     0 ... 14540 14540 14540] [    0     0  1042 ... 14246 14446 14521] [116 353  31 ... 166  82  82]\n",
      "(14540, 14521, 82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Aplikasi\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:106: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nodes: 14541, # edges: 544230\n"
     ]
    }
   ],
   "source": [
    "# build test graph\n",
    "test_graph, test_rel, test_norm = build_test_graph(\n",
    "    num_nodes, num_rels, train_data)\n",
    "test_deg = test_graph.in_degrees(range(test_graph.number_of_nodes())).float().view(-1,1)\n",
    "test_node_id = torch.arange(0, num_nodes, dtype=torch.long).view(-1, 1)\n",
    "test_rel = torch.from_numpy(test_rel)\n",
    "test_norm = node_norm_to_edge_norm(test_graph, torch.from_numpy(test_norm).view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-d364b9dc124c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_edges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "g.add_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    " # build adj list and calculate degrees for sampling\n",
    "adj_list, degrees = get_adj_and_degrees(num_nodes, train_data)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "# sampled nodes: 11815\n",
      "# sampled edges: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Aplikasi\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:106: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nodes: 11815, # edges: 30000\n",
      "Done edge sampling\n",
      "Epoch 0001 | Loss 4.8210 | Best MRR 0.0000 | Forward 59.7757s | Backward 26.2488s\n",
      "# sampled nodes: 11784\n",
      "# sampled edges: 30000\n",
      "# nodes: 11784, # edges: 30000\n",
      "Done edge sampling\n",
      "Epoch 0002 | Loss 3.4957 | Best MRR 0.0000 | Forward 6.3529s | Backward 10.1972s\n",
      "start eval\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-a87e18e75a8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"start eval\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_node_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_rel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         mrr = utils.calc_mrr(embed, model.w_relation, torch.LongTensor(train_data),\n\u001b[0;32m     59\u001b[0m                                 \u001b[0mvalid_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_bz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Aplikasi\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-ce55831bee9c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, g, h, r, norm)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrgcn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregularization_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-b14c1a7ab464>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, g, h, r, norm)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Aplikasi\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Aplikasi\\Anaconda\\lib\\site-packages\\dgl\\nn\\pytorch\\conv\\relgraphconv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, g, x, etypes, norm)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mloop_message\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul_maybe_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloop_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;31m# message passing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'msg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'h'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m             \u001b[1;31m# apply bias and activation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mnode_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'h'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Aplikasi\\Anaconda\\lib\\site-packages\\dgl\\graph.py\u001b[0m in \u001b[0;36mupdate_all\u001b[1;34m(self, message_func, reduce_func, apply_node_func)\u001b[0m\n\u001b[0;32m   3236\u001b[0m                                           \u001b[0mreduce_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3237\u001b[0m                                           apply_func=apply_node_func)\n\u001b[1;32m-> 3238\u001b[1;33m             \u001b[0mRuntime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3240\u001b[0m     def prop_nodes(self,\n",
      "\u001b[1;32mC:\\Aplikasi\\Anaconda\\lib\\site-packages\\dgl\\runtime\\runtime.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(prog)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mexe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;31m# prog.pprint_exe(exe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mexe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Aplikasi\\Anaconda\\lib\\site-packages\\dgl\\runtime\\ir\\executor.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0medge_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfdedge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mdst_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfddst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[0mudf_ret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mudf_ret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Aplikasi\\Anaconda\\lib\\site-packages\\dgl\\runtime\\scheduler.py\u001b[0m in \u001b[0;36m_mfunc_wrapper\u001b[1;34m(src_data, edge_data, dst_data)\u001b[0m\n\u001b[0;32m    970\u001b[0m                            \u001b[0msrc_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m                            canonical_etype=canonical_etype)\n\u001b[1;32m--> 972\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mebatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    973\u001b[0m     \u001b[0m_mfunc_wrapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFUNC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_mfunc_wrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEDGE_UDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_mfunc_wrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfdsrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfdedge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfddst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Aplikasi\\Anaconda\\lib\\site-packages\\dgl\\nn\\pytorch\\conv\\relgraphconv.py\u001b[0m in \u001b[0;36mbdd_message_func\u001b[1;34m(self, edges)\u001b[0m\n\u001b[0;32m    146\u001b[0m             -1, self.submat_in, self.submat_out)\n\u001b[0;32m    147\u001b[0m         \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0medges\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'h'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmat_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'norm'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0medges\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0medges\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'norm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "model_state_file = 'model_state.pth'\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "use_cuda=False\n",
    "# training loop\n",
    "print(\"start training...\")\n",
    "\n",
    "epoch = 0\n",
    "best_mrr = 0\n",
    "while True:\n",
    "    model.train()\n",
    "    epoch += 1\n",
    "\n",
    "    # perform edge neighborhood sampling to generate training graph and data\n",
    "    g, node_id, edge_type, node_norm, data, labels = \\\n",
    "        generate_sampled_graph_and_labels(\n",
    "            train_data, 30000, 0.5,\n",
    "            num_rels, adj_list, degrees, 10,\n",
    "            'uniform')\n",
    "    print(\"Done edge sampling\")\n",
    "\n",
    "        # set node/edge feature\n",
    "    node_id = torch.from_numpy(node_id).view(-1, 1).long()\n",
    "    edge_type = torch.from_numpy(edge_type)\n",
    "    edge_norm = node_norm_to_edge_norm(g, torch.from_numpy(node_norm).view(-1, 1))\n",
    "    data, labels = torch.from_numpy(data), torch.from_numpy(labels)\n",
    "    deg = g.in_degrees(range(g.number_of_nodes())).float().view(-1, 1)\n",
    "    if use_cuda:\n",
    "        node_id, deg = node_id.cuda(), deg.cuda()\n",
    "        edge_type, edge_norm = edge_type.cuda(), edge_norm.cuda()\n",
    "        data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "    t0 = time.time()\n",
    "    embed = model(g, node_id, edge_type, edge_norm)\n",
    "    loss = model.get_loss(g, embed, data, labels)\n",
    "    t1 = time.time()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clip gradients\n",
    "    optimizer.step()\n",
    "    t2 = time.time()\n",
    "\n",
    "    forward_time.append(t1 - t0)\n",
    "    backward_time.append(t2 - t1)\n",
    "    print(\"Epoch {:04d} | Loss {:.4f} | Best MRR {:.4f} | Forward {:.4f}s | Backward {:.4f}s\".\n",
    "            format(epoch, loss.item(), best_mrr, forward_time[-1], backward_time[-1]))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "        # validation\n",
    "    if epoch % 2 == 0:\n",
    "        # perform validation on CPU because full graph is too large\n",
    "        if use_cuda:\n",
    "            model.cpu()\n",
    "        model.eval()\n",
    "        print(\"start eval\")\n",
    "        embed = model(test_graph, test_node_id, test_rel, test_norm)\n",
    "        mrr = utils.calc_mrr(embed, model.w_relation, torch.LongTensor(train_data),\n",
    "                                valid_data, test_data, hits=[1, 3, 10], eval_bz=500,\n",
    "                                eval_p='filtered')\n",
    "        # save best model\n",
    "        if mrr < best_mrr:\n",
    "            if epoch >= 10:\n",
    "                break\n",
    "        else:\n",
    "            best_mrr = mrr\n",
    "            torch.save({'state_dict': model.state_dict(), 'epoch': epoch},\n",
    "                        model_state_file)\n",
    "        if use_cuda:\n",
    "            model.cuda()\n",
    "\n",
    "print(\"training done\")\n",
    "print(\"Mean forward time: {:4f}s\".format(np.mean(forward_time)))\n",
    "print(\"Mean Backward time: {:4f}s\".format(np.mean(backward_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DGLGraph(num_nodes=11774, num_edges=30000,\n",
       "         ndata_schemes={}\n",
       "         edata_schemes={})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14541"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DGLGraph(num_nodes=11752, num_edges=30000,\n",
       "          ndata_schemes={}\n",
       "          edata_schemes={}), 11752, 30000, 30000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g, len(node_id), len(edge_type), len(edge_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
