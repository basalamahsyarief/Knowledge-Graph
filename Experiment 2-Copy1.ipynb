{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "from functools import partial\n",
    "import dgl\n",
    "from dgl.contrib.data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch import RelGraphConv\n",
    "class BaseRGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels, num_bases,\n",
    "                 num_hidden_layers=1, dropout=0,\n",
    "                 use_self_loop=False, use_cuda=False):\n",
    "        super(BaseRGCN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = None if num_bases < 0 else num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_self_loop = use_self_loop\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # i2h\n",
    "        i2h = self.build_input_layer()\n",
    "        if i2h is not None:\n",
    "            self.layers.append(i2h)\n",
    "        # h2h\n",
    "        for idx in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer(idx)\n",
    "            self.layers.append(h2h)\n",
    "        # h2o\n",
    "        h2o = self.build_output_layer()\n",
    "        if h2o is not None:\n",
    "            self.layers.append(h2o)\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return None\n",
    "\n",
    "    def build_hidden_layer(self, idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return None\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h, r, norm)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_nodes, h_dim)\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        return self.embedding(h.squeeze())\n",
    "\n",
    "class RGCN(BaseRGCN):\n",
    "    def build_input_layer(self):\n",
    "        return EmbeddingLayer(self.num_nodes, self.h_dim)\n",
    "\n",
    "    def build_hidden_layer(self, idx):\n",
    "        act = F.relu if idx < self.num_hidden_layers - 1 else None\n",
    "        return RelGraphConv(self.h_dim, self.h_dim, self.num_rels, \"bdd\",\n",
    "                self.num_bases, activation=act, self_loop=True,\n",
    "                dropout=self.dropout)\n",
    "\n",
    "class LinkPredict(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, num_rels, num_bases=-1,\n",
    "                 num_hidden_layers=1, dropout=0, use_cuda=False, reg_param=0):\n",
    "        super(LinkPredict, self).__init__()\n",
    "        self.rgcn = RGCN(in_dim, h_dim, h_dim, num_rels * 2, num_bases,\n",
    "                         num_hidden_layers, dropout, use_cuda)\n",
    "        self.reg_param = reg_param\n",
    "        self.w_relation = nn.Parameter(torch.Tensor(num_rels, h_dim))\n",
    "        nn.init.xavier_uniform_(self.w_relation,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def calc_score(self, embedding, triplets):\n",
    "        # DistMult\n",
    "        s = embedding[triplets[:,0]]\n",
    "        r = self.w_relation[triplets[:,1]]\n",
    "        o = embedding[triplets[:,2]]\n",
    "        score = torch.sum(s * r * o, dim=1)\n",
    "        return score\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        return self.rgcn.forward(g, h, r, norm)\n",
    "\n",
    "    def regularization_loss(self, embedding):\n",
    "        return torch.mean(embedding.pow(2)) + torch.mean(self.w_relation.pow(2))\n",
    "\n",
    "    def get_loss(self, g, embed, triplets, labels):\n",
    "        # triplets is a list of data samples (positive and negative)\n",
    "        # each row in the triplets is a 3-tuple of (source, relation, destination)\n",
    "        score = self.calc_score(embed, triplets)\n",
    "        predict_loss = F.binary_cross_entropy_with_logits(score, labels)\n",
    "        reg_loss = self.regularization_loss(embed)\n",
    "        return predict_loss + self.reg_param * reg_loss\n",
    "\n",
    "def node_norm_to_edge_norm(g, node_norm):\n",
    "    g = g.local_var()\n",
    "    # convert to edge norm\n",
    "    g.ndata['norm'] = node_norm\n",
    "    g.apply_edges(lambda edges : {'norm' : edges.dst['norm']})\n",
    "    return g.edata['norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# entities: 14541\n",
      "# relations: 237\n",
      "# edges: 272115\n"
     ]
    }
   ],
   "source": [
    "data = load_data('FB15k-237')\n",
    "num_nodes = data.num_nodes\n",
    "train_data = data.train\n",
    "valid_data = data.valid\n",
    "test_data = data.test\n",
    "num_rels = data.num_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = data.num_nodes\n",
    "train_data = data.train\n",
    "valid_data = data.valid\n",
    "test_data = data.test\n",
    "num_rels = data.num_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('sample_dataset_learnavi.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(data.source_name.append(data.target_name).unique())\n",
    "num_rels = len(data.edge.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinkPredict(num_nodes,\n",
    "                    140,\n",
    "                    num_rels,\n",
    "                    num_bases=140,\n",
    "                    num_hidden_layers=2,\n",
    "                    dropout=0.2,\n",
    "                    use_cuda=-1,\n",
    "                    reg_param=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, valid_data = train_test_split(data[['source','edge','target']].values, test_size=0.33, random_state=42)\n",
    "valid_data, test_data = train_test_split(valid_data, test_size=0.5, random_state=42)\n",
    "# data = shuffle(data).reset_index(drop=True)\n",
    "# train_data = data[:6055]\n",
    "# valid_data = data[6055:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 569,    6, 2511],\n",
       "        [1044,    6, 2511],\n",
       "        [ 659,    6, 2511],\n",
       "        ...,\n",
       "        [ 632,    6, 2511],\n",
       "        [2511,    3, 2511],\n",
       "        [1160,    4, 2511]], dtype=int64), array([[1149,    4, 2511],\n",
       "        [2463,    0, 2511],\n",
       "        [2511,    3, 2511],\n",
       "        ...,\n",
       "        [2100,    5, 2511],\n",
       "        [2265,    0, 2511],\n",
       "        [2511,    1, 2511]], dtype=int64))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = torch.LongTensor(valid_data)\n",
    "test_data = torch.LongTensor(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_and_degrees(num_nodes, triplets):\n",
    "    \"\"\" Get adjacency list and degrees of the graph\n",
    "    \"\"\"\n",
    "    adj_list = [[] for _ in range(num_nodes)]\n",
    "    for i,triplet in enumerate(triplets):\n",
    "        adj_list[triplet[0]].append([i, triplet[2]])\n",
    "        adj_list[triplet[2]].append([i, triplet[0]])\n",
    "\n",
    "    degrees = np.array([len(a) for a in adj_list])\n",
    "    adj_list = [np.array(a) for a in adj_list]\n",
    "    return adj_list, degrees\n",
    "\n",
    "def sample_edge_neighborhood(adj_list, degrees, n_triplets, sample_size):\n",
    "    \"\"\"Sample edges by neighborhool expansion.\n",
    "    This guarantees that the sampled edges form a connected graph, which\n",
    "    may help deeper GNNs that require information from more than one hop.\n",
    "    \"\"\"\n",
    "    edges = np.zeros((sample_size), dtype=np.int32)\n",
    "\n",
    "    #initialize\n",
    "    sample_counts = np.array([d for d in degrees])\n",
    "    picked = np.array([False for _ in range(n_triplets)])\n",
    "    seen = np.array([False for _ in degrees])\n",
    "\n",
    "    for i in range(0, sample_size):\n",
    "        weights = sample_counts * seen\n",
    "\n",
    "        if np.sum(weights) == 0:\n",
    "            weights = np.ones_like(weights)\n",
    "            weights[np.where(sample_counts == 0)] = 0\n",
    "\n",
    "        probabilities = (weights) / np.sum(weights)\n",
    "        chosen_vertex = np.random.choice(np.arange(degrees.shape[0]),\n",
    "                                         p=probabilities)\n",
    "        chosen_adj_list = adj_list[chosen_vertex]\n",
    "        seen[chosen_vertex] = True\n",
    "\n",
    "        chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
    "        chosen_edge = chosen_adj_list[chosen_edge]\n",
    "        edge_number = chosen_edge[0]\n",
    "\n",
    "        while picked[edge_number]:\n",
    "            chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
    "            chosen_edge = chosen_adj_list[chosen_edge]\n",
    "            edge_number = chosen_edge[0]\n",
    "\n",
    "        edges[i] = edge_number\n",
    "        other_vertex = chosen_edge[1]\n",
    "        picked[edge_number] = True\n",
    "        sample_counts[chosen_vertex] -= 1\n",
    "        sample_counts[other_vertex] -= 1\n",
    "        seen[other_vertex] = True\n",
    "\n",
    "    return edges\n",
    "\n",
    "def sample_edge_uniform(adj_list, degrees, n_triplets, sample_size):\n",
    "    \"\"\"Sample edges uniformly from all the edges.\"\"\"\n",
    "    all_edges = np.arange(n_triplets)\n",
    "    return np.random.choice(all_edges, sample_size, replace=False)\n",
    "\n",
    "def generate_sampled_graph_and_labels(triplets, sample_size, split_size,\n",
    "                                      num_rels, adj_list, degrees,\n",
    "                                      negative_rate, sampler=\"uniform\"):\n",
    "    \"\"\"Get training graph and signals\n",
    "    First perform edge neighborhood sampling on graph, then perform negative\n",
    "    sampling to generate negative samples\n",
    "    \"\"\"\n",
    "    # perform edge neighbor sampling\n",
    "    if sampler == \"uniform\":\n",
    "        edges = sample_edge_uniform(adj_list, degrees, len(triplets), sample_size)\n",
    "    elif sampler == \"neighbor\":\n",
    "        edges = sample_edge_neighborhood(adj_list, degrees, len(triplets), sample_size)\n",
    "    else:\n",
    "        raise ValueError(\"Sampler type must be either 'uniform' or 'neighbor'.\")\n",
    "\n",
    "    # relabel nodes to have consecutive node ids\n",
    "    edges = triplets[edges]\n",
    "    src, rel, dst = edges.transpose()\n",
    "    uniq_v, edges = np.unique((src, dst), return_inverse=True)\n",
    "    src, dst = np.reshape(edges, (2, -1))\n",
    "    relabeled_edges = np.stack((src, rel, dst)).transpose()\n",
    "\n",
    "    # negative sampling\n",
    "    samples, labels = negative_sampling(relabeled_edges, len(uniq_v),\n",
    "                                        negative_rate)\n",
    "\n",
    "    # further split graph, only half of the edges will be used as graph\n",
    "    # structure, while the rest half is used as unseen positive samples\n",
    "    split_size = int(sample_size * split_size)\n",
    "    graph_split_ids = np.random.choice(np.arange(sample_size),\n",
    "                                       size=split_size, replace=False)\n",
    "    src = src[graph_split_ids]\n",
    "    dst = dst[graph_split_ids]\n",
    "    rel = rel[graph_split_ids]\n",
    "\n",
    "    # build DGL graph\n",
    "    print(\"# sampled nodes: {}\".format(len(uniq_v)))\n",
    "    print(\"# sampled edges: {}\".format(len(src) * 2))\n",
    "    g, rel, norm = build_graph_from_triplets(len(uniq_v), num_rels,\n",
    "                                             (src, rel, dst))\n",
    "    return g, uniq_v, rel, norm, samples, labels\n",
    "\n",
    "def comp_deg_norm(g):\n",
    "    g = g.local_var()\n",
    "    in_deg = g.in_degrees(range(g.number_of_nodes())).float().numpy()\n",
    "    norm = 1.0 / in_deg\n",
    "    norm[np.isinf(norm)] = 0\n",
    "    return norm\n",
    "\n",
    "def build_graph_from_triplets(num_nodes, num_rels, triplets):\n",
    "    \"\"\" Create a DGL graph. The graph is bidirectional because RGCN authors\n",
    "        use reversed relations.\n",
    "        This function also generates edge type and normalization factor\n",
    "        (reciprocal of node incoming degree)\n",
    "    \"\"\"\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(num_nodes)\n",
    "    src, rel, dst = triplets\n",
    "    src, dst = np.concatenate((src, dst)), np.concatenate((dst, src))\n",
    "    rel = np.concatenate((rel, rel + num_rels))\n",
    "    edges = sorted(zip(dst, src, rel))\n",
    "    dst, src, rel = np.array(edges).transpose()\n",
    "    g.add_edges(src, dst)\n",
    "    norm = comp_deg_norm(g)\n",
    "    print(\"# nodes: {}, # edges: {}\".format(num_nodes, len(src)))\n",
    "    return g, rel.astype('int64'), norm.astype('int64')\n",
    "\n",
    "def build_test_graph(num_nodes, num_rels, edges):\n",
    "    src, rel, dst = edges.transpose()\n",
    "    print(\"Test graph:\")\n",
    "    return build_graph_from_triplets(num_nodes, num_rels, (src, rel, dst))\n",
    "\n",
    "def negative_sampling(pos_samples, num_entity, negative_rate):\n",
    "    size_of_batch = len(pos_samples)\n",
    "    num_to_generate = size_of_batch * negative_rate\n",
    "    neg_samples = np.tile(pos_samples, (negative_rate, 1))\n",
    "    labels = np.zeros(size_of_batch * (negative_rate + 1), dtype=np.float32)\n",
    "    labels[: size_of_batch] = 1\n",
    "    values = np.random.randint(num_entity, size=num_to_generate)\n",
    "    choices = np.random.uniform(size=num_to_generate)\n",
    "    subj = choices > 0.5\n",
    "    obj = choices <= 0.5\n",
    "    neg_samples[subj, 0] = values[subj]\n",
    "    neg_samples[obj, 2] = values[obj]\n",
    "\n",
    "    return np.concatenate((pos_samples, neg_samples)), labels\n",
    "\n",
    "#######################################################################\n",
    "#\n",
    "# Utility functions for evaluations (raw)\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "def sort_and_rank(score, target):\n",
    "    _, indices = torch.sort(score, dim=1, descending=True)\n",
    "    indices = torch.nonzero(indices == target.view(-1, 1))\n",
    "    indices = indices[:, 1].view(-1)\n",
    "    return indices\n",
    "\n",
    "def perturb_and_get_raw_rank(embedding, w, a, r, b, test_size, batch_size=100):\n",
    "    \"\"\" Perturb one element in the triplets\n",
    "    \"\"\"\n",
    "    n_batch = (test_size + batch_size - 1) // batch_size\n",
    "    ranks = []\n",
    "    for idx in range(n_batch):\n",
    "        print(\"batch {} / {}\".format(idx, n_batch))\n",
    "        batch_start = idx * batch_size\n",
    "        batch_end = min(test_size, (idx + 1) * batch_size)\n",
    "        batch_a = a[batch_start: batch_end]\n",
    "        batch_r = r[batch_start: batch_end]\n",
    "        emb_ar = embedding[batch_a] * w[batch_r]\n",
    "        emb_ar = emb_ar.transpose(0, 1).unsqueeze(2) # size: D x E x 1\n",
    "        emb_c = embedding.transpose(0, 1).unsqueeze(1) # size: D x 1 x V\n",
    "        # out-prod and reduce sum\n",
    "        out_prod = torch.bmm(emb_ar, emb_c) # size D x E x V\n",
    "        score = torch.sum(out_prod, dim=0) # size E x V\n",
    "        score = torch.sigmoid(score)\n",
    "        target = b[batch_start: batch_end]\n",
    "        ranks.append(sort_and_rank(score, target))\n",
    "    return torch.cat(ranks)\n",
    "\n",
    "# return MRR (raw), and Hits @ (1, 3, 10)\n",
    "def calc_raw_mrr(embedding, w, test_triplets, hits=[], eval_bz=100):\n",
    "    with torch.no_grad():\n",
    "        s = test_triplets[:, 0]\n",
    "        r = test_triplets[:, 1]\n",
    "        o = test_triplets[:, 2]\n",
    "        test_size = test_triplets.shape[0]\n",
    "\n",
    "        # perturb subject\n",
    "        ranks_s = perturb_and_get_raw_rank(embedding, w, o, r, s, test_size, eval_bz)\n",
    "        # perturb object\n",
    "        ranks_o = perturb_and_get_raw_rank(embedding, w, s, r, o, test_size, eval_bz)\n",
    "\n",
    "        ranks = torch.cat([ranks_s, ranks_o])\n",
    "        ranks += 1 # change to 1-indexed\n",
    "\n",
    "        mrr = torch.mean(1.0 / ranks.float())\n",
    "        print(\"MRR (raw): {:.6f}\".format(mrr.item()))\n",
    "\n",
    "        for hit in hits:\n",
    "            avg_count = torch.mean((ranks <= hit).float())\n",
    "            print(\"Hits (raw) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
    "    return mrr.item()\n",
    "\n",
    "#######################################################################\n",
    "#\n",
    "# Utility functions for evaluations (filtered)\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "def filter_o(triplets_to_filter, target_s, target_r, target_o, num_entities):\n",
    "    target_s, target_r, target_o = int(target_s), int(target_r), int(target_o)\n",
    "    filtered_o = []\n",
    "    # Do not filter out the test triplet, since we want to predict on it\n",
    "    if (target_s, target_r, target_o) in triplets_to_filter:\n",
    "        triplets_to_filter.remove((target_s, target_r, target_o))\n",
    "    # Do not consider an object if it is part of a triplet to filter\n",
    "    for o in range(num_entities):\n",
    "        if (target_s, target_r, o) not in triplets_to_filter:\n",
    "            filtered_o.append(o)\n",
    "    return torch.LongTensor(filtered_o)\n",
    "\n",
    "def filter_s(triplets_to_filter, target_s, target_r, target_o, num_entities):\n",
    "    target_s, target_r, target_o = int(target_s), int(target_r), int(target_o)\n",
    "    filtered_s = []\n",
    "    # Do not filter out the test triplet, since we want to predict on it\n",
    "    if (target_s, target_r, target_o) in triplets_to_filter:\n",
    "        triplets_to_filter.remove((target_s, target_r, target_o))\n",
    "    # Do not consider a subject if it is part of a triplet to filter\n",
    "    for s in range(num_entities):\n",
    "        if (s, target_r, target_o) not in triplets_to_filter:\n",
    "            filtered_s.append(s)\n",
    "    return torch.LongTensor(filtered_s)\n",
    "\n",
    "def perturb_o_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter):\n",
    "    \"\"\" Perturb object in the triplets\n",
    "    \"\"\"\n",
    "    num_entities = embedding.shape[0]\n",
    "    ranks = []\n",
    "    for idx in range(test_size):\n",
    "        if idx % 100 == 0:\n",
    "            print(\"test triplet {} / {}\".format(idx, test_size))\n",
    "        target_s = s[idx]\n",
    "        target_r = r[idx]\n",
    "        target_o = o[idx]\n",
    "        filtered_o = filter_o(triplets_to_filter, target_s, target_r, target_o, num_entities)\n",
    "        target_o_idx = int((filtered_o == target_o).nonzero())\n",
    "        emb_s = embedding[target_s]\n",
    "        emb_r = w[target_r]\n",
    "        emb_o = embedding[filtered_o]\n",
    "        emb_triplet = emb_s * emb_r * emb_o\n",
    "        scores = torch.sigmoid(torch.sum(emb_triplet, dim=1))\n",
    "        _, indices = torch.sort(scores, descending=True)\n",
    "        rank = int((indices == target_o_idx).nonzero())\n",
    "        ranks.append(rank)\n",
    "    return torch.LongTensor(ranks)\n",
    "\n",
    "def perturb_s_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter):\n",
    "    \"\"\" Perturb subject in the triplets\n",
    "    \"\"\"\n",
    "    num_entities = embedding.shape[0]\n",
    "    ranks = []\n",
    "    for idx in range(test_size):\n",
    "        if idx % 100 == 0:\n",
    "            print(\"test triplet {} / {}\".format(idx, test_size))\n",
    "        target_s = s[idx]\n",
    "        target_r = r[idx]\n",
    "        target_o = o[idx]\n",
    "        filtered_s = filter_s(triplets_to_filter, target_s, target_r, target_o, num_entities)\n",
    "        target_s_idx = int((filtered_s == target_s).nonzero())\n",
    "        emb_s = embedding[filtered_s]\n",
    "        emb_r = w[target_r]\n",
    "        emb_o = embedding[target_o]\n",
    "        emb_triplet = emb_s * emb_r * emb_o\n",
    "        scores = torch.sigmoid(torch.sum(emb_triplet, dim=1))\n",
    "        _, indices = torch.sort(scores, descending=True)\n",
    "        rank = int((indices == target_s_idx).nonzero())\n",
    "        ranks.append(rank)\n",
    "    return torch.LongTensor(ranks)\n",
    "\n",
    "def calc_filtered_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits=[]):\n",
    "    with torch.no_grad():\n",
    "        s = test_triplets[:, 0]\n",
    "        r = test_triplets[:, 1]\n",
    "        o = test_triplets[:, 2]\n",
    "        test_size = test_triplets.shape[0]\n",
    "\n",
    "        triplets_to_filter = torch.cat([train_triplets, valid_triplets, test_triplets]).tolist()\n",
    "        triplets_to_filter = {tuple(triplet) for triplet in triplets_to_filter}\n",
    "        print('Perturbing subject...')\n",
    "        ranks_s = perturb_s_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter)\n",
    "        print('Perturbing object...')\n",
    "        ranks_o = perturb_o_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter)\n",
    "\n",
    "        ranks = torch.cat([ranks_s, ranks_o])\n",
    "        ranks += 1 # change to 1-indexed\n",
    "\n",
    "        mrr = torch.mean(1.0 / ranks.float())\n",
    "        print(\"MRR (filtered): {:.6f}\".format(mrr.item()))\n",
    "\n",
    "        for hit in hits:\n",
    "            avg_count = torch.mean((ranks <= hit).float())\n",
    "            print(\"Hits (filtered) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
    "    return mrr.item()\n",
    "\n",
    "#######################################################################\n",
    "#\n",
    "# Main evaluation function\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "def calc_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits=[], eval_bz=100, eval_p=\"filtered\"):\n",
    "    if eval_p == \"filtered\":\n",
    "        mrr = calc_filtered_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits)\n",
    "    else:\n",
    "        mrr = calc_raw_mrr(embedding, w, test_triplets, hits, eval_bz)\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test graph:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Aplikasi\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:106: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nodes: 2512, # edges: 10142\n"
     ]
    }
   ],
   "source": [
    "# build test graph\n",
    "test_graph, test_rel, test_norm = build_test_graph(\n",
    "    num_nodes, num_rels, train_data)\n",
    "test_deg = test_graph.in_degrees(range(test_graph.number_of_nodes())).float().view(-1,1)\n",
    "test_node_id = torch.arange(0, num_nodes, dtype=torch.long).view(-1, 1)\n",
    "test_rel = torch.from_numpy(test_rel)\n",
    "test_norm = node_norm_to_edge_norm(test_graph, torch.from_numpy(test_norm).view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    " # build adj list and calculate degrees for sampling\n",
    "adj_list, degrees = get_adj_and_degrees(num_nodes, train_data)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "# sampled nodes: 14\n",
      "# sampled edges: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Aplikasi\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:106: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nodes: 14, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0001 | Loss 2.7152 | Best MRR 0.0000 | Forward 0.0130s | Backward 0.0281s\n",
      "# sampled nodes: 19\n",
      "# sampled edges: 20\n",
      "# nodes: 19, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0002 | Loss 1.9811 | Best MRR 0.0000 | Forward 0.0110s | Backward 0.0271s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.003038\n",
      "Hits (filtered) @ 1: 0.000000\n",
      "Hits (filtered) @ 3: 0.000801\n",
      "Hits (filtered) @ 10: 0.003203\n",
      "# sampled nodes: 16\n",
      "# sampled edges: 20\n",
      "# nodes: 16, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0003 | Loss 1.5228 | Best MRR 0.0030 | Forward 0.0341s | Backward 0.0281s\n",
      "# sampled nodes: 14\n",
      "# sampled edges: 20\n",
      "# nodes: 14, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0004 | Loss 0.8991 | Best MRR 0.0030 | Forward 0.0100s | Backward 0.0261s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.133531\n",
      "Hits (filtered) @ 1: 0.090472\n",
      "Hits (filtered) @ 3: 0.140512\n",
      "Hits (filtered) @ 10: 0.190552\n",
      "# sampled nodes: 13\n",
      "# sampled edges: 20\n",
      "# nodes: 13, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0005 | Loss 0.7816 | Best MRR 0.1335 | Forward 0.0361s | Backward 0.0256s\n",
      "# sampled nodes: 14\n",
      "# sampled edges: 20\n",
      "# nodes: 14, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0006 | Loss 0.6585 | Best MRR 0.1335 | Forward 0.0155s | Backward 0.0271s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.580482\n",
      "Hits (filtered) @ 1: 0.472778\n",
      "Hits (filtered) @ 3: 0.685749\n",
      "Hits (filtered) @ 10: 0.692154\n",
      "# sampled nodes: 17\n",
      "# sampled edges: 20\n",
      "# nodes: 17, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0007 | Loss 0.5253 | Best MRR 0.5805 | Forward 0.0376s | Backward 0.0271s\n",
      "# sampled nodes: 15\n",
      "# sampled edges: 20\n",
      "# nodes: 15, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0008 | Loss 0.4268 | Best MRR 0.5805 | Forward 0.0180s | Backward 0.0266s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.692236\n",
      "Hits (filtered) @ 1: 0.690953\n",
      "Hits (filtered) @ 3: 0.691353\n",
      "Hits (filtered) @ 10: 0.692954\n",
      "# sampled nodes: 12\n",
      "# sampled edges: 20\n",
      "# nodes: 12, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0009 | Loss 0.3730 | Best MRR 0.6922 | Forward 0.0291s | Backward 0.0261s\n",
      "# sampled nodes: 15\n",
      "# sampled edges: 20\n",
      "# nodes: 15, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0010 | Loss 0.4063 | Best MRR 0.6922 | Forward 0.0135s | Backward 0.0266s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.692445\n",
      "Hits (filtered) @ 1: 0.691353\n",
      "Hits (filtered) @ 3: 0.691753\n",
      "Hits (filtered) @ 10: 0.692554\n",
      "# sampled nodes: 11\n",
      "# sampled edges: 20\n",
      "# nodes: 11, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0011 | Loss 0.3190 | Best MRR 0.6924 | Forward 0.0336s | Backward 0.0251s\n",
      "# sampled nodes: 15\n",
      "# sampled edges: 20\n",
      "# nodes: 15, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0012 | Loss 0.3508 | Best MRR 0.6924 | Forward 0.0165s | Backward 0.0226s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.693033\n",
      "Hits (filtered) @ 1: 0.691753\n",
      "Hits (filtered) @ 3: 0.692154\n",
      "Hits (filtered) @ 10: 0.693755\n",
      "# sampled nodes: 10\n",
      "# sampled edges: 20\n",
      "# nodes: 10, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0013 | Loss 0.2888 | Best MRR 0.6930 | Forward 0.0381s | Backward 0.0246s\n",
      "# sampled nodes: 14\n",
      "# sampled edges: 20\n",
      "# nodes: 14, # edges: 20\n",
      "Done edge sampling\n",
      "Epoch 0014 | Loss 0.3606 | Best MRR 0.6930 | Forward 0.0130s | Backward 0.0256s\n",
      "start eval\n",
      "Perturbing subject...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "Perturbing object...\n",
      "test triplet 0 / 1249\n",
      "test triplet 100 / 1249\n",
      "test triplet 200 / 1249\n",
      "test triplet 300 / 1249\n",
      "test triplet 400 / 1249\n",
      "test triplet 500 / 1249\n",
      "test triplet 600 / 1249\n",
      "test triplet 700 / 1249\n",
      "test triplet 800 / 1249\n",
      "test triplet 900 / 1249\n",
      "test triplet 1000 / 1249\n",
      "test triplet 1100 / 1249\n",
      "test triplet 1200 / 1249\n",
      "MRR (filtered): 0.692278\n",
      "Hits (filtered) @ 1: 0.690953\n",
      "Hits (filtered) @ 3: 0.691353\n",
      "Hits (filtered) @ 10: 0.692954\n",
      "training done\n",
      "Mean forward time: 0.022811s\n",
      "Mean Backward time: 0.026142s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "model_state_file = 'model_state.pth'\n",
    "forward_time = []\n",
    "backward_time = []\n",
    "use_cuda=False\n",
    "# training loop\n",
    "print(\"start training...\")\n",
    "\n",
    "epoch = 0\n",
    "best_mrr = 0\n",
    "while True:\n",
    "    model.train()\n",
    "    epoch += 1\n",
    "\n",
    "    # perform edge neighborhood sampling to generate training graph and data\n",
    "    g, node_id, edge_type, node_norm, data, labels = \\\n",
    "        generate_sampled_graph_and_labels(\n",
    "            train_data, 20, 0.5,\n",
    "            num_rels, adj_list, degrees, 10,\n",
    "            'uniform')\n",
    "    print(\"Done edge sampling\")\n",
    "\n",
    "        # set node/edge feature\n",
    "    node_id = torch.from_numpy(node_id).view(-1, 1).long()\n",
    "    edge_type = torch.from_numpy(edge_type)\n",
    "    edge_norm = node_norm_to_edge_norm(g, torch.from_numpy(node_norm).view(-1, 1))\n",
    "    data, labels = torch.from_numpy(data), torch.from_numpy(labels)\n",
    "    deg = g.in_degrees(range(g.number_of_nodes())).float().view(-1, 1)\n",
    "    if use_cuda:\n",
    "        node_id, deg = node_id.cuda(), deg.cuda()\n",
    "        edge_type, edge_norm = edge_type.cuda(), edge_norm.cuda()\n",
    "        data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "    t0 = time.time()\n",
    "    embed = model(g, node_id, edge_type, edge_norm)\n",
    "    loss = model.get_loss(g, embed, data, labels)\n",
    "    t1 = time.time()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clip gradients\n",
    "    optimizer.step()\n",
    "    t2 = time.time()\n",
    "\n",
    "    forward_time.append(t1 - t0)\n",
    "    backward_time.append(t2 - t1)\n",
    "    print(\"Epoch {:04d} | Loss {:.4f} | Best MRR {:.4f} | Forward {:.4f}s | Backward {:.4f}s\".\n",
    "            format(epoch, loss.item(), best_mrr, forward_time[-1], backward_time[-1]))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "        # validation\n",
    "    if epoch % 2 == 0:\n",
    "        # perform validation on CPU because full graph is too large\n",
    "        if use_cuda:\n",
    "            model.cpu()\n",
    "        model.eval()\n",
    "        print(\"start eval\")\n",
    "        embed = model(test_graph, test_node_id, test_rel, test_norm)\n",
    "        mrr = calc_mrr(embed, model.w_relation, torch.LongTensor(train_data),\n",
    "                                valid_data, test_data, hits=[1, 3, 10], eval_bz=500,\n",
    "                                eval_p='filtered')\n",
    "        # save best model\n",
    "        if mrr < best_mrr:\n",
    "            if epoch >= 10:\n",
    "                break\n",
    "        else:\n",
    "            best_mrr = mrr\n",
    "            torch.save({'state_dict': model.state_dict(), 'epoch': epoch},\n",
    "                        model_state_file)\n",
    "        if use_cuda:\n",
    "            model.cuda()\n",
    "\n",
    "print(\"training done\")\n",
    "print(\"Mean forward time: {:4f}s\".format(np.mean(forward_time)))\n",
    "print(\"Mean Backward time: {:4f}s\".format(np.mean(backward_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
